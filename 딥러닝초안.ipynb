{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24e432f",
   "metadata": {},
   "source": [
    "### ì ë”¥ëŸ¬ë‹ ì‹œì‘í•´ ë³´ìê³ ~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823daf4",
   "metadata": {},
   "source": [
    "- ì¼ë‹¨ ê°€ìƒí™˜ê²½ë¶€í„° ë§Œë“¤ì–´ì¤Œ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd814f5f",
   "metadata": {},
   "source": [
    "í™˜ê²½ì’!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ade55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suakim/Documents/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/flutter á„ƒá…µá†¸á„…á…¥á„‚á…µá†¼ á„‘á…³á†¯á„Œá…¦á†¨/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.6\n",
      "TF: 2.16.1\n",
      "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "CWD: /Users/suakim/Documents/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/flutter á„ƒá…µá†¸á„…á…¥á„‚á…µá†¼ á„‘á…³á†¯á„Œá…¦á†¨/Model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, platform, pathlib\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"CWD:\", pathlib.Path().resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3131276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dogs images: 4978, breeds: 25\n",
      "ì˜ˆì‹œ í’ˆì¢… 10ê°œ: ['american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees']\n",
      "Oxford root: /Users/suakim/Documents/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/flutter á„ƒá…µá†¸á„…á…¥á„‚á…µá†¼ á„‘á…³á†¯á„Œá…¦á†¨/data_sources/oxford\n",
      "Export dir: /Users/suakim/Documents/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/flutter á„ƒá…µá†¸á„…á…¥á„‚á…µá†¼ á„‘á…³á†¯á„Œá…¦á†¨/Model/data\n"
     ]
    }
   ],
   "source": [
    "import pathlib, re, os, shutil\n",
    "\n",
    "# 1) Oxford ë£¨íŠ¸ ìë™ íƒìƒ‰\n",
    "def find_oxford_root():\n",
    "    here = pathlib.Path().resolve()\n",
    "    for p in [\n",
    "        here / \"data_sources\" / \"oxford\",\n",
    "        here.parent / \"data_sources\" / \"oxford\",\n",
    "        here.parent.parent / \"data_sources\" / \"oxford\",\n",
    "        here.parent.parent.parent / \"data_sources\" / \"oxford\",\n",
    "    ]:\n",
    "        if (p / \"annotations\" / \"list.txt\").exists() and (p / \"images\").exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "root = find_oxford_root()\n",
    "if root is None:\n",
    "    raise FileNotFoundError(\"Oxford-IIIT Pet ê²½ë¡œë¥¼ ëª» ì°¾ì•˜ì–´ìš”. data_sources/oxford ì•„ë˜ì— images/ì™€ annotations/list.txtê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "img_dir, ann_dir = root/\"images\", root/\"annotations\"\n",
    "dest = pathlib.Path(\"data\"); dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "USE_HARDLINKS = True   # ê°™ì€ ë””ìŠ¤í¬ë©´ ìš©ëŸ‰ ì ˆì•½\n",
    "count = 0; breeds = set()\n",
    "\n",
    "with open(ann_dir/\"list.txt\") as f:\n",
    "    for raw in f:\n",
    "        # 2) ì£¼ì„/ì—¬ë¶„ í† í° ë°©ì–´ ì½”ë“œ\n",
    "        line = raw.split('#', 1)[0].strip()   # # ë’¤ ì£¼ì„ ì œê±°\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        name, cls_id, species, breed_id = parts[:4]  # ì• 4ê°œë§Œ ì‚¬ìš©\n",
    "\n",
    "        # 3) ê°•ì•„ì§€(species==2)ë§Œ ì„ ë³„\n",
    "        try:\n",
    "            if int(species) != 2:   # 1=cat, 2=dog\n",
    "                continue\n",
    "        except ValueError:\n",
    "            continue  # ë¹„ì •ìƒ ì¤„ ë°©ì–´\n",
    "\n",
    "        # 4) íŒŒì¼ëª… â†’ í’ˆì¢…ëª… ì¶”ì¶œ (ë§¨ ë _ìˆ«ì ì œê±°)\n",
    "        breed = re.sub(r\"_(\\d+)$\", \"\", name)\n",
    "        src = img_dir / (name + \".jpg\")\n",
    "        if not src.exists():\n",
    "            continue\n",
    "\n",
    "        (dest / breed).mkdir(parents=True, exist_ok=True)\n",
    "        dst = dest / breed / (name + \".jpg\")\n",
    "        try:\n",
    "            if USE_HARDLINKS:\n",
    "                if dst.exists(): dst.unlink()\n",
    "                os.link(src, dst)\n",
    "            else:\n",
    "                shutil.copy2(src, dst)\n",
    "        except OSError:\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "        breeds.add(breed); count += 1\n",
    "\n",
    "print(f\"âœ… dogs images: {count}, breeds: {len(breeds)}\")\n",
    "print(\"ì˜ˆì‹œ í’ˆì¢… 10ê°œ:\", sorted(list(breeds))[:10])\n",
    "print(\"Oxford root:\", root)\n",
    "print(\"Export dir:\", dest.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5573f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /Users/suakim/Documents/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/flutter á„ƒá…µá†¸á„…á…¥á„‚á…µá†¼ á„‘á…³á†¯á„Œá…¦á†¨/Model/data\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "# Model í´ë”ì—ì„œ ì‘ì—… ì¤‘ì´ë©´ \"data\", ë£¨íŠ¸ë©´ \"Model/data\"\n",
    "candidates = [pathlib.Path(\"data\"), pathlib.Path(\"Model/data\")]\n",
    "for p in candidates:\n",
    "    if p.exists() and any(p.iterdir()):\n",
    "        DATA_DIR = p\n",
    "        break\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cfbb6",
   "metadata": {},
   "source": [
    "## í’ˆì¢…ë§ˆë‹¤ ê°¯ìˆ˜ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae85a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ì´ë¯¸ì§€: 4978\n",
      "í´ë˜ìŠ¤ ìˆ˜: 25\n",
      "ìƒìœ„ 10ê°œ: [('wheaten_terrier', 200), ('great_pyrenees', 200), ('american_bulldog', 200), ('pomeranian', 200), ('american_pit_bull_terrier', 200), ('yorkshire_terrier', 200), ('japanese_chin', 200), ('miniature_pinscher', 200), ('basset_hound', 200), ('saint_bernard', 200)]\n"
     ]
    }
   ],
   "source": [
    "import pathlib, collections\n",
    "root = DATA_DIR\n",
    "cnt = {d.name: len(list(d.glob(\"*.jpg\"))) for d in root.iterdir() if d.is_dir()}\n",
    "print(\"ì´ ì´ë¯¸ì§€:\", sum(cnt.values()))\n",
    "print(\"í´ë˜ìŠ¤ ìˆ˜:\", len(cnt))\n",
    "print(\"ìƒìœ„ 10ê°œ:\", sorted(cnt.items(), key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcedfc",
   "metadata": {},
   "source": [
    "## ì ì´ì œ ë”¥ëŸ¬ë‹ì„ ë‚´ê°€ ë§Œë“¤ì–´ë³´ì\n",
    "- ì„ ìƒë‹˜ì´ ì—†ì´ í˜¼ìí•˜ë ¤ë‹ˆê¹Œ ì •ë§ ì–´ë µë‹¤ ì¼ë‹¨ í•´ë³´ì \n",
    "- import tensorflow as tfê°€ ë­”ë°?\n",
    "- tensorflow: êµ¬ê¸€ì´ ë§Œë“  ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬.\n",
    "- ê± ë‚´ê°€ tensorflow ë§Œë“ ë‹¤ êµ¬ê¸€ë¯¸ì•ˆí•˜ë‹¤ ì‘¤ì•„ê°€ ê°„ë‹¤ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹\n",
    "- ssuatensorflow gogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdacc1",
   "metadata": {},
   "source": [
    "### ì—­ì „íŒŒë€? ë©€ê¹Œ?\n",
    "\n",
    "- ê²°ê³¼(ì†ì‹¤)ê°€ ì¡°ê¸ˆ ë°”ë€” ë•Œ, ê° ì…ë ¥/ê°€ì¤‘ì¹˜ê°€ ì–¼ë§ˆë‚˜ â€˜ì±…ì„â€™ì´ ìˆëŠ”ì§€ ê±°ê¾¸ë¡œ ê³„ì‚°í•´ ì£¼ëŠ” ì ˆì°¨\n",
    "- ìˆ˜í•™ ê·œì¹™ì´ ì—°ì‡„ë²•ì¹™(Chain Rule).\n",
    "- ì—¬ê¸°ì„œ ë§í•˜ëŠ” ì†ì‹¤ì€ (ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’)^ ì„ ì˜ë¯¸í•¨ (=MSE)\n",
    "- mseë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œëŠ” ê¸°ìš¸ê¸°ë¥¼ ì¤„ì—¬ì•¼ í•˜ë‹ˆê¹Œ ê·¸ë˜ì„œ ë¯¸ë¶„ì´ ë˜ì–´ì•¼ í•¨\n",
    "- ë¯¸ë¶„ì´ ë˜ì–´ì•¼ ë˜ê¸° ë•Œë¬¸ì— ì—°ì‡„ë²•ì¹™ì´ ì´ìš©ë¨\n",
    "- ì—°ì‡„ë²•ì¹™ì´ ì´ìš©ë˜ëŠ” ì´ìœ ëŠ” ëª¨ë¸ì˜ ì†ì‹¤ ğ¿, Lì€ ë³´í†µ ì—¬ëŸ¬ ì—°ì‚°ì´ ê²¹ê²¹ì´ í•©ì„±ëœ í•¨ìˆ˜\n",
    "- ì´ë•Œ dy/dxëŠ” ë°”ë¡œ ëª» êµ¬í•˜ë‹ˆê¹Œ ì¡°ê°ì¡°ê° ë‚´ì„œ ê³±í•´ì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ë¨\n",
    "- ê·¸ë˜ì„œ ê²°ë¡ ì€ ì—­ì „íŒŒ = ì—°ì‡„ë²•ì¹™ ìœ¼ë¡œ ë³´ë©´ ë¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0976125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssuatensortflow/tensor.py ê°±ì‹  ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "code = r'''\n",
    "# ssuatensortflow/tensor.py\n",
    "# class Tensor : ìë™ë¯¸ë¶„ ì½”ì–´\n",
    "# ------------------------------------------------------------\n",
    "# \"ê°’(data) + ê·¸ë¼ë””ì–¸íŠ¸(grad) + ì—°ì‚°ê·¸ë˜í”„(parent ì—°ê²°) + ì—­ì „íŒŒ ê·œì¹™(_backward)\"\n",
    "# ì„ ë¬¶ì–´ì„œ ë‹¤ë£¨ëŠ” ì•„ì£¼ ì‘ì€ ìë™ë¯¸ë¶„ ì—”ì§„ì˜ ì½”ì–´ì…ë‹ˆë‹¤.\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"\n",
    "    ìˆ«ì ë°°ì—´(np.ndarray)ì„ ê°ì‹¸ëŠ” í´ë˜ìŠ¤.\n",
    "    - data: ì‹¤ì œ ê°’ (float32 ë°°ì—´)\n",
    "    - requires_grad: ì´ í…ì„œì— ëŒ€í•´ grad(âˆ‚L/âˆ‚self)ë¥¼ ì¶”ì í• ì§€ ì—¬ë¶€\n",
    "    - grad: backward() í›„ ì±„ì›Œì§€ëŠ” ê·¸ë¼ë””ì–¸íŠ¸ ë²„í¼ (dataì™€ ê°™ì€ shape)\n",
    "    - _prev: ì´ í…ì„œë¥¼ ë§Œë“  ë¶€ëª¨ í…ì„œë“¤(ì—°ì‚° ê·¸ë˜í”„ ì—°ê²°ìš©)\n",
    "    - _op: ì–´ë–¤ ì—°ì‚°ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€(ë””ë²„ê¹…/ì‹œê°í™”ìš© ë¼ë²¨)\n",
    "    - _backward: ì´ ë…¸ë“œì—ì„œ ë¶€ëª¨ë¡œ gradë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜(ì—°ì‚°ë³„ ê·œì¹™ ì €ì¥)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, requires_grad=False, _children=(), _op=\"\"):\n",
    "        # 1. ê°’ì„ float32ë¡œ í†µì¼(ì—°ì‚° ì¼ê´€ì„±/ì†ë„)\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "\n",
    "        # 2. ì´ í…ì„œì— ëŒ€í•´ ë¯¸ë¶„ì„ ì¶”ì í• ì§€\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        # 3. grad ë²„í¼: ì¶”ì í•  ë•Œë§Œ 0ìœ¼ë¡œ ì´ˆê¸°í™”, ì•„ë‹ˆë©´ None\n",
    "        self.grad = np.zeros_like(self.data) if requires_grad else None\n",
    "\n",
    "        # 4. ë¶€ëª¨ ì—°ê²°: ì—°ì‚°ì´ ê²°ê³¼ í…ì„œë¥¼ ë§Œë“¤ ë•Œ (ë¶€ëª¨ë“¤)ì„ ë„˜ê²¨ì¤Œ\n",
    "        self._prev = set(_children)\n",
    "\n",
    "        # 5. ì–´ë–¤ ì—°ì‚°ì˜ ê²°ê³¼ì¸ì§€ ë¼ë²¨(ë””ë²„ê¹…ìš©)\n",
    "        self._op = _op\n",
    "\n",
    "        # 6. ì—­ì „íŒŒ ê·œì¹™ ìŠ¬ë¡¯: ê° ì—°ì‚°ì´ outì„ ë§Œë“¤ ë•Œ ì—¬ê¸°ì— ê·œì¹™ì„ ì£¼ì…\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_tensor(x):\n",
    "        \"\"\"ìŠ¤ì¹¼ë¼/ndarrayë„ Tensorë¡œ í†µì¼í•´ì„œ ì—°ì‚°ì„ ë‹¨ìˆœí™”.\"\"\"\n",
    "        return x if isinstance(x, Tensor) else Tensor(x)\n",
    "\n",
    "    # -------------------- ê¸°ë³¸ ì—°ì‚°ë“¤ --------------------\n",
    "    # +\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        ë§ì…ˆ: z = x + y\n",
    "        grad ê·œì¹™: âˆ‚z/âˆ‚x = 1, âˆ‚z/âˆ‚y = 1  â†’ ë¶€ëª¨ì˜ gradì— out.gradë¥¼ ê·¸ëŒ€ë¡œ ë”í•¨\n",
    "        \"\"\"\n",
    "        other = Tensor._to_tensor(other)\n",
    "        out = Tensor(self.data + other.data,\n",
    "                     requires_grad=self.requires_grad or other.requires_grad,\n",
    "                     _children=(self, other), _op=\"+\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad\n",
    "            if other.requires_grad:\n",
    "                other.grad = other.grad + out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # êµí™˜ë²•ì¹™ì„ ê·¸ë‹ˆê¹Œ ë°˜ëŒ€ë¡œ ë“¤ì–´ì˜¤ëŠ”ê±° ìë¦¬ ë°”ê¿”ì„œ ì¸ì‹ í•œë‹¤ê³  \n",
    "    #  3 + x ==> x + 3 ìš”ëŸ° ëŠë‚Œì“° ã…‡ã…‹?\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"\n",
    "        ì „ë¶€í•©: s = x.sum()  (ìŠ¤ì¹¼ë¼)\n",
    "        grad ê·œì¹™: âˆ‚s/âˆ‚x = 1(ê°™ì€ shape) â†’ out.grad * 1 ì„ ë¶€ëª¨ë¡œ ë³´ëƒ„\n",
    "        \"\"\"\n",
    "        out = Tensor(self.data.sum(),\n",
    "                     requires_grad=self.requires_grad,\n",
    "                     _children=(self,), _op=\"sum\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + np.ones_like(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # ë¶€í˜¸ë°˜ì „: y = -x\n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        grad ê·œì¹™: âˆ‚y/âˆ‚x = -1 â†’ ë¶€ëª¨ gradì— -out.grad ë”í•˜ê¸°\n",
    "        \"\"\"\n",
    "        out = Tensor(-self.data, requires_grad=self.requires_grad, _children=(self,), _op=\"neg\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad - out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # - ëº„ì…ˆ: x - y = x + (-y)\n",
    "    def __sub__(self, other):\n",
    "        \n",
    "        other = Tensor._to_tensor(other)\n",
    "        return self.__add__(-other)\n",
    "    # ìë¦¬ ë¹ ê¾¼ - \n",
    "    def __rsub__(self, other):\n",
    "        other = Tensor._to_tensor(other)\n",
    "        return other.__sub__(self)\n",
    "    #  ì›ì†Œê³±: z = x * y\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        grad ê·œì¹™: âˆ‚z/âˆ‚x = y, âˆ‚z/âˆ‚y = x â†’ ë¶€ëª¨ gradì— ìƒëŒ€ë°© dataë¥¼ ê³±í•´ ëˆ„ì \n",
    "        \"\"\"\n",
    "        other = Tensor._to_tensor(other)\n",
    "        out = Tensor(self.data * other.data,\n",
    "                     requires_grad=self.requires_grad or other.requires_grad,\n",
    "                     _children=(self, other), _op=\"*\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad * other.data\n",
    "            if other.requires_grad:\n",
    "                other.grad = other.grad + out.grad * self.data\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # ìë¦¬ ë°”ê¾¼ ê³±ê³±\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "\n",
    "    # í‰ê· : m = x.mean() (ìŠ¤ì¹¼ë¼)\n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        grad ê·œì¹™: âˆ‚m/âˆ‚x = 1/N â†’ ones_like(x) * (out.grad/N)\n",
    "        \"\"\"\n",
    "        n = self.data.size\n",
    "        out = Tensor(self.data.mean(),\n",
    "                     requires_grad=self.requires_grad,\n",
    "                     _children=(self,), _op=\"mean\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + (np.ones_like(self.data) * (out.grad / n))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    #  ê±°ë“­ì œê³±: y = x ** p  (pëŠ” ìŠ¤ì¹¼ë¼)\n",
    "    def pow(self, p: float):\n",
    "        \"\"\"\n",
    "        grad ê·œì¹™: âˆ‚y/âˆ‚x = p * x^(p-1)\n",
    "        \"\"\"\n",
    "        out = Tensor(self.data ** p,\n",
    "                     requires_grad=self.requires_grad,\n",
    "                     _children=(self,), _op=f\"pow{p}\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad * (p * (self.data ** (p - 1)))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        í–‰ë ¬ê³±: Z = A @ B  (ë‹¨ìˆœí™”: 2D @ 2Dë§Œ ì§€ì›)\n",
    "        grad ê·œì¹™:\n",
    "          dL/dA = dL/dZ @ B^T\n",
    "          dL/dB = A^T @ dL/dZ\n",
    "        \"\"\"\n",
    "        other = Tensor._to_tensor(other)\n",
    "        A, B = self.data, other.data\n",
    "        out = Tensor(A @ B,\n",
    "                     requires_grad=self.requires_grad or other.requires_grad,\n",
    "                     _children=(self, other), _op=\"@\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad @ B.T\n",
    "            if other.requires_grad:\n",
    "                other.grad = other.grad + A.T @ out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # ì™œ ìŒìˆ˜ê°’ì€ ë±‰ì–´ë‚´ê³  ì–‘ìˆ˜ ê°’ë§Œ ë°›ì„ê¹Œ\n",
    "    # z > 0 : â€œê·¸ íŠ¹ì§•ì´ ìˆë‹¤ê³  íŒë‹¨â€ â†’ ê°’ì„ ê·¸ëŒ€ë¡œ ë‹¤ìŒ ì¸µì— ì „ë‹¬\n",
    "    # z â‰¤ 0 : â€œê·¸ íŠ¹ì§•ì´ ì—†ë‹¤/ì•½í•˜ë‹¤â€ â†’ 0(ì¹¨ë¬µ) ìœ¼ë¡œ ë³´ëƒ„\n",
    "    # íŠ¹ì§•ì´ ìˆëƒ ì—†ëƒë¥¼ ë³´ê³  íŒë‹¨ í•˜ëŠ” í•¨ìˆ˜ë¼ì„œ ê·¸ëŸ°ê±°ì„ ã…‡ã…‡\n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        ReLU: y = max(0, x)\n",
    "        grad ê·œì¹™: âˆ‚y/âˆ‚x = 1(x>0), else 0  â†’ ë§ˆìŠ¤í¬ë¡œ ê³±í•´ ì „ë‹¬\n",
    "        \"\"\"\n",
    "        mask = (self.data > 0).astype(np.float32)\n",
    "        out = Tensor(self.data * mask,\n",
    "                     requires_grad=self.requires_grad,\n",
    "                     _children=(self,), _op=\"relu\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad * mask\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # -------------------- ì—­ì „íŒŒ ì—”ì§„ --------------------\n",
    "    # ì—°ì‡„ë²•ì¹™(Chain Rule)ì„ ì´ìš©í•´, í˜„ì¬ ê³„ì‚° ê·¸ë˜í”„ì˜ ëª¨ë“  í…ì„œì— ëŒ€í•œ ê¸°ìš¸ê¸°(gradient)ë¥¼ ìë™ìœ¼ë¡œ ê³„ì‚°í•´ì„œ Tensor.gradì— ì±„ì›Œë„£ëŠ” í•¨ìˆ˜\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        loss(ë³´í†µ ìŠ¤ì¹¼ë¼)ì—ì„œ ì‹œì‘í•´ ë¶€ëª¨ ë°©í–¥ìœ¼ë¡œ gradë¥¼ ì „íŒŒ.\n",
    "        # ì ˆì°¨:\n",
    "        # 1) ìœ„ìƒì •ë ¬\n",
    "        # 2) ëª¨ë“  grad 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "        # 3) ë£¨íŠ¸(self)ì˜ grad = 1 ì„¤ì • (dL/dL = 1)\n",
    "        # 4) ë¦¬ìŠ¤íŠ¸ë¥¼ ë’¤ì§‘ì–´ ê° ë…¸ë“œì˜ _backward() ì‹¤í–‰ â†’ ë¶€ëª¨ grad ëˆ„ì \n",
    "        \"\"\"\n",
    "        # 1. ê·¸ë˜í”„ ìˆœì„œ ë§Œë“¤ê¸°  ìœ„ìƒì •ë ¬(DFS)\n",
    "        #    ìì‹ì´ ë’¤, ë¶€ëª¨ê°€ ì•\n",
    "        topo, visited = [], set()\n",
    "        def build(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for p in v._prev:\n",
    "                    build(p)\n",
    "                topo.append(v)\n",
    "        build(self)\n",
    "\n",
    "        # 2. ì „íŒŒëŠ” ëˆ„ì (+=) ì´ë¯€ë¡œ, ì´ì „ ê°’ì´ ë‚¨ì•„ìˆì§€ ì•Šê²Œ 0ì—ì„œ ì‹œì‘.\n",
    "        for v in topo:\n",
    "            if v.requires_grad:\n",
    "                if v.grad is None:\n",
    "                    v.grad = np.zeros_like(v.data)\n",
    "                else:\n",
    "                    v.grad[...] = 0.0\n",
    "\n",
    "        # (3) ë£¨íŠ¸ grad = 1 (lossì—ì„œ ì‹œì‘)\n",
    "        if self.grad is None:\n",
    "            self.grad = np.ones_like(self.data)\n",
    "        else:\n",
    "            self.grad[...] = 1.0\n",
    "\n",
    "        # (4) ìì‹ - >ë¶€ëª¨ë¡œ ì—­ì „íŒŒ\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "'''\n",
    "Path(\"ssuatensortflow/tensor.py\").write_text(code, encoding=\"utf-8\")\n",
    "print(\"ssuatensortflow/tensor.py ê°±ì‹  ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334655ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ssuatensortflow/nn.py ê°±ì‹  ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "code = r'''\n",
    "# ssuatensortflow/nn.py\n",
    "# ì†ì‹¤ í•¨ìˆ˜\n",
    "# ------------------------------------------------------------\n",
    "# ë¶„ë¥˜ìš© ì†ì‹¤: ì†Œí”„íŠ¸ë§¥ìŠ¤ + í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼(ìˆ˜ì¹˜ì•ˆì •í˜•)\n",
    "# ë‹¤ì¤‘ë¶„ë¥˜(multi-class classification) ë¬¸ì œì—ì„œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤(loss) í•¨ìˆ˜\n",
    "# logits: (B, C) Tensor, y_int: (B,) int64 numpy array\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "from .tensor import Tensor\n",
    "\n",
    "def softmax_cross_entropy(logits: Tensor, y_int: np.ndarray) -> Tensor:\n",
    "    \"\"\"\n",
    "    ì•ˆì •í˜• softmax:\n",
    "    # ì•½ê°„ softmax = ê°’ì„ í™•ë¥ ë¡œ ë°”ê¿”ì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ë¨ \n",
    "    # ëª¨ë“  í´ë˜ìŠ¤ì˜ í•©ì€ 1ì´ë‹¤\n",
    "    # croos_sentropy = ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ë¶ˆì¼ì¹˜ ì •ë„ë¼ê³  ë³´ë©´ ë˜ëŠ”ë° \n",
    "    # ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ ë†’ì´ê³ , ì˜¤ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ ë‚®ì¶”ë„ë¡\n",
    "      z = logits - max(logits, axis=1)\n",
    "      probs = exp(z) / sum(exp(z), axis=1)\n",
    "\n",
    "    êµì°¨ì—”íŠ¸ë¡œí”¼:\n",
    "      L = - í‰ê· ( log( probs[range(B), y] ) )\n",
    "\n",
    "    ì—­ì „íŒŒ ê·œì¹™(ìœ ë„ ê²°ê³¼):\n",
    "      dL/dlogits = (probs - onehot(y)) / B\n",
    "    \"\"\"\n",
    "    assert logits.data.ndim == 2, \"logits must be (B, C)\"\n",
    "    B, C = logits.data.shape\n",
    "    y_int = y_int.astype(np.int64)\n",
    "\n",
    "    # (1) ì•ˆì •í˜• softmax\n",
    "    z = logits.data - logits.data.max(axis=1, keepdims=True)  # overflow ë°©ì§€\n",
    "    exp = np.exp(z)\n",
    "    probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # (2) cross-entropy\n",
    "    eps = 1e-12\n",
    "    loss_val = -np.mean(np.log(probs[np.arange(B), y_int] + eps))\n",
    "\n",
    "    # (3) ê²°ê³¼ í…ì„œ(ìŠ¤ì¹¼ë¼)\n",
    "    out = Tensor(loss_val, requires_grad=logits.requires_grad,\n",
    "                 _children=(logits,), _op=\"softmax_ce\")\n",
    "\n",
    "    # (4) ì—­ì „íŒŒ: dL/dlogits = (probs - onehot)/B\n",
    "    y_onehot = np.eye(C, dtype=np.float32)[y_int]\n",
    "    def _backward():\n",
    "        if logits.requires_grad:\n",
    "            grad_logits = (probs - y_onehot) / B\n",
    "            logits.grad = logits.grad + grad_logits\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "'''\n",
    "Path(\"ssuatensortflow/nn.py\").write_text(code, encoding=\"utf-8\")\n",
    "print(\" ssuatensortflow/nn.py ê°±ì‹  ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "79b2d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# ssuatensortflow/layers.py\n",
    "# ------------------------------------------------------------\n",
    "# ê°„ë‹¨ ë ˆì´ì–´/ì˜µí‹°ë§ˆì´ì € (ê°œì„ íŒ):\n",
    "#   - add_bias: (B,F) + (1,F) ì „ìš© ë§ì…ˆ(ë¸Œë¡œë“œìºìŠ¤íŒ… ì—†ì´ ì—­ì „íŒŒ ì•ˆì „)\n",
    "#   - dropout(x,p,train): í•¨ìˆ˜í˜• ë“œë¡­ì•„ì›ƒ (inverted ë°©ì‹)\n",
    "#   - Linear(in,out, init=...): Xavier/He ì´ˆê¸°í™” ì„ íƒ\n",
    "#   - SGD: momentum / weight_decay(L2) / grad clipping ì§€ì›\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "code = r'''\n",
    "# Tensor ì„í¬íŠ¸ (ë…¸íŠ¸ë¶/ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë§¥ë½ ëª¨ë‘ ì§€ì›)\n",
    "import numpy as np\n",
    "from .tensor import Tensor\n",
    "\n",
    "\n",
    "# ======= ìœ í‹¸: fan_in/out & ì´ˆê¸°í™” =======\n",
    "\n",
    "def _fan_in_out(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    else:\n",
    "        # Conv ë“± ê³ ë ¤í•  ë•Œì˜ ì¼ë°˜ì‹(ì—¬ê¸´ 2Dë§Œ ì“°ì§€ë§Œ ì•ˆì „í•˜ê²Œ)\n",
    "        fan_in = shape[0] if len(shape) > 0 else 1\n",
    "        fan_out = shape[1] if len(shape) > 1 else 1\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _xavier_uniform(shape, rng):\n",
    "    fan_in, fan_out = _fan_in_out(shape)\n",
    "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return rng.uniform(-limit, limit, size=shape).astype(np.float32)\n",
    "\n",
    "def _he_uniform(shape, rng):\n",
    "    fan_in, _ = _fan_in_out(shape)\n",
    "    limit = np.sqrt(6.0 / max(1, fan_in))\n",
    "    return rng.uniform(-limit, limit, size=shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# ======= Op: add_bias =======\n",
    "\n",
    "def add_bias(z: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    (B,F) + (1,F) í˜•íƒœì˜ í¸í–¥ ë§ì…ˆì„ ì „ìš© opë¡œ êµ¬í˜„.\n",
    "    ì—­ì „íŒŒ ì‹œ b.gradëŠ” ë°°ì¹˜ ì¶•ìœ¼ë¡œ í•©ì‚°í•´ (1,F) ëª¨ì–‘ìœ¼ë¡œ ëˆ„ì .\n",
    "    \"\"\"\n",
    "    out = Tensor(z.data + b.data,\n",
    "                 requires_grad=z.requires_grad or b.requires_grad,\n",
    "                 _children=(z, b), _op=\"add_bias\")\n",
    "    def _backward():\n",
    "        if z.requires_grad:\n",
    "            if z.grad is None:\n",
    "                z.grad = np.zeros_like(z.data, dtype=z.data.dtype)\n",
    "            z.grad = z.grad + out.grad\n",
    "        if (b is not None) and b.requires_grad:\n",
    "            if b.grad is None:\n",
    "                b.grad = np.zeros_like(b.data, dtype=b.data.dtype)\n",
    "            b.grad = b.grad + out.grad.sum(axis=0, keepdims=True)\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======= Op: dropout (inverted) =======\n",
    "\n",
    "def dropout(x: Tensor, p: float = 0.5, train: bool = True) -> Tensor:\n",
    "    \"\"\"\n",
    "    í•¨ìˆ˜í˜• ë“œë¡­ì•„ì›ƒ (í•™ìŠµì‹œë§Œ ì ìš©).\n",
    "    inverted dropout: í•™ìŠµ ì‹œ 1/(1-p)ë¡œ ìŠ¤ì¼€ì¼ â†’ ì¶”ë¡  ì‹œ ë³„ë„ ë³´ì • ë¶ˆí•„ìš”.\n",
    "    \"\"\"\n",
    "    assert 0.0 <= p < 1.0, \"dropout p must be in [0,1)\"\n",
    "    if (not train) or p == 0.0:\n",
    "        # ì¶”ë¡  ëª¨ë“œ ë˜ëŠ” p=0: ê·¸ë˜í”„ëŠ” ìœ ì§€\n",
    "        out = Tensor(x.data.copy(), requires_grad=x.requires_grad, _children=(x,), _op=\"dropout_pass\")\n",
    "        def _backward():\n",
    "            if x.requires_grad:\n",
    "                if x.grad is None:\n",
    "                    x.grad = np.zeros_like(x.data, dtype=x.data.dtype)\n",
    "                x.grad = x.grad + out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    keep = 1.0 - p\n",
    "    rng = np.random.default_rng()\n",
    "    mask = (rng.random(x.data.shape) < keep).astype(x.data.dtype)\n",
    "    y_data = (x.data * mask) / keep\n",
    "\n",
    "    out = Tensor(y_data, requires_grad=x.requires_grad, _children=(x,), _op=\"dropout\")\n",
    "    def _backward():\n",
    "        if x.requires_grad:\n",
    "            if x.grad is None:\n",
    "                x.grad = np.zeros_like(x.data, dtype=x.data.dtype)\n",
    "            x.grad = x.grad + (out.grad * mask) / keep\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======= Layer: Linear =======\n",
    "\n",
    "class Linear:\n",
    "    \"\"\"\n",
    "    ì™„ì „ì—°ê²°ì¸µ: y = x @ W (+ b)\n",
    "      - W: (in_features, out_features)\n",
    "      - b: (1, out_features)\n",
    "    init:\n",
    "      - 'xavier' (ê¸°ë³¸)\n",
    "      - 'he'     (ReLU ê³„ì—´ì—ì„œ ê¶Œì¥)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int,\n",
    "                 bias: bool = True, rng=None, init: str = \"xavier\"):\n",
    "        rng = np.random.default_rng() if rng is None else rng\n",
    "        shape = (in_features, out_features)\n",
    "        init = (init or \"xavier\").lower()\n",
    "\n",
    "        if init == \"he\":\n",
    "            W_init = _he_uniform(shape, rng)\n",
    "        else:\n",
    "            W_init = _xavier_uniform(shape, rng)\n",
    "\n",
    "        self.W = Tensor(W_init, requires_grad=True)\n",
    "        self.b = Tensor(np.zeros((1, out_features), np.float32), requires_grad=True) if bias else None\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸(W, b).\"\"\"\n",
    "        return [p for p in (self.W, self.b) if p is not None]\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"ìˆœì „íŒŒ: y = x @ W (+ b)\"\"\"\n",
    "        y = x @ self.W\n",
    "        if self.b is not None:\n",
    "            y = add_bias(y, self.b)\n",
    "        return y\n",
    "\n",
    "\n",
    "# ======= Optimizer: SGD (momentum / L2 / grad clip) =======\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    í™•ì¥ SGD:\n",
    "      - momentum: v = m*v - lr*g ; p += v\n",
    "      - weight_decay: g <- g + wd * p.data (L2 regularization)\n",
    "      - max_norm: ì „ì²´ grad L2ë¥¼ max_normìœ¼ë¡œ í´ë¦½\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.1, momentum=0.0, weight_decay=0.0, max_norm=None):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_norm = max_norm\n",
    "        self._vel = {id(p): np.zeros_like(p.data, dtype=p.data.dtype) for p in self.params}\n",
    "\n",
    "    def _apply_weight_decay(self, p, g):\n",
    "        if self.weight_decay != 0.0:\n",
    "            g = g + self.weight_decay * p.data\n",
    "        return g\n",
    "\n",
    "    def _clip_grad_norm_(self):\n",
    "        if self.max_norm is None:\n",
    "            return\n",
    "        total_sq = 0.0\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                total_sq += float(np.sum(p.grad * p.grad))\n",
    "        total_norm = np.sqrt(total_sq)\n",
    "        if total_norm > self.max_norm and total_norm > 0:\n",
    "            scale = self.max_norm / (total_norm + 1e-12)\n",
    "            for p in self.params:\n",
    "                if p.grad is not None:\n",
    "                    p.grad *= scale\n",
    "\n",
    "    def step(self):\n",
    "        # ê·¸ë¼ë“œ í´ë¦¬í•‘(ì˜µì…˜)\n",
    "        self._clip_grad_norm_()\n",
    "\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = self._apply_weight_decay(p, p.grad)\n",
    "\n",
    "            if self.momentum != 0.0:\n",
    "                v = self._vel[id(p)]\n",
    "                v[:] = self.momentum * v - self.lr * g\n",
    "                p.data += v\n",
    "            else:\n",
    "                p.data -= self.lr * g\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad[...] = 0.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711fa32",
   "metadata": {},
   "source": [
    "# importlib ì“°ëŠ” ì´ìœ  \n",
    "- ë…¸íŠ¸ë¶ì—ì„œ ìš°ë¦¬ê°€ ssuatensortflow/tensor.py íŒŒì¼ì„ ê³ ì³ë„ ì´í•´ë¥¼ ëª» í•  ìˆ˜ê°€ ìˆìœ¼ë‹ˆê¹Œ ë¦¬ë¡œë“œ í• ë ¤ê³  ì“°ì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fc431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.data: [11. 22. 33.]\n",
      "s.data: 66.0\n",
      "x.grad: [1. 1. 1.]\n",
      "y.grad: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import importlib, ssuatensortflow as stf\n",
    "# í•¨ìˆ˜ ë°”ë€ê±° ìˆìœ¼ë©´ ë¡œë“œí•´ë¼\n",
    "importlib.reload(stf ) # ë‚´ê°€ ë§Œë“¤ì–´ ë‘” stf ì•½ì–´ ã…‡ã…‡ \n",
    "\n",
    "\n",
    "x = stf.Tensor([1,2,3], requires_grad=True)\n",
    "y = stf.Tensor([10,20,30], requires_grad=True)\n",
    "\n",
    "z = x + y      # [11, 22, 33]\n",
    "s = z.sum()    # 66 (ìŠ¤ì¹¼ë¼)\n",
    "s.backward()\n",
    "\n",
    "print(\"z.data:\", z.data)   # [11. 22. 33.]\n",
    "print(\"s.data:\", s.data)   # 66.0\n",
    "print(\"x.grad:\", x.grad)   # [1. 1. 1.]\n",
    "print(\"y.grad:\", y.grad)   # [1. 1. 1.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49820d27",
   "metadata": {},
   "source": [
    "íŒŒì´ì¬ì€ x + yë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ x.__add__(y) ë¡œ ë°”ê¿” í˜¸ì¶œ\n",
    "\n",
    "ìš°ë¦¬ Tensor.__add__ ë©”ì„œë“œ ì•ˆì—ì„œ ì§ì ‘ ìƒˆ í…ì„œë¥¼ ë§Œë“¤ ë•Œ _op=\"+\"ë¥¼ ë„£ì–´ì¤¬ì–´.\n",
    "\n",
    "ê·¸ë˜ì„œ ê²°ê³¼ í…ì„œì˜ _opê°€ \"+\"ê°€ ë¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db300704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x._op: \n",
      "z._op: +\n",
      "s._op: sum\n"
     ]
    }
   ],
   "source": [
    "import importlib, ssuatensortflow as stf\n",
    "importlib.reload(stf)\n",
    "\n",
    "x = stf.Tensor([1,2,3], requires_grad=True)   # ë¦¬í”„: _op == \"\"\n",
    "y = stf.Tensor([10,20,30], requires_grad=True)\n",
    "\n",
    "z = x + y     # z._op == \"+\"\n",
    "s = z.sum()   # s._op == \"sum\"\n",
    "\n",
    "print(\"x._op:\", x._op)  # \"\"\n",
    "print(\"z._op:\", z._op)  # \"+\"\n",
    "print(\"s._op:\", s._op)  # \"sum\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb3fba",
   "metadata": {},
   "source": [
    "### ëª¨ë“ˆ ë¦¬ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a5a06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, numpy as np, pathlib, random\n",
    "from PIL import Image\n",
    "import ssuatensortflow as stf\n",
    "import ssuatensortflow.nn as nn\n",
    "import ssuatensortflow.layers as L\n",
    "importlib.reload(stf); importlib.reload(nn); importlib.reload(L)\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "assert DATA_DIR.exists(), DATA_DIR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55a65159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²½ë¡œ: /Users/suakim/Documents/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/flutter á„ƒá…µá†¸á„…á…¥á„‚á…µá†¼ á„‘á…³á†¯á„Œá…¦á†¨/Model/data\n",
      "exists? True\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "print(\"ê²½ë¡œ:\", DATA_DIR.resolve())\n",
    "print(\"exists?\", DATA_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1801f6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í´ë˜ìŠ¤ ìˆ˜: 25\n",
      "ì˜ˆì‹œ í’ˆì¢… 5ê°œ: ['american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer']\n",
      "ì´ ì´ë¯¸ì§€ ê°œìˆ˜: 4978\n",
      "Train: 3980, Val: 998\n",
      "ì…ë ¥ ì°¨ì›: 4096\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "IMG_SIZE = 64  # í•œ ì´ë¯¸ì§€ë¥¼ 64x64ë¡œ ì¶•ì†Œ\n",
    "\n",
    "# (1) í’ˆì¢…ë³„ í´ë” í™•ì¸\n",
    "breeds = sorted([p.name for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "breed_to_idx = {b:i for i,b in enumerate(breeds)}\n",
    "num_classes = len(breeds)\n",
    "print(f\"í´ë˜ìŠ¤ ìˆ˜: {num_classes}\")\n",
    "print(\"ì˜ˆì‹œ í’ˆì¢… 5ê°œ:\", breeds[:5])\n",
    "\n",
    "# (2) ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘\n",
    "files = []\n",
    "for b in breeds:\n",
    "    for jpg in (DATA_DIR/b).glob(\"*.jpg\"):\n",
    "        files.append((jpg, breed_to_idx[b]))\n",
    "print(f\"ì´ ì´ë¯¸ì§€ ê°œìˆ˜: {len(files)}\")\n",
    "\n",
    "# (3) train/val 8:2 ë¶„í• \n",
    "random.seed(0)\n",
    "by_class = defaultdict(list)\n",
    "for fp, y in files:\n",
    "    by_class[y].append(fp)\n",
    "\n",
    "train, val = [], []\n",
    "for y, fps in by_class.items():\n",
    "    random.shuffle(fps)\n",
    "    k = int(len(fps)*0.8)\n",
    "    train += [(fp, y) for fp in fps[:k]]\n",
    "    val   += [(fp, y) for fp in fps[k:]]\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}\")\n",
    "\n",
    "# (4) ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•´ numpyë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "# í‘ë°±ìœ¼ë¡œ ë°”ê¾¸ëŠ” ê±° \n",
    "# def load_img_as_vec(path, size=IMG_SIZE):\n",
    "#     im = Image.open(path).convert(\"L\").resize((size, size))\n",
    "#     arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "#     return arr.reshape(-1)  # (D,)\n",
    "#D = IMG_SIZE * IMG_SIZE\n",
    "\n",
    "def load_img_as_vec(path, size=IMG_SIZE):\n",
    "    im = Image.open(path).convert(\"RGB\").resize((size, size))\n",
    "    arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "    return arr.reshape(-1)  # (D,)\n",
    "\n",
    "\n",
    "print(f\"ì…ë ¥ ì°¨ì›: {D}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5349cf",
   "metadata": {},
   "source": [
    "## ì „ì²´ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7a14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(pairs, batch_size=64, shuffle=True):\n",
    "    idxs = np.arange(len(pairs))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idxs)\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        j = idxs[i:i+batch_size]\n",
    "        xs = [load_img_as_vec(pairs[k][0]) for k in j]\n",
    "        ys = [pairs[k][1] for k in j]\n",
    "        X = np.stack(xs).astype(np.float32)     \n",
    "        y = np.array(ys, dtype=np.int64)        \n",
    "        yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61389369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ssuatensortflow.tensor as T\n",
    "import ssuatensortflow.layers as L\n",
    "import ssuatensortflow.nn as nn\n",
    "import ssuatensortflow as stf\n",
    "\n",
    "# 1) ëª¨ë“ˆì„ 'tensor -> layers -> nn -> stf' ìˆœìœ¼ë¡œ ë¦¬ë¡œë“œ\n",
    "importlib.reload(T)\n",
    "importlib.reload(L)\n",
    "importlib.reload(nn)\n",
    "importlib.reload(stf)\n",
    "\n",
    "# 2) (ì¤‘ìš”) ê¸°ì¡´ ëª¨ë¸/ì˜µí‹°ë§ˆì´ì € ê°ì²´ ë²„ë¦¬ê³  ìƒˆë¡œ ìƒì„±\n",
    "# í‘ë°±ì¼ ë•Œ ì°¨ì›ì´ ì ì–´ì„œ \n",
    "# D = IMG_SIZE * IMG_SIZE         # ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜í–ˆìŒ(64x64=4096)\n",
    "# fc1 = L.Linear(D, 256, bias=True)\n",
    "# fc2 = L.Linear(256, len(breeds), bias=True)\n",
    "\n",
    "# ì¹¼ë¼ë¡œ ë°”ê¾¸ê³  ì°¨ì›ì´ ëŠ˜ì—ˆìŒ \n",
    "D = IMG_SIZE * IMG_SIZE * 3  # ì´ì œ 3ì±„ë„\n",
    "fc1 = L.Linear(D, 1024)       # ì°¨ì› ëŠ˜ì–´ë‚˜ì„œ ì€ë‹‰ì¸µë„ ì¡°ê¸ˆ í¬ê²Œ!\n",
    "fc2 = L.Linear(1024, num_classes)\n",
    "\n",
    "\n",
    "def forward(X_np):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)  # ì´ì œ 'ìƒˆ Tensor' í´ë˜ìŠ¤\n",
    "    h = fc1(x).relu()\n",
    "    logits = fc2(h)\n",
    "    return logits\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(), lr=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "658629e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] loss=4.6506\n",
      "[02] loss=3.1939\n",
      "[03] loss=3.2089\n",
      "[04] loss=3.1773\n",
      "[05] loss=3.1771\n",
      "[06] loss=3.1775\n",
      "[07] loss=3.2173\n",
      "[08] loss=3.1760\n",
      "[09] loss=3.1726\n",
      "[10] loss=3.2023\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH  = 64\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "    for X, y in make_batches(train, batch_size=BATCH, shuffle=True):\n",
    "        logits = forward(X)\n",
    "        loss   = nn.softmax_cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(float(loss.data))\n",
    "    print(f\"[{ep:02d}] loss={np.mean(losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b404fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ssuatensortflow.tensor as T\n",
    "import ssuatensortflow.layers as L\n",
    "import ssuatensortflow.nn as nn\n",
    "import ssuatensortflow as stf\n",
    "\n",
    "# 1) ëª¨ë“ˆì„ 'tensor -> layers -> nn -> stf' ìˆœìœ¼ë¡œ ë¦¬ë¡œë“œ\n",
    "importlib.reload(T)\n",
    "importlib.reload(L)\n",
    "importlib.reload(nn)\n",
    "importlib.reload(stf)\n",
    "\n",
    "# D = IMG_SIZE * IMG_SIZE * 3\n",
    "# num_classes = len(breeds)\n",
    "\n",
    "fc1 = L.Linear(D, 1024, init=\"he\")\n",
    "fc2 = L.Linear(1024, num_classes, init=\"xavier\")\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(),\n",
    "            lr=0.01, momentum=0.9, weight_decay=1e-4, max_norm=5.0)\n",
    "\n",
    "def forward(X_np, train=True):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x).relu()\n",
    "    h = L.dropout(h, p=0.3, train=train)  # í•™ìŠµ ì‹œë§Œ í™œì„±\n",
    "    logits = fc2(h)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2e964c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ, loss = 2.048614501953125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ssuatensortflow as stf\n",
    "import ssuatensortflow.layers as L\n",
    "import ssuatensortflow.nn as nn\n",
    "\n",
    "D, num_classes = 64*64*3, 10  # ì˜ˆì‹œìš©\n",
    "X_dummy = np.random.rand(8, D).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, num_classes, size=(8,))\n",
    "\n",
    "fc1 = L.Linear(D, 1024, init=\"he\")\n",
    "fc2 = L.Linear(1024, num_classes, init=\"xavier\")\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(),\n",
    "            lr=0.01, momentum=0.9, weight_decay=1e-4, max_norm=5.0)\n",
    "\n",
    "def forward(X_np, train=True):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x).relu()\n",
    "    h = L.dropout(h, p=0.3, train=train)\n",
    "    logits = fc2(h)\n",
    "    return logits\n",
    "\n",
    "# ìˆœì „íŒŒ + ì—­ì „íŒŒ í…ŒìŠ¤íŠ¸\n",
    "logits = forward(X_dummy)\n",
    "loss = nn.softmax_cross_entropy(logits, y_dummy)\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "print(\"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ, loss =\", float(loss.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb9c99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜¤ë²„í• í…ŒìŠ¤íŠ¸ ì „ìš©\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(), lr=0.05)  # momentum=0, wd=0\n",
    "def forward_overfit(X_np):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x).relu()\n",
    "    logits = fc2(h)  # dropout ë”\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "efc2ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] classes=25, train=3980, val=998, D=12288, mean=[0.48045987 0.45337072 0.39187315], std=[0.2551374  0.25123423 0.2606304 ]\n"
     ]
    }
   ],
   "source": [
    "# train_rgb_mlp.py  (1) ë°ì´í„° ì¤€ë¹„\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import numpy as np, random\n",
    "\n",
    "DATA_DIR = Path(\"data\")          # í’ˆì¢… í´ë”ë“¤ì´ ë“¤ì–´ìˆëŠ” ë£¨íŠ¸\n",
    "IMG_SIZE = 64\n",
    "random.seed(0); np.random.seed(0)\n",
    "\n",
    "# 1) í´ë˜ìŠ¤/ë¼ë²¨\n",
    "breeds = sorted([p.name for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "breed_to_idx = {b:i for i,b in enumerate(breeds)}\n",
    "num_classes = len(breeds)\n",
    "\n",
    "files = []\n",
    "for b in breeds:\n",
    "    for jpg in (DATA_DIR/b).glob(\"*.jpg\"):\n",
    "        files.append((jpg, breed_to_idx[b]))\n",
    "\n",
    "# 2) í´ë˜ìŠ¤ë³„ 8:2 ë¶„í• \n",
    "by_class = defaultdict(list)\n",
    "for fp, y in files: by_class[y].append(fp)\n",
    "\n",
    "train, val = [], []\n",
    "for y, fps in by_class.items():\n",
    "    random.shuffle(fps)\n",
    "    k = int(len(fps)*0.8)\n",
    "    train += [(fp, y) for fp in fps[:k]]\n",
    "    val   += [(fp, y) for fp in fps[k:]]\n",
    "\n",
    "# 3) ì±„ë„ë³„ mean/std ì¶”ì •\n",
    "def compute_mean_std(pairs, n_samples=1024):\n",
    "    sel = pairs if len(pairs)<=n_samples else random.sample(pairs, n_samples)\n",
    "    xs = []\n",
    "    for fp,_ in sel:\n",
    "        im = Image.open(fp).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "        arr = np.asarray(im, np.float32)/255.0   # (H,W,3)\n",
    "        xs.append(arr.reshape(-1,3))\n",
    "    X = np.concatenate(xs, axis=0)\n",
    "    mean = X.mean(axis=0); std = X.std(axis=0) + 1e-6\n",
    "    return mean, std\n",
    "\n",
    "mean3, std3 = compute_mean_std(train)\n",
    "\n",
    "# 4) ë¡œë” & ë¯¸ë‹ˆë°°ì¹˜\n",
    "def load_img_as_vec(path):\n",
    "    im = Image.open(path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR)\n",
    "    arr = np.asarray(im, np.float32)/255.0\n",
    "    arr = (arr - mean3)/std3\n",
    "    return arr.reshape(-1)  # (D,)\n",
    "\n",
    "def iterate_minibatches(pairs, batch_size=64, shuffle=True):\n",
    "    idx = np.arange(len(pairs))\n",
    "    if shuffle: np.random.shuffle(idx)\n",
    "    for s in range(0, len(pairs), batch_size):\n",
    "        sel = idx[s:s+batch_size]\n",
    "        Xb = np.stack([load_img_as_vec(pairs[i][0]) for i in sel], axis=0)\n",
    "        yb = np.array([pairs[i][1] for i in sel], dtype=np.int64)\n",
    "        yield Xb, yb\n",
    "\n",
    "D = IMG_SIZE*IMG_SIZE*3\n",
    "print(f\"[DATA] classes={num_classes}, train={len(train)}, val={len(val)}, D={D}, mean={mean3}, std={std3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "30369b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rgb_mlp.py  (2) ëª¨ë¸/ì˜µí‹°ë§ˆì´ì €/forward\n",
    "import importlib\n",
    "import ssuatensortflow as stf\n",
    "import ssuatensortflow.layers as L\n",
    "import ssuatensortflow.nn as nn\n",
    "\n",
    "importlib.reload(L); importlib.reload(nn)\n",
    "\n",
    "fc1 = L.Linear(D, 1024, init=\"he\")\n",
    "fc2 = L.Linear(1024, num_classes, init=\"xavier\")\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(),\n",
    "            lr=0.01, momentum=0.9, weight_decay=1e-4, max_norm=5.0)\n",
    "\n",
    "def forward(X_np, train=True):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x).relu()\n",
    "    h = L.dropout(h, p=0.3, train=train)   # í•™ìŠµ ë•Œë§Œ í™œì„±\n",
    "    logits = fc2(h)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9154e60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train_loss=4.1572 acc=0.093 | val_loss=3.8882 acc=0.122 | lr=0.01\n",
      "[02] train_loss=3.2960 acc=0.220 | val_loss=3.9292 acc=0.118 | lr=0.01\n",
      "[03] train_loss=2.6762 acc=0.312 | val_loss=4.0136 acc=0.142 | lr=0.01\n",
      "[04] train_loss=2.3449 acc=0.395 | val_loss=3.8690 acc=0.138 | lr=0.01\n",
      "[05] train_loss=1.9013 acc=0.470 | val_loss=4.1558 acc=0.144 | lr=0.01\n",
      "[06] train_loss=1.6769 acc=0.535 | val_loss=4.4493 acc=0.147 | lr=0.01\n",
      "[07] train_loss=1.4803 acc=0.591 | val_loss=4.4037 acc=0.152 | lr=0.01\n",
      "  â†³ plateau detected: reduce lr to 0.005\n",
      "[08] train_loss=0.9989 acc=0.712 | val_loss=4.0707 acc=0.156 | lr=0.005\n",
      "[09] train_loss=0.7182 acc=0.787 | val_loss=4.2060 acc=0.171 | lr=0.005\n",
      "[10] train_loss=0.6224 acc=0.831 | val_loss=4.2276 acc=0.161 | lr=0.005\n",
      "  â†³ plateau detected: reduce lr to 0.0025\n"
     ]
    }
   ],
   "source": [
    "# train_rgb_mlp.py  (3) í‰ê°€ + í•™ìŠµ ë£¨í”„\n",
    "def evaluate(pairs, batch_size=128):\n",
    "    losses, correct, total = [], 0, 0\n",
    "    for Xb, yb in iterate_minibatches(pairs, batch_size=batch_size, shuffle=False):\n",
    "        logits = forward(Xb, train=False)\n",
    "        loss = nn.softmax_cross_entropy(logits, yb)\n",
    "        losses.append(float(loss.data))\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        correct += int((pred == yb).sum())\n",
    "        total   += int(yb.size)\n",
    "    return (np.mean(losses) if losses else float(\"nan\")), (correct/total if total else 0.0)\n",
    "\n",
    "EPOCHS = 10\n",
    "best_val = float(\"inf\")\n",
    "patience, bad = 3, 0\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    # ---- Train ----\n",
    "    train_losses = []\n",
    "    train_correct, train_total = 0, 0\n",
    "\n",
    "    for Xb, yb in iterate_minibatches(train, batch_size=64, shuffle=True):\n",
    "        logits = forward(Xb, train=True)\n",
    "        loss = nn.softmax_cross_entropy(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        train_losses.append(float(loss.data))\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        train_correct += int((pred == yb).sum())\n",
    "        train_total   += int(yb.size)\n",
    "\n",
    "    tr_loss = np.mean(train_losses)\n",
    "    tr_acc  = train_correct / train_total\n",
    "\n",
    "    # ---- Val ----\n",
    "    val_loss, val_acc = evaluate(val)\n",
    "    print(f\"[{ep:02d}] train_loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.3f} | lr={opt.lr:g}\")\n",
    "\n",
    "    # plateau â†’ lr ë°˜ê°\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            opt.lr *= 0.5\n",
    "            bad = 0\n",
    "            print(f\"  â†³ plateau detected: reduce lr to {opt.lr:g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835a853",
   "metadata": {},
   "source": [
    "transforms: ë°ì´í„° ë‹¤ì–‘í™” â†’ ê³¼ì í•© ê°ì†Œ, ì¼ë°˜í™”â†‘\n",
    "\n",
    "BatchNorm: ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™” ê°ì†Œ â†’ ìˆ˜ë ´ ë¹¨ë¼ì§ + ì´ˆê¸°í™”/í•™ìŠµë¥ ì— ëœ ë¯¼ê°\n",
    "\n",
    "ReduceLROnPlateau: ë©ˆì¶”ë©´ ìë™ìœ¼ë¡œ lr ë‚®ì¶¤ â†’ ë” ë‚®ì€ ìµœì†Œì  íƒìƒ‰\n",
    "\n",
    "L2/Dropout ê°•í™”: í•™ìŠµì…‹ ì•”ê¸°(ì˜¤ë²„í•) ì–µì œ â†’ val ì„±ëŠ¥ ê°œì„ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839805ed",
   "metadata": {},
   "source": [
    "Tensor ì—°ì‚° êµ¬ì¡°\n",
    "\n",
    "Linear, BatchNorm1d, SGD ê°™ì€ ê¸°ë³¸ ë ˆì´ì–´ì™€ ì˜µí‹°ë§ˆì´ì €\n",
    "\n",
    "softmax_cross_entropy ì†ì‹¤\n",
    "\n",
    "DataLoader, Compose ë“± ë°ì´í„° íŒŒì´í”„ë¼ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "23d71997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssuatensortflow.transforms as Tfm\n",
    "import ssuatensortflow.utils as U\n",
    "import ssuatensortflow.schedulers as S\n",
    "U.seed_everything(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d5b04429",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = Tfm.Compose([\n",
    "    Tfm.RandomHorizontalFlip(p=0.5),\n",
    "    Tfm.RandomPadCrop(IMG_SIZE, pad=4),\n",
    "    Tfm.ToRGBResize(IMG_SIZE),\n",
    "    Tfm.Normalize01(mean3, std3)\n",
    "])\n",
    "val_tf = Tfm.Compose([\n",
    "    Tfm.ToRGBResize(IMG_SIZE),\n",
    "    Tfm.Normalize01(mean3, std3)\n",
    "])\n",
    "\n",
    "def load_img_as_vec(path, train_mode=False):\n",
    "    img = Image.open(path)\n",
    "    tfm = train_tf if train_mode else val_tf\n",
    "    arr = tfm(img)                 # (H,W,3)\n",
    "    return Tfm.to_vec(arr)         # (D,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2278f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(pairs, batch_size=64, shuffle=True, train_mode=False):\n",
    "    idx = np.arange(len(pairs))\n",
    "    if shuffle: np.random.shuffle(idx)\n",
    "    for s in range(0, len(pairs), batch_size):\n",
    "        sel = idx[s:s+batch_size]\n",
    "        Xb = np.stack([load_img_as_vec(pairs[i][0], train_mode=train_mode) for i in sel], axis=0)\n",
    "        yb = np.array([pairs[i][1] for i in sel], dtype=np.int64)\n",
    "        yield Xb, yb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a30cc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = L.Linear(D, 512, init=\"he\")\n",
    "bn1 = L.BatchNorm1d(512)          # â˜… ì¶”ê°€\n",
    "fc2 = L.Linear(512, num_classes, init=\"xavier\")\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + bn1.parameters() + fc2.parameters(),\n",
    "            lr=0.005, momentum=0.9, weight_decay=3e-4, max_norm=5.0)\n",
    "\n",
    "def forward(X_np, train=True):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x)\n",
    "    h = bn1(h, train=train)       # â˜… BN\n",
    "    h = h.relu()\n",
    "    h = L.dropout(h, p=0.5, train=train)\n",
    "    logits = fc2(h)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "15407b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = S.ReduceLROnPlateau(opt, factor=0.5, patience=2, min_lr=1e-6, threshold=1e-4)\n",
    "best_path = \"best_fc_bn.npz\"\n",
    "best_val = float(\"inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "126b2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥\n",
    "if val_loss < best_val - 1e-4:\n",
    "    best_val = val_loss\n",
    "    U.save_checkpoint(best_path, [fc1, fc2])  # bn1ì€ ëŸ¬ë‹í†µê³„ë§Œ ìˆê³  ê°€ì¤‘ì¹˜ gamma/betaëŠ” bn1.parameters()ë¡œ í¬í•¨(ì´ë¯¸ optì— í¬í•¨)\n",
    "# ìŠ¤ì¼€ì¤„\n",
    "reduced = scheduler.step(val_loss)\n",
    "if reduced:\n",
    "    print(f\"  â†³ ReduceLROnPlateau: lr -> {opt.lr:g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4f694e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1d ì¡´ì¬? -> True\n",
      "layers.py ê²½ë¡œ: /Users/suakim/Documents/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/flutter á„ƒá…µá†¸á„…á…¥á„‚á…µá†¼ á„‘á…³á†¯á„Œá…¦á†¨/Model/ssuatensortflow/layers.py\n"
     ]
    }
   ],
   "source": [
    "import importlib, inspect\n",
    "import ssuatensortflow.layers as L\n",
    "importlib.reload(L)\n",
    "print(\"BatchNorm1d ì¡´ì¬? ->\", hasattr(L, \"BatchNorm1d\"))\n",
    "print(\"layers.py ê²½ë¡œ:\", inspect.getfile(L))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3e36fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train_loss=3.7451 acc=0.048 | val_loss=3.2558 acc=0.054 | lr=0.005\n",
      "[02] train_loss=3.5702 acc=0.058 | val_loss=3.2366 acc=0.044 | lr=0.005\n",
      "[03] train_loss=3.5212 acc=0.052 | val_loss=3.2271 acc=0.053 | lr=0.005\n",
      "[04] train_loss=3.4932 acc=0.065 | val_loss=3.2243 acc=0.049 | lr=0.005\n",
      "[05] train_loss=3.4488 acc=0.059 | val_loss=3.2216 acc=0.061 | lr=0.005\n",
      "[06] train_loss=3.4062 acc=0.059 | val_loss=3.2326 acc=0.051 | lr=0.005\n",
      "[07] train_loss=3.3662 acc=0.064 | val_loss=3.2158 acc=0.056 | lr=0.005\n",
      "[08] train_loss=3.3643 acc=0.066 | val_loss=3.2213 acc=0.056 | lr=0.005\n",
      "[09] train_loss=3.3542 acc=0.067 | val_loss=3.2205 acc=0.047 | lr=0.005\n",
      "  â†³ ReduceLROnPlateau: lr -> 0.0025\n",
      "[10] train_loss=3.2980 acc=0.069 | val_loss=3.2130 acc=0.057 | lr=0.0025\n"
     ]
    }
   ],
   "source": [
    "for Xb, yb in iterate_minibatches(train, batch_size=64, shuffle=True, train_mode=True):\n",
    "    ...\n",
    "def evaluate(pairs, batch_size=128):\n",
    "    losses, correct, total = [], 0, 0\n",
    "    for Xb, yb in iterate_minibatches(pairs, batch_size=batch_size, shuffle=False, train_mode=False):\n",
    "        logits = forward(Xb, train=False)\n",
    "        loss = nn.softmax_cross_entropy(logits, yb)\n",
    "        losses.append(float(loss.data))\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        correct += int((pred == yb).sum()); total += int(yb.size)\n",
    "    return (np.mean(losses) if losses else float(\"nan\")), (correct/total if total else 0.0)\n",
    "\n",
    "EPOCHS = 10\n",
    "best_path = \"best_fc_bn.npz\"\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    # ---- Train (ì¦ê°• ON) ----\n",
    "    train_losses = []; train_correct = train_total = 0\n",
    "    for Xb, yb in iterate_minibatches(train, batch_size=64, shuffle=True, train_mode=True):\n",
    "        logits = forward(Xb, train=True)\n",
    "        loss = nn.softmax_cross_entropy(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        train_losses.append(float(loss.data))\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        train_correct += int((pred == yb).sum()); train_total += int(yb.size)\n",
    "\n",
    "    tr_loss = float(np.mean(train_losses)); tr_acc = train_correct/train_total\n",
    "\n",
    "    # ---- Val ----\n",
    "    val_loss, val_acc = evaluate(val)\n",
    "\n",
    "    print(f\"[{ep:02d}] train_loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.3f} | lr={opt.lr:g}\")\n",
    "\n",
    "    # ---- ì €ì¥ ----\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        U.save_checkpoint(best_path, [fc1, fc2])   # bn1ì˜ gamma/betaëŠ” optì— ì´ë¯¸ í¬í•¨ë˜ì–´ í•™ìŠµë¨\n",
    "\n",
    "    # ---- ìŠ¤ì¼€ì¤„ ----\n",
    "    reduced = scheduler.step(val_loss)\n",
    "    if reduced:\n",
    "        print(f\"  â†³ ReduceLROnPlateau: lr -> {opt.lr:g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1312c800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥: prep.npz best_fc_bn.npz\n"
     ]
    }
   ],
   "source": [
    "np.savez(\"prep.npz\", mean3=mean3, std3=std3, breeds=np.array(breeds, dtype=object))\n",
    "print(\"ì €ì¥:\", \"prep.npz\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ab4f513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss=2.4066 acc=0.125\n",
      "[100] loss=0.0056 acc=1.000\n",
      "[200] loss=0.0033 acc=1.000\n",
      "[300] loss=0.0023 acc=1.000\n",
      "[400] loss=0.0018 acc=1.000\n",
      "[500] loss=0.0014 acc=1.000\n",
      "[600] loss=0.0012 acc=1.000\n",
      "[700] loss=0.0010 acc=1.000\n",
      "[800] loss=0.0009 acc=1.000\n",
      "[900] loss=0.0008 acc=1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ssuatensortflow.tensor import Tensor\n",
    "from ssuatensortflow.layers import Linear, SGD\n",
    "from ssuatensortflow.nn import softmax_cross_entropy\n",
    "from ssuatensortflow.utils import seed_everything\n",
    "from ssuatensortflow.transforms import to_vec\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "# ê°€ì§œ ë°ì´í„° (B=16, C=10), ì…ë ¥ 32x32x3\n",
    "B, C = 16, 10\n",
    "X_np = np.random.randn(B, 32*32*3).astype(np.float32) * 0.2\n",
    "y_np = np.random.randint(0, C, size=(B,), dtype=np.int64)\n",
    "\n",
    "# ì•„ì£¼ ì‘ì€ MLP (BN ë¹¼ê³  ë¨¼ì € í™•ì¸)\n",
    "l1 = Linear(32*32*3, 128, init=\"he\")\n",
    "l2 = Linear(128, C, init=\"xavier\")\n",
    "layers = [l1, l2]\n",
    "\n",
    "opt = SGD([p for L in layers for p in L.parameters()], lr=1e-2, momentum=0.9, weight_decay=0.0)\n",
    "\n",
    "for t in range(1000):\n",
    "    x = Tensor(X_np, requires_grad=False)           # (B, D)\n",
    "    h = l1(x).relu()\n",
    "    logits = l2(h)\n",
    "    loss = softmax_cross_entropy(logits, y_np)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        acc = (pred == y_np).mean()\n",
    "        print(f\"[{t}] loss={loss.data:.4f} acc={acc:.3f}\")\n",
    "\n",
    "# ê¸°ëŒ€: accê°€ 0.9~1.0 ê°€ê¹Œì´ ì˜¬ë¼ê°(ê³¼ì í•©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "80b98e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib, sys, os\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "importlib.invalidate_caches()\n",
    "import ssuatensortflow\n",
    "importlib.reload(ssuatensortflow)\n",
    "from ssuatensortflow.data import build_index, split_train_val, ImageFolderDataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4dfa2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, sys, os\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "importlib.invalidate_caches()\n",
    "import ssuatensortflow\n",
    "importlib.reload(ssuatensortflow)\n",
    "\n",
    "from ssuatensortflow.data import build_index, split_train_val, ImageFolderDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8dd28",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¡œë“œ + DataLoader ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8f207b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í´ë˜ìŠ¤ ìˆ˜: 25\n",
      "ì˜ˆì‹œ í´ë˜ìŠ¤ 5ê°œ: ['american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer']\n",
      "train: 4481  | val: 497\n"
     ]
    }
   ],
   "source": [
    "from ssuatensortflow.transforms import Compose, ToRGBResize, RandomHorizontalFlip, RandomPadCrop, Normalize01, to_chw\n",
    "from ssuatensortflow.utils import seed_everything\n",
    "import numpy as np\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "data_root = \"data\"\n",
    "\n",
    "paths, labels, classes = build_index(data_root)\n",
    "print(\"í´ë˜ìŠ¤ ìˆ˜:\", len(classes))\n",
    "print(\"ì˜ˆì‹œ í´ë˜ìŠ¤ 5ê°œ:\", classes[:5])\n",
    "\n",
    "(train_paths, train_labels), (val_paths, val_labels) = split_train_val(paths, labels, val_ratio=0.1, seed=0)\n",
    "print(\"train:\", len(train_paths), \" | val:\", len(val_paths))\n",
    "\n",
    "# ë³€í™˜ ì •ì˜\n",
    "train_tf = Compose([\n",
    "    ToRGBResize(64),\n",
    "    RandomPadCrop(64, pad=4),\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    Normalize01([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
    "])\n",
    "val_tf = Compose([\n",
    "    ToRGBResize(64),\n",
    "    Normalize01([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "train_ds = ImageFolderDataset(train_paths, train_labels, transform=train_tf)\n",
    "val_ds   = ImageFolderDataset(val_paths, val_labels, transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6dbc6",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ êµ¬ì„± + í•™ìŠµ ë£¨í”„ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "854e10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: data\n",
      "í•˜ìœ„ í´ë” ëª©ë¡: ['wheaten_terrier', 'great_pyrenees', 'american_bulldog', 'pomeranian', 'american_pit_bull_terrier', 'yorkshire_terrier', 'japanese_chin', 'english_cocker_spaniel', 'miniature_pinscher', 'basset_hound', 'saint_bernard', 'chihuahua', 'newfoundland', 'pug', 'havanese', 'beagle', 'german_shorthaired', 'staffordshire_bull_terrier', 'samoyed', 'scottish_terrier', 'leonberger', 'keeshond', 'boxer', 'english_setter', 'shiba_inu']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_ROOT = \"data\"\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"í•˜ìœ„ í´ë” ëª©ë¡:\", os.listdir(DATA_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359dc403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 25, Train: 4229, Val: 749\n",
      "[001] lr=0.00500  train loss=3.6698 acc=0.075 | val loss=3.0866 acc=0.123  (28.0s)\n",
      "[002] lr=0.00500  train loss=3.3722 acc=0.092 | val loss=3.0427 acc=0.142  (23.4s)\n",
      "[003] lr=0.00500  train loss=3.2689 acc=0.113 | val loss=3.0283 acc=0.147  (22.9s)\n",
      "[004] lr=0.00500  train loss=3.1831 acc=0.120 | val loss=3.0216 acc=0.148  (23.5s)\n",
      "[005] lr=0.00500  train loss=3.1407 acc=0.136 | val loss=3.0159 acc=0.146  (22.9s)\n",
      "[006] lr=0.00500  train loss=3.1014 acc=0.145 | val loss=3.0095 acc=0.152  (23.5s)\n",
      "[007] lr=0.00450  train loss=3.0789 acc=0.143 | val loss=3.0090 acc=0.152  (23.5s)\n",
      "[008] lr=0.00450  train loss=3.0554 acc=0.150 | val loss=2.9981 acc=0.150  (23.5s)\n",
      "[009] lr=0.00450  train loss=3.0360 acc=0.152 | val loss=2.9957 acc=0.166  (22.7s)\n",
      "[010] lr=0.00450  train loss=3.0181 acc=0.162 | val loss=2.9921 acc=0.159  (26.3s)\n",
      "[011] lr=0.00450  train loss=3.0042 acc=0.163 | val loss=2.9844 acc=0.164  (23.2s)\n",
      "[012] lr=0.00405  train loss=2.9970 acc=0.162 | val loss=2.9838 acc=0.156  (26.6s)\n",
      "[013] lr=0.00405  train loss=2.9833 acc=0.173 | val loss=2.9791 acc=0.160  (29.7s)\n",
      "[014] lr=0.00405  train loss=2.9673 acc=0.179 | val loss=2.9777 acc=0.170  (28.1s)\n",
      "[015] lr=0.00405  train loss=2.9526 acc=0.190 | val loss=2.9841 acc=0.158  (32.5s)\n",
      "[016] lr=0.00365  train loss=2.9438 acc=0.182 | val loss=2.9845 acc=0.179  (28.2s)\n",
      "[017] lr=0.00365  train loss=2.9448 acc=0.176 | val loss=2.9805 acc=0.160  (24.2s)\n",
      "[018] lr=0.00365  train loss=2.9319 acc=0.195 | val loss=2.9818 acc=0.163  (22.1s)\n",
      "[019] lr=0.00328  train loss=2.9158 acc=0.198 | val loss=2.9816 acc=0.160  (22.7s)\n",
      "[020] lr=0.00295  train loss=2.9142 acc=0.199 | val loss=2.9817 acc=0.152  (23.1s)\n",
      "[021] lr=0.00295  train loss=2.9197 acc=0.199 | val loss=2.9763 acc=0.175  (26.6s)\n",
      "[022] lr=0.00295  train loss=2.9099 acc=0.207 | val loss=2.9724 acc=0.160  (27.1s)\n",
      "[023] lr=0.00295  train loss=2.9036 acc=0.205 | val loss=2.9702 acc=0.158  (48.5s)\n",
      "[024] lr=0.00295  train loss=2.9018 acc=0.202 | val loss=2.9688 acc=0.174  (27.5s)\n",
      "[025] lr=0.00295  train loss=2.8950 acc=0.212 | val loss=2.9722 acc=0.172  (31.2s)\n",
      "[026] lr=0.00266  train loss=2.8981 acc=0.212 | val loss=2.9716 acc=0.182  (54.1s)\n",
      "[027] lr=0.00266  train loss=2.8937 acc=0.205 | val loss=2.9684 acc=0.176  (22.9s)\n",
      "[028] lr=0.00266  train loss=2.8900 acc=0.214 | val loss=2.9631 acc=0.175  (21.4s)\n",
      "[029] lr=0.00266  train loss=2.8643 acc=0.223 | val loss=2.9698 acc=0.168  (21.6s)\n",
      "[030] lr=0.00266  train loss=2.8739 acc=0.225 | val loss=2.9660 acc=0.182  (21.4s)\n",
      "[031] lr=0.00239  train loss=2.8702 acc=0.214 | val loss=2.9656 acc=0.175  (21.5s)\n",
      "[032] lr=0.00239  train loss=2.8658 acc=0.226 | val loss=2.9682 acc=0.167  (21.8s)\n",
      "[033] lr=0.00239  train loss=2.8514 acc=0.231 | val loss=2.9625 acc=0.174  (21.6s)\n",
      "[034] lr=0.00239  train loss=2.8565 acc=0.225 | val loss=2.9653 acc=0.168  (23.2s)\n",
      "[035] lr=0.00239  train loss=2.8524 acc=0.234 | val loss=2.9578 acc=0.187  (23.0s)\n",
      "[036] lr=0.00215  train loss=2.8527 acc=0.235 | val loss=2.9580 acc=0.172  (22.5s)\n",
      "[037] lr=0.00194  train loss=2.8412 acc=0.229 | val loss=2.9579 acc=0.192  (28.2s)\n",
      "[038] lr=0.00194  train loss=2.8509 acc=0.238 | val loss=2.9541 acc=0.186  (29.1s)\n",
      "[039] lr=0.00194  train loss=2.8385 acc=0.233 | val loss=2.9597 acc=0.190  (26.2s)\n",
      "[040] lr=0.00194  train loss=2.8289 acc=0.237 | val loss=2.9584 acc=0.191  (24.3s)\n",
      "[041] lr=0.00194  train loss=2.8263 acc=0.238 | val loss=2.9633 acc=0.188  (27.4s)\n",
      "[042] lr=0.00194  train loss=2.8304 acc=0.237 | val loss=2.9608 acc=0.191  (27.2s)\n",
      "[043] lr=0.00174  train loss=2.8279 acc=0.240 | val loss=2.9601 acc=0.186  (27.6s)\n",
      "[044] lr=0.00174  train loss=2.8258 acc=0.244 | val loss=2.9614 acc=0.179  (27.3s)\n",
      "[045] lr=0.00174  train loss=2.8198 acc=0.245 | val loss=2.9650 acc=0.180  (27.3s)\n",
      "[046] lr=0.00174  train loss=2.8238 acc=0.235 | val loss=2.9571 acc=0.187  (27.7s)\n",
      "[047] lr=0.00174  train loss=2.8165 acc=0.245 | val loss=2.9557 acc=0.190  (28.8s)\n",
      "[048] lr=0.00174  train loss=2.8096 acc=0.257 | val loss=2.9574 acc=0.195  (26.9s)\n",
      "[049] lr=0.00174  train loss=2.8099 acc=0.243 | val loss=2.9609 acc=0.188  (22.6s)\n",
      "[050] lr=0.00157  train loss=2.8112 acc=0.247 | val loss=2.9607 acc=0.190  (812.6s)\n",
      "Early stopped at epoch 50\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# train_ssua_standalone.py (v2: ê³¼ì í•© ì™„í™” íŠœë‹ ë°˜ì˜)\n",
    "# ------------------------------------------------------------\n",
    "# - transforms / dataset / dataloader / split ë‚´ì¥\n",
    "# - ssuatensortflow: Tensor / Linear / BatchNorm1d / SGD / dropout / LS-CE / EarlyStopping ì‚¬ìš©\n",
    "# - ë³€ê²½ì : ì…ë ¥/íˆë“  ì¶•ì†Œ, weight_decayâ†‘, label smoothingâ†‘, dropoutâ†‘, PadCrop ì¦ê°•, ê°„ë‹¨ LR decay\n",
    "# - DataLoaderì—ì„œ (B,H,W,C) â†’ (B,F) í‰íƒ„í™”\n",
    "# ------------------------------------------------------------\n",
    "import os, sys, time, random, glob\n",
    "import numpy as np\n",
    "\n",
    "# 0) ssuatensortflow ê²½ë¡œ ì¶”ê°€\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"Model\"))\n",
    "\n",
    "# 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ê³µì£¼ë‹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬)\n",
    "from ssuatensortflow.tensor import Tensor\n",
    "from ssuatensortflow.layers import Linear, BatchNorm1d, SGD, dropout  # í•¨ìˆ˜í˜• dropout\n",
    "from ssuatensortflow.nn import cross_entropy_with_label_smoothing\n",
    "from ssuatensortflow.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# A. ì´ íŒŒì¼ ì•ˆì—ì„œë§Œ ì“°ëŠ” ì „ì²˜ë¦¬/ë°ì´í„° ìœ í‹¸ (ë²„ì „ì°¨ì´ ë°©ì§€ìš©)\n",
    "# ============================================================\n",
    "from PIL import Image\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms): self.transforms = transforms\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms: img = t(img)\n",
    "        return img\n",
    "\n",
    "class ToRGBResize:\n",
    "    def __init__(self, width, height):\n",
    "        self.w, self.h = width, height\n",
    "    def __call__(self, img):\n",
    "        # img íƒ€ì…ì— ìƒê´€ì—†ì´ ì•ˆì „í•˜ê²Œ PIL.Imageë¡œ ë³€í™˜\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img)\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            arr = img\n",
    "            if arr.dtype != np.uint8:\n",
    "                arr = np.clip(arr * (255.0 if arr.max() <= 1.0 else 1.0), 0, 255).astype(np.uint8)\n",
    "            img = Image.fromarray(arr)\n",
    "        img = img.convert(\"RGB\").resize((self.w, self.h))\n",
    "        return np.array(img, dtype=np.float32) / 255.0  # 0~1 ìŠ¤ì¼€ì¼\n",
    "\n",
    "class Normalize01:\n",
    "    def __init__(self, mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)):\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "        self.std  = np.array(std,  dtype=np.float32)\n",
    "    def __call__(self, img):\n",
    "        x = img.data if isinstance(img, Tensor) else img\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str):\n",
    "            return img  # ê²½ë¡œë©´ ë‹¤ìŒ ë³€í™˜ì—ì„œ ì²˜ë¦¬\n",
    "        if np.random.rand() < self.p:\n",
    "            if isinstance(img, np.ndarray):\n",
    "                return np.fliplr(img)\n",
    "        return img\n",
    "\n",
    "class RandomPadCrop:\n",
    "    \"\"\"ì´ë¯¸ì§€ ê°€ì¥ìë¦¬ë¥¼ ë°˜ì‚¬ íŒ¨ë”© í›„ ì›ë˜ í¬ê¸°ë¡œ ëœë¤ í¬ë¡­\"\"\"\n",
    "    def __init__(self, padding=8):\n",
    "        self.padding = padding\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str):\n",
    "            return img  # ê²½ë¡œ ë¬¸ìì—´ì€ ë‹¤ìŒ ë³€í™˜ì—ì„œ ì²˜ë¦¬\n",
    "        if self.padding <= 0:\n",
    "            return img\n",
    "        h, w, c = img.shape\n",
    "        pad = self.padding\n",
    "        padded = np.pad(img, ((pad,pad),(pad,pad),(0,0)), mode=\"reflect\")\n",
    "        top  = np.random.randint(0, 2*pad)\n",
    "        left = np.random.randint(0, 2*pad)\n",
    "        return padded[top:top+h, left:left+w]\n",
    "\n",
    "def stratified_split(data_root, val_ratio=0.15, seed=42):\n",
    "    \"\"\"data_root/{class}/*.jpg êµ¬ì¡°ë¥¼ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¶„í• \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    classes = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
    "    cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    train, val = [], []\n",
    "    for c in classes:\n",
    "        files = []\n",
    "        for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\"):\n",
    "            files += glob.glob(os.path.join(data_root, c, ext))\n",
    "        files.sort(); rng.shuffle(files)\n",
    "        k = int(len(files) * (1 - val_ratio))\n",
    "        train += [(f, cls_to_idx[c]) for f in files[:k]]\n",
    "        val   += [(f, cls_to_idx[c]) for f in files[k:]]\n",
    "    rng.shuffle(train); rng.shuffle(val)\n",
    "    return train, val\n",
    "\n",
    "class ImageFolderDataset:\n",
    "    \"\"\"(path, class_idx) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì´ë¯¸ì§€ë¥¼ ë¡œë“œ/ì „ì²˜ë¦¬\"\"\"\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(set([y for _, y in samples]))\n",
    "        self.num_classes = len(self.classes)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, y = self.samples[idx]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(path)  # ë³€í™˜ íŒŒì´í”„ë¼ì¸ì— ë¡œë“œ í¬í•¨\n",
    "        else:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            x = np.array(img, dtype=np.float32) / 255.0\n",
    "        if not isinstance(x, Tensor):\n",
    "            x = Tensor(x.astype(np.float32))  # Tensorë¡œ ê°ì‹¸ê¸° (H,W,C)\n",
    "        return x, np.int64(y)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=64, shuffle=True, drop_last=False):\n",
    "        self.ds, self.bs, self.shuffle, self.drop_last = dataset, batch_size, shuffle, drop_last\n",
    "    def __iter__(self):\n",
    "        idxs = list(range(len(self.ds)))\n",
    "        if self.shuffle: random.shuffle(idxs)\n",
    "        batch = []\n",
    "        for i in idxs:\n",
    "            batch.append(self.ds[i])\n",
    "            if len(batch) == self.bs:\n",
    "                yield self._collate(batch); batch = []\n",
    "        if batch and not self.drop_last:\n",
    "            yield self._collate(batch)\n",
    "    def __len__(self):\n",
    "        n = len(self.ds) // self.bs\n",
    "        if len(self.ds) % self.bs and not self.drop_last: n += 1\n",
    "        return max(1, n)\n",
    "    def _collate(self, batch):\n",
    "        # (B,H,W,C) ìŠ¤íƒ í›„ (B,F) í‰íƒ„í™”\n",
    "        xs, ys = zip(*batch)\n",
    "        x_np = np.stack([x.data for x in xs], axis=0).astype(np.float32)\n",
    "        if x_np.ndim == 4:\n",
    "            if x_np.shape[-1] == 3:  # (B,H,W,C)\n",
    "                B = x_np.shape[0]\n",
    "                x_np = x_np.reshape(B, -1)\n",
    "            else:  # (B,3,H,W)\n",
    "                B, C, H, W = x_np.shape\n",
    "                x_np = x_np.reshape(B, C*H*W)\n",
    "        return Tensor(x_np), np.array(ys, dtype=np.int64)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def save_checkpoint(path, state: dict):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    np.savez_compressed(path, **state)\n",
    "\n",
    "# ============================================================\n",
    "# B. ì„¤ì • (v2 íŠœë‹)\n",
    "# ============================================================\n",
    "# â˜… ê³µì£¼ë‹˜ ì‹¤ì œ ë°ì´í„° ê²½ë¡œ (ìº¡ì²˜ ê¸°ì¤€ 'Model/data')\n",
    "DATA_ROOT = \"data\"\n",
    "\n",
    "IMG_SIZE  = 160      # 224 -> 160\n",
    "BATCH     = 64\n",
    "EPOCHS    = 60\n",
    "LR        = 5e-3     # 1e-2 -> 5e-3\n",
    "MOM       = 0.9\n",
    "WD        = 5e-4     # 1e-4 -> 5e-4\n",
    "SMOOTH    = 0.2      # 0.1 -> 0.2\n",
    "PATIENCE  = 12       # 8  -> 12\n",
    "VAL_RATIO = 0.15\n",
    "SEED      = 42\n",
    "\n",
    "HIDDEN    = 256      # 512 -> 256\n",
    "\n",
    "# ============================================================\n",
    "# C. ë°ì´í„°\n",
    "# ============================================================\n",
    "seed_everything(SEED)\n",
    "\n",
    "def build_transforms(split):\n",
    "    if split == \"train\":\n",
    "        return Compose([\n",
    "            ToRGBResize(IMG_SIZE, IMG_SIZE),\n",
    "            RandomPadCrop(padding=8),     # â˜… ì¶”ê°€ ì¦ê°•\n",
    "            RandomHorizontalFlip(0.5),\n",
    "            Normalize01(),\n",
    "        ])\n",
    "    else:\n",
    "        return Compose([\n",
    "            ToRGBResize(IMG_SIZE, IMG_SIZE),\n",
    "            Normalize01(),\n",
    "        ])\n",
    "\n",
    "if not os.path.isdir(DATA_ROOT):\n",
    "    raise FileNotFoundError(f\"DATA_ROOTê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_ROOT}\")\n",
    "\n",
    "train_files, val_files = stratified_split(DATA_ROOT, val_ratio=VAL_RATIO, seed=SEED)\n",
    "train_ds = ImageFolderDataset(train_files, transform=build_transforms(\"train\"))\n",
    "val_ds   = ImageFolderDataset(val_files,   transform=build_transforms(\"val\"))\n",
    "\n",
    "num_classes = train_ds.num_classes\n",
    "in_features = IMG_SIZE * IMG_SIZE * 3\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, drop_last=False)\n",
    "\n",
    "# ============================================================\n",
    "# D. ëª¨ë¸\n",
    "# ============================================================\n",
    "class FCNet:\n",
    "    def __init__(self, in_features, hidden=HIDDEN, num_classes=10):\n",
    "        self.fc1 = Linear(in_features, hidden, init=\"he\")\n",
    "        self.bn1 = BatchNorm1d(hidden)\n",
    "        self.fc2 = Linear(hidden, num_classes, init=\"xavier\")\n",
    "        self.training = True\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.fc1.parameters() + self.bn1.parameters() + self.fc2.parameters()\n",
    "\n",
    "    def train(self): self.training = True\n",
    "    def eval(self):  self.training = False\n",
    "\n",
    "    def relu(self, x):\n",
    "        mask = (x.data > 0).astype(x.data.dtype)\n",
    "        return x * mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DataLoaderì—ì„œ ì´ë¯¸ (B,F)\n",
    "        y = self.fc1(x)\n",
    "        y = self.bn1(y, train=self.training)\n",
    "        y = self.relu(y)\n",
    "        y = dropout(y, p=0.6, train=self.training)  # 0.4 -> 0.6\n",
    "        y = self.fc2(y)\n",
    "        return y\n",
    "\n",
    "    # ì–¼ë¦¬ ìŠ¤íƒ‘ ì €ì¥/ë³µì›\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"fc1.W\": self.fc1.W.data.copy(),\n",
    "            \"fc1.b\": None if self.fc1.b is None else self.fc1.b.data.copy(),\n",
    "            \"bn1.gamma\": self.bn1.gamma.data.copy(),\n",
    "            \"bn1.beta\":  self.bn1.beta.data.copy(),\n",
    "            \"bn1.running_mean\": self.bn1.running_mean.copy(),\n",
    "            \"bn1.running_var\":  self.bn1.running_var.copy(),\n",
    "            \"fc2.W\": self.fc2.W.data.copy(),\n",
    "            \"fc2.b\": None if self.fc2.b is None else self.fc2.b.data.copy(),\n",
    "        }\n",
    "    def load_state_dict(self, sd):\n",
    "        self.fc1.W.data[:] = sd[\"fc1.W\"]\n",
    "        if self.fc1.b is not None and sd[\"fc1.b\"] is not None:\n",
    "            self.fc1.b.data[:] = sd[\"fc1.b\"]\n",
    "        self.bn1.gamma.data[:] = sd[\"bn1.gamma\"]\n",
    "        self.bn1.beta.data[:]  = sd[\"bn1.beta\"]\n",
    "        self.bn1.running_mean[:] = sd[\"bn1.running_mean\"]\n",
    "        self.bn1.running_var[:]  = sd[\"bn1.running_var\"]\n",
    "        self.fc2.W.data[:] = sd[\"fc2.W\"]\n",
    "        if self.fc2.b is not None and sd[\"fc2.b\"] is not None:\n",
    "            self.fc2.b.data[:] = sd[\"fc2.b\"]\n",
    "\n",
    "model = FCNet(in_features, hidden=HIDDEN, num_classes=num_classes)\n",
    "optimizer = SGD(model.parameters(), lr=LR, momentum=MOM, weight_decay=WD)\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "# ============================================================\n",
    "# E. í•™ìŠµ/í‰ê°€ ë£¨í”„ (+ ê°„ë‹¨ LR decay)\n",
    "# ============================================================\n",
    "def run_one_epoch(loader, train=True):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        logits = model.forward(x)\n",
    "        loss = cross_entropy_with_label_smoothing(logits, y, eps=SMOOTH)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.data)\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        total_correct += int((pred == y).sum())\n",
    "        total_count  += y.shape[0]\n",
    "\n",
    "    return total_loss / max(1, len(loader)), total_correct / max(1, total_count)\n",
    "\n",
    "# ============================================================\n",
    "# F. ì‹¤í–‰\n",
    "# ============================================================\n",
    "best_path = os.path.join(\"Model\", \"checkpoints\", \"best_fc_bn.npz\")\n",
    "os.makedirs(os.path.dirname(best_path), exist_ok=True)\n",
    "\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "print(f\"Classes: {num_classes}, Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = run_one_epoch(train_loader, True)\n",
    "    va_loss, va_acc = run_one_epoch(val_loader, False)\n",
    "\n",
    "    # ê°„ë‹¨í•œ plateau ê°ì§€ â†’ lr 0.9ë°°\n",
    "    if epoch > 3 and abs(prev_val_loss - va_loss) < 1e-3:\n",
    "        optimizer.lr *= 0.9\n",
    "    prev_val_loss = va_loss\n",
    "\n",
    "    early.update(va_loss, model.state_dict, model.load_state_dict)\n",
    "\n",
    "    try: save_checkpoint(best_path, model.state_dict())\n",
    "    except Exception: pass\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[{epoch:03d}] lr={optimizer.lr:.5f}  train loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val loss={va_loss:.4f} acc={va_acc:.3f}  ({dt:.1f}s)\")\n",
    "\n",
    "    if early.stopped:\n",
    "        print(f\"Early stopped at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d6f72",
   "metadata": {},
   "source": [
    "## ìê¾¸ ê³¼ì í•© ì•„ë‹ˆë©´ ì •í™•ë„ê°€ 15% ë°–ì— ì•ˆë‚˜ì˜´\n",
    "- ì´ìœ  ì‚¬ì§„ì˜ í”½ì…€ì„ ë­‰í……ì´ë¡œ ë³´ëŠ”ê²Œ ì•„ë‹ˆë¼ ê°ì ë”°ë¡œ ë³´ê¸° ë•Œë¬¸ì— ê·¸ëŸ°ê±°ì„ \n",
    "- ì»¨ë³¼ë£¨ì…˜(Conv) + í’€ë§(MaxPool) ìœ¼ë¡œ = CNN 3*3 ë­ ë¯¸ëŸ°ì‹ìœ¼ë¡œ ë´ì„œ ê·€ í˜•íƒœë¥¼ ì¡ëŠ”ë‹¤ê±°ë‚˜ ì´ëŸ°ì‹ìœ¼ë¡œ ì ‘ê·¼ í•˜ë©´ ë ë“¯?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 25, Train: 4229, Val: 749\n"
     ]
    }
   ],
   "source": [
    "# train_ssua_cnn_standalone.py\n",
    "# ------------------------------------------------------------\n",
    "# ê°•ì•„ì§€ ì¢… ë¶„ë¥˜: ìˆœìˆ˜ NumPy CNN (Conv2d + MaxPool2d + GAP + FC)\n",
    "# - ë©”ëª¨ë¦¬-ì„¸ì´í”„ Conv2d (ìƒ˜í”Œë³„ im2col/grad ì¬ê³„ì‚°)\n",
    "# - ì¶• ì•ˆì „ ë²¡í„°í™” MaxPool2d (2x2,stride=2 ì „ìš©)\n",
    "# - Global Average Poolingìœ¼ë¡œ ì¼ë°˜í™”â†‘, íŒŒë¼ë¯¸í„°â†“\n",
    "# - EarlyStopping, Label Smoothing(ì´ˆê¸° 0), Dropout(0.2), BN1d\n",
    "# - DataLoaderê°€ (B,C,H,W)ë¡œ ë³€í™˜í•´ CNNì— íˆ¬ì…\n",
    "# ------------------------------------------------------------\n",
    "import os, sys, time, random, glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---- ssuatensortflow ê²½ë¡œ ----\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"Model\"))\n",
    "\n",
    "# ---- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ----\n",
    "from ssuatensortflow.tensor import Tensor\n",
    "from ssuatensortflow.layers import Linear, BatchNorm1d, SGD, dropout\n",
    "from ssuatensortflow.nn import cross_entropy_with_label_smoothing\n",
    "from ssuatensortflow.callbacks import EarlyStopping\n",
    "\n",
    "# ================= ì „ì²˜ë¦¬/ë°ì´í„° ìœ í‹¸ =================\n",
    "class Compose:\n",
    "    def __init__(self, transforms): self.transforms = transforms\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms: img = t(img)\n",
    "        return img\n",
    "\n",
    "class ToRGBResize:\n",
    "    def __init__(self, width, height):\n",
    "        self.w, self.h = width, height\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img)\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            arr = img\n",
    "            if arr.dtype != np.uint8:\n",
    "                arr = np.clip(arr * (255.0 if arr.max() <= 1.0 else 1.0), 0, 255).astype(np.uint8)\n",
    "            img = Image.fromarray(arr)\n",
    "        img = img.convert(\"RGB\").resize((self.w, self.h))\n",
    "        return np.array(img, dtype=np.float32) / 255.0  # (H,W,3)\n",
    "\n",
    "class Normalize01:\n",
    "    def __init__(self, mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)):\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "        self.std  = np.array(std,  dtype=np.float32)\n",
    "    def __call__(self, img):\n",
    "        x = img.data if isinstance(img, Tensor) else img\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str): return img\n",
    "        if np.random.rand() < self.p and isinstance(img, np.ndarray):\n",
    "            return np.fliplr(img)\n",
    "        return img\n",
    "\n",
    "class RandomPadCrop:\n",
    "    def __init__(self, padding=8): self.padding = padding\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str): return img\n",
    "        if self.padding <= 0: return img\n",
    "        h, w, c = img.shape\n",
    "        pad = self.padding\n",
    "        padded = np.pad(img, ((pad,pad),(pad,pad),(0,0)), mode=\"reflect\")\n",
    "        top  = np.random.randint(0, 2*pad)\n",
    "        left = np.random.randint(0, 2*pad)\n",
    "        return padded[top:top+h, left:left+w]\n",
    "\n",
    "def stratified_split(data_root, val_ratio=0.15, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    classes = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
    "    cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    train, val = [], []\n",
    "    for c in classes:\n",
    "        files = []\n",
    "        for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\"):\n",
    "            files += glob.glob(os.path.join(data_root, c, ext))\n",
    "        files.sort(); rng.shuffle(files)\n",
    "        k = int(len(files) * (1 - val_ratio))\n",
    "        train += [(f, cls_to_idx[c]) for f in files[:k]]\n",
    "        val   += [(f, cls_to_idx[c]) for f in files[k:]]\n",
    "    rng.shuffle(train); rng.shuffle(val)\n",
    "    return train, val\n",
    "\n",
    "class ImageFolderDataset:\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(set([y for _, y in samples]))\n",
    "        self.num_classes = len(self.classes)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, y = self.samples[idx]\n",
    "        x = self.transform(path) if self.transform else np.array(Image.open(path).convert(\"RGB\"), dtype=np.float32)/255.0\n",
    "        if not isinstance(x, Tensor):\n",
    "            x = Tensor(x.astype(np.float32))  # (H,W,C)\n",
    "        return x, np.int64(y)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=64, shuffle=True, drop_last=False):\n",
    "        self.ds, self.bs, self.shuffle, self.drop_last = dataset, batch_size, shuffle, drop_last\n",
    "    def __iter__(self):\n",
    "        idxs = list(range(len(self.ds)))\n",
    "        if self.shuffle: random.shuffle(idxs)\n",
    "        batch=[]\n",
    "        for i in idxs:\n",
    "            batch.append(self.ds[i])\n",
    "            if len(batch)==self.bs:\n",
    "                yield self._collate(batch); batch=[]\n",
    "        if batch and not self.drop_last:\n",
    "            yield self._collate(batch)\n",
    "    def __len__(self):\n",
    "        n = len(self.ds)//self.bs\n",
    "        if len(self.ds)%self.bs and not self.drop_last: n+=1\n",
    "        return max(1,n)\n",
    "    def _collate(self, batch):\n",
    "        xs, ys = zip(*batch)\n",
    "        x_np = np.stack([x.data for x in xs], axis=0).astype(np.float32)  # (B,H,W,C)\n",
    "        x_np = np.transpose(x_np, (0,3,1,2)).copy()  # â†’ (B,C,H,W)\n",
    "        return Tensor(x_np), np.array(ys, dtype=np.int64)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def save_checkpoint(path, state: dict):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    np.savez_compressed(path, **state)\n",
    "\n",
    "# ================= Conv / Pool (ë©”ëª¨ë¦¬-ì„¸ì´í”„) =================\n",
    "def _im2col(x, kh, kw, sh, sw, ph, pw):\n",
    "    # x: (B,C,H,W) numpy\n",
    "    B,C,H,W = x.shape\n",
    "    H_out = (H + 2*ph - kh)//sh + 1\n",
    "    W_out = (W + 2*pw - kw)//sw + 1\n",
    "    x_pad = np.pad(x, ((0,0),(0,0),(ph,ph),(pw,pw)), mode='constant')\n",
    "    cols = np.zeros((B, C, kh, kw, H_out, W_out), dtype=x.dtype)\n",
    "    for i in range(kh):\n",
    "        i_lim = i + sh*H_out\n",
    "        for j in range(kw):\n",
    "            j_lim = j + sw*W_out\n",
    "            cols[:, :, i, j, :, :] = x_pad[:, :, i:i_lim:sh, j:j_lim:sw]\n",
    "    cols = cols.reshape(B, C*kh*kw, H_out*W_out)\n",
    "    return cols, H_out, W_out\n",
    "\n",
    "def _col2im(dcols, x_shape, kh, kw, sh, sw, ph, pw):\n",
    "    B,C,H,W = x_shape\n",
    "    H_out = (H + 2*ph - kh)//sh + 1\n",
    "    W_out = (W + 2*pw - kw)//sw + 1\n",
    "    dcols = dcols.reshape(B, C, kh, kw, H_out, W_out)\n",
    "    dx_pad = np.zeros((B, C, H+2*ph, W+2*pw), dtype=dcols.dtype)\n",
    "    for i in range(kh):\n",
    "        i_lim = i + sh*H_out\n",
    "        for j in range(kw):\n",
    "            j_lim = j + sw*W_out\n",
    "            dx_pad[:, :, i:i_lim:sh, j:j_lim:sw] += dcols[:, :, i, j, :, :]\n",
    "    return dx_pad[:, :, ph:ph+H, pw:pw+W]\n",
    "\n",
    "class Conv2d:\n",
    "    \"\"\"ë©”ëª¨ë¦¬-ì„¸ì´í”„ Conv2d (ìƒ˜í”Œë³„ im2col)\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, init='he'):\n",
    "        kh = kw = kernel_size if isinstance(kernel_size, int) else kernel_size[0]\n",
    "        self.kh, self.kw = kh, kw\n",
    "        self.sh = stride if isinstance(stride, int) else stride[0]\n",
    "        self.sw = stride if isinstance(stride, int) else stride[1]\n",
    "        self.ph = padding if isinstance(padding, int) else padding[0]\n",
    "        self.pw = padding if isinstance(padding, int) else padding[1]\n",
    "        fan_in = in_channels*kh*kw\n",
    "        limit = np.sqrt(6.0/max(1,fan_in)) if init=='he' else np.sqrt(6.0/(fan_in+out_channels*kh*kw))\n",
    "        W = np.random.uniform(-limit, limit, size=(out_channels, in_channels, kh, kw)).astype(np.float32)\n",
    "        self.W = Tensor(W, requires_grad=True)\n",
    "        self.b = Tensor(np.zeros((1,out_channels,1,1), np.float32), requires_grad=True) if bias else None\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for p in (self.W, self.b) if p is not None]\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        X = x.data  # (B,C,H,W)\n",
    "        B, Cin, H, W = X.shape\n",
    "        Cout = self.W.data.shape[0]\n",
    "        # ì¶œë ¥ í¬ê¸° ê³„ì‚°\n",
    "        _, H_out, W_out = _im2col(X[:1], self.kh, self.kw, self.sh, self.sw, self.ph, self.pw)\n",
    "        Y = np.zeros((B, Cout, H_out, W_out), dtype=np.float32)\n",
    "        W_col = self.W.data.reshape(Cout, -1)  # (Cout, K)\n",
    "\n",
    "        # ìƒ˜í”Œë³„ë¡œ cols ìƒì„± â†’ matmul â†’ ë²„ë¦¬ê¸° (í”¼í¬ ë©”ëª¨ë¦¬â†“)\n",
    "        for b in range(B):\n",
    "            cols_b, _, _ = _im2col(X[b:b+1], self.kh, self.kw, self.sh, self.sw, self.ph, self.pw)  # (1,K,L)\n",
    "            cols_b = cols_b[0]  # (K, L)\n",
    "            Yb = W_col @ cols_b  # (Cout, L)\n",
    "            if self.b is not None: Yb += self.b.data.reshape(-1,1)\n",
    "            Y[b] = Yb.reshape(Cout, H_out, W_out)\n",
    "\n",
    "        out_t = Tensor(\n",
    "            Y,\n",
    "            requires_grad=x.requires_grad or self.W.requires_grad or (self.b is not None and self.b.requires_grad),\n",
    "            _children=(x, self.W, self.b) if self.b is not None else (x, self.W,),\n",
    "            _op=\"conv2d_memsafe\"\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            if out_t.grad is None: return\n",
    "            gy = out_t.grad  # (B,Cout,H_out,W_out)\n",
    "            Cout, Cin, kh, kw = self.W.data.shape\n",
    "            W_col = self.W.data.reshape(Cout, -1)\n",
    "            dW = np.zeros_like(self.W.data)\n",
    "            db = np.zeros((1, Cout, 1, 1), dtype=np.float32) if self.b is not None else None\n",
    "            dX = np.zeros_like(X) if x.requires_grad else None\n",
    "\n",
    "            # ìƒ˜í”Œë³„ë¡œ cols ì¬ê³„ì‚° (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "            for b in range(B):\n",
    "                cols_b, _, _ = _im2col(X[b:b+1], self.kh, self.kw, self.sh, self.sw, self.ph, self.pw)  # (1,K,L)\n",
    "                cols_b = cols_b[0]                 # (K, L)\n",
    "                gyb = gy[b].reshape(Cout, -1)     # (Cout, L)\n",
    "\n",
    "                dW += (gyb @ cols_b.T).reshape(dW.shape)\n",
    "                if db is not None:\n",
    "                    db += gyb.sum(axis=1).reshape(1, Cout, 1, 1)\n",
    "                if dX is not None:\n",
    "                    dx_cols = W_col.T @ gyb        # (K, L)\n",
    "                    dX[b:b+1] += _col2im(dx_cols.reshape(1, *dx_cols.shape), X[b:b+1].shape,\n",
    "                                         self.kh, self.kw, self.sh, self.sw, self.ph, self.pw)\n",
    "\n",
    "            if self.W.requires_grad:\n",
    "                self.W.grad = (self.W.grad if self.W.grad is not None else 0) + dW\n",
    "            if (self.b is not None) and self.b.requires_grad:\n",
    "                self.b.grad = (self.b.grad if self.b.grad is not None else 0) + db\n",
    "            if x.requires_grad:\n",
    "                x.grad = (x.grad if x.grad is not None else 0) + dX\n",
    "\n",
    "        out_t._backward = _backward\n",
    "        return out_t\n",
    "\n",
    "class MaxPool2d:\n",
    "    \"\"\"ë²¡í„°í™” MaxPool2d (2x2, stride=2 ì „ìš©; ì¶• ì•ˆì „ ë²„ì „)\"\"\"\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kh = self.kw = kernel_size if isinstance(kernel_size,int) else kernel_size[0]\n",
    "        self.sh = self.sw = stride if isinstance(stride,int) else stride[0]\n",
    "        assert self.kh==2 and self.kw==2 and self.sh==2 and self.sw==2, \"2x2,stride=2ë§Œ ì§€ì›\"\n",
    "        self._mask = None\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        X = x.data  # (B,C,H,W)\n",
    "        B,C,H,W = X.shape\n",
    "        H2 = (H//2)*2; W2 = (W//2)*2\n",
    "        Xc = X[:, :, :H2, :W2]\n",
    "        Hh, Ww = H2//2, W2//2\n",
    "\n",
    "        # (B,C,H/2,2,W/2,2)\n",
    "        Xr = Xc.reshape(B, C, Hh, 2, Ww, 2)\n",
    "\n",
    "        # ë‘ ì¶•(3,5)ì—ì„œ í•œ ë²ˆì— max â†’ (B,C,H/2,W/2)\n",
    "        out = Xr.max(axis=(3,5))\n",
    "\n",
    "        # argmaxë¡œ ì •í™•í•œ ë§ˆìŠ¤í¬ êµ¬ì„±\n",
    "        flat = Xr.reshape(B, C, Hh, Ww, 4)   # 2x2 â†’ 4\n",
    "        arg  = flat.argmax(axis=4)           # (B,C,H/2,W/2) âˆˆ {0..3}\n",
    "        mask = np.zeros_like(flat, dtype=bool)\n",
    "        b = np.arange(B)[:,None,None,None]\n",
    "        c = np.arange(C)[None,:,None,None]\n",
    "        h = np.arange(Hh)[None,None,:,None]\n",
    "        w = np.arange(Ww)[None,None,None,:]\n",
    "        mask[b, c, h, w, arg] = True\n",
    "        mask = mask.reshape(B, C, Hh, 2, Ww, 2)\n",
    "\n",
    "        out_t = Tensor(out, requires_grad=x.requires_grad, _children=(x,), _op=\"maxpool2d_vec\")\n",
    "        self._mask = (mask, X.shape)\n",
    "\n",
    "        def _backward():\n",
    "            if out_t.grad is None or not x.requires_grad: \n",
    "                return\n",
    "            mask, orig_shape = self._mask\n",
    "            B,C,H,W = orig_shape\n",
    "            H2 = (H//2)*2; W2 = (W//2)*2\n",
    "            Hh, Ww = H2//2, W2//2\n",
    "\n",
    "            grad_out = out_t.grad.reshape(B, C, Hh, 1, Ww, 1)   # (B,C,H/2,1,W/2,1)\n",
    "            go2 = np.broadcast_to(grad_out, mask.shape)         # (B,C,H/2,2,W/2,2)\n",
    "            gx_small = np.where(mask, go2, 0.0)\n",
    "\n",
    "            gx = np.zeros((B,C,H,W), dtype=out_t.grad.dtype)\n",
    "            gx[:, :, :H2, :W2] = gx_small.reshape(B, C, H2, W2)\n",
    "            x.grad = (x.grad if x.grad is not None else 0) + gx\n",
    "\n",
    "        out_t._backward = _backward\n",
    "        return out_t\n",
    "\n",
    "# ================= ì„¤ì • =================\n",
    "DATA_ROOT = \"data\"   # (ëŒ€ë¬¸ì M)\n",
    "IMG_SIZE  = 112\n",
    "BATCH     = 16\n",
    "EPOCHS    = 10          # â†‘ ì•ˆì • ëª¨ë“œ\n",
    "LR        = 5e-3        # â†“ ë°œì‚° ë°©ì§€\n",
    "MOM       = 0.9\n",
    "WD        = 1e-4        # ê·œì œ ì•½ê°„\n",
    "SMOOTH    = 0.0         # ì´ˆë°˜ì€ 0.0ìœ¼ë¡œ ì‹ í˜¸ ì„¸ê²Œ\n",
    "PATIENCE  = 12\n",
    "VAL_RATIO = 0.15\n",
    "SEED      = 42\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "def build_transforms(split):\n",
    "    if split == \"train\":\n",
    "        return Compose([\n",
    "            ToRGBResize(IMG_SIZE, IMG_SIZE),\n",
    "            # RandomPadCrop(padding=8),   # ì†ë„ ì—¬ìœ  ìƒê¸°ë©´ ì¼œê¸°\n",
    "            RandomHorizontalFlip(0.5),\n",
    "            Normalize01(),\n",
    "        ])\n",
    "    else:\n",
    "        return Compose([\n",
    "            ToRGBResize(IMG_SIZE, IMG_SIZE),\n",
    "            Normalize01(),\n",
    "        ])\n",
    "\n",
    "if not os.path.isdir(DATA_ROOT):\n",
    "    raise FileNotFoundError(f\"DATA_ROOTê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_ROOT}\")\n",
    "\n",
    "train_files, val_files = stratified_split(DATA_ROOT, val_ratio=VAL_RATIO, seed=SEED)\n",
    "train_ds = ImageFolderDataset(train_files, transform=build_transforms(\"train\"))\n",
    "val_ds   = ImageFolderDataset(val_files,   transform=build_transforms(\"val\"))\n",
    "\n",
    "num_classes = train_ds.num_classes\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, drop_last=False)\n",
    "\n",
    "# ================= ì‘ì€ CNN (GAP ì‚¬ìš©) =================\n",
    "class SmallCNN:\n",
    "    def __init__(self, in_ch=3, num_classes=10, hidden=256):\n",
    "        # ì•ë‹¨ stride=2ë¡œ ë‹¤ìš´ìƒ˜í”Œ â†’ ë©”ëª¨ë¦¬/ì†ë„ ì•ˆì •\n",
    "        self.conv1 = Conv2d(in_ch, 16, kernel_size=3, stride=2, padding=1, init='he')\n",
    "        self.conv2 = Conv2d(16,   32, kernel_size=3, stride=1, padding=1, init='he')\n",
    "        self.pool1 = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = Conv2d(32,   64, kernel_size=3, stride=1, padding=1, init='he')\n",
    "        self.conv4 = Conv2d(64,  128, kernel_size=3, stride=1, padding=1, init='he')\n",
    "        self.pool2 = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = None; self.bn1 = None; self.fc2 = None\n",
    "        self.hidden = hidden\n",
    "        self.num_classes = num_classes\n",
    "        self.training = True\n",
    "\n",
    "    def parameters(self):\n",
    "        ps = []\n",
    "        for m in [self.conv1, self.conv2, self.conv3, self.conv4]:\n",
    "            ps += m.parameters()\n",
    "        if self.fc1 is not None:\n",
    "            ps += self.fc1.parameters() + self.bn1.parameters() + self.fc2.parameters()\n",
    "        return ps\n",
    "\n",
    "    def train(self): self.training = True\n",
    "    def eval(self):  self.training = False\n",
    "\n",
    "    def relu(self, x):\n",
    "        mask = (x.data > 0).astype(x.data.dtype)\n",
    "        return x * mask\n",
    "\n",
    "    def _ensure_fc(self, feat_4d: Tensor):\n",
    "        B,C,H,W = feat_4d.data.shape\n",
    "        F = C              # âœ… GAP ì“°ë‹ˆê¹Œ ì±„ë„ ìˆ˜ë§Œ ì…ë ¥\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = Linear(F, self.hidden, init=\"he\")\n",
    "            self.bn1 = BatchNorm1d(self.hidden)\n",
    "            self.fc2 = Linear(self.hidden, self.num_classes, init=\"xavier\")\n",
    "\n",
    "    def forward(self, x):  # x: (B,C,H,W)\n",
    "        y = self.conv1(x); y = self.relu(y)\n",
    "        y = self.conv2(y); y = self.relu(y)\n",
    "        y = self.pool1(y)\n",
    "        y = self.conv3(y); y = self.relu(y)\n",
    "        y = self.conv4(y); y = self.relu(y)\n",
    "        y = self.pool2(y)\n",
    "\n",
    "        # FC êµ¬ì„±ì€ GAP ì „ì—\n",
    "        self._ensure_fc(y)\n",
    "\n",
    "        # âœ… Global Average Pool: (B,C,H,W) -> (B,C)\n",
    "        B, C, H, W = y.data.shape\n",
    "        avg = y.data.mean(axis=(2,3))  # (B,C)\n",
    "        y_gap = Tensor(avg, requires_grad=y.requires_grad, _children=(y,), _op=\"gap\")\n",
    "        def _bw_gap():\n",
    "            if y_gap.grad is None or not y.requires_grad: return\n",
    "            g = y_gap.grad[:, :, None, None] / (H*W)     # (B,C,1,1)\n",
    "            g = np.broadcast_to(g, (B,C,H,W))\n",
    "            y.grad = (y.grad if y.grad is not None else 0) + g\n",
    "        y_gap._backward = _bw_gap\n",
    "\n",
    "        z = self.fc1(y_gap)\n",
    "        z = self.bn1(z, train=self.training)\n",
    "        z = self.relu(z)\n",
    "        z = dropout(z, p=0.2, train=self.training)   # 0.3 -> 0.2\n",
    "        z = self.fc2(z)\n",
    "        return z\n",
    "\n",
    "# ================= í•™ìŠµ ë£¨í”„ =================\n",
    "model = SmallCNN(in_ch=3, num_classes=num_classes, hidden=256)\n",
    "\n",
    "# ì›Œë°ì—…(FC ìƒì„±) + ì˜µí‹°ë§ˆì´ì € ì¤€ë¹„\n",
    "x_warm, y_warm = next(iter(train_loader))\n",
    "_ = model.forward(x_warm)\n",
    "optimizer = SGD(model.parameters(), lr=LR, momentum=MOM, weight_decay=WD)\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "def run_one_epoch(loader, train=True):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "    for bi, (x,y) in enumerate(loader, 1):\n",
    "        logits = model.forward(x)\n",
    "        loss = cross_entropy_with_label_smoothing(logits, y, eps=SMOOTH)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += float(loss.data)\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        total_correct += int((pred==y).sum())\n",
    "        total_count  += y.shape[0]\n",
    "    return total_loss/max(1,len(loader)), total_correct/max(1,total_count)\n",
    "\n",
    "best_path = os.path.join(\"Model\", \"checkpoints\", \"best_cnn.npz\")\n",
    "os.makedirs(os.path.dirname(best_path), exist_ok=True)\n",
    "\n",
    "print(f\"Classes: {num_classes}, Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "prev_val = float(\"inf\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0=time.time()\n",
    "    tr_loss, tr_acc = run_one_epoch(train_loader, True)\n",
    "    va_loss, va_acc = run_one_epoch(val_loader, False)\n",
    "\n",
    "    # plateau ê°ì§€ëŠ” 5epoch ì´í›„ë¶€í„°\n",
    "    if epoch > 5 and (prev_val - va_loss) < 5e-4:\n",
    "        optimizer.lr *= 0.9\n",
    "    prev_val = va_loss\n",
    "\n",
    "    # ì–¼ë¦¬ ìŠ¤íƒ‘ ì²´í¬í¬ì¸íŠ¸\n",
    "    early.update(va_loss,\n",
    "        lambda: {\n",
    "            \"conv1.W\": model.conv1.W.data.copy(),\n",
    "            \"conv1.b\": None if model.conv1.b is None else model.conv1.b.data.copy(),\n",
    "            \"conv2.W\": model.conv2.W.data.copy(),\n",
    "            \"conv2.b\": None if model.conv2.b is None else model.conv2.b.data.copy(),\n",
    "            \"conv3.W\": model.conv3.W.data.copy(),\n",
    "            \"conv3.b\": None if model.conv3.b is None else model.conv3.b.data.copy(),\n",
    "            \"conv4.W\": model.conv4.W.data.copy(),\n",
    "            \"conv4.b\": None if model.conv4.b is None else model.conv4.b.data.copy(),\n",
    "            \"fc1.W\":   model.fc1.W.data.copy(),\n",
    "            \"fc1.b\":   None if model.fc1.b is None else model.fc1.b.data.copy(),\n",
    "            \"bn1.gamma\": model.bn1.gamma.data.copy(),\n",
    "            \"bn1.beta\":  model.bn1.beta.data.copy(),\n",
    "            \"bn1.running_mean\": model.bn1.running_mean.copy(),\n",
    "            \"bn1.running_var\":  model.bn1.running_var.copy(),\n",
    "            \"fc2.W\":   model.fc2.W.data.copy(),\n",
    "            \"fc2.b\":   None if model.fc2.b is None else model.fc2.b.data.copy(),\n",
    "        },\n",
    "        lambda sd: None\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        save_checkpoint(best_path, {\n",
    "            \"conv1.W\": model.conv1.W.data,\n",
    "            \"conv1.b\": None if model.conv1.b is None else model.conv1.b.data,\n",
    "            \"conv2.W\": model.conv2.W.data,\n",
    "            \"conv2.b\": None if model.conv2.b is None else model.conv2.b.data,\n",
    "            \"conv3.W\": model.conv3.W.data,\n",
    "            \"conv3.b\": None if model.conv3.b is None else model.conv3.b.data,\n",
    "            \"conv4.W\": model.conv4.W.data,\n",
    "            \"conv4.b\": None if model.conv4.b is None else model.conv4.b.data,\n",
    "            \"fc1.W\":   model.fc1.W.data,\n",
    "            \"fc1.b\":   None if model.fc1.b is None else model.fc1.b.data,\n",
    "            \"bn1.gamma\": model.bn1.gamma.data,\n",
    "            \"bn1.beta\":  model.bn1.beta.data,\n",
    "            \"bn1.running_mean\": model.bn1.running_mean,\n",
    "            \"bn1.running_var\":  model.bn1.running_var,\n",
    "            \"fc2.W\":   model.fc2.W.data,\n",
    "            \"fc2.b\":   None if model.fc2.b is None else model.fc2.b.data,\n",
    "        })\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dt=time.time()-t0\n",
    "    print(f\"[{epoch:03d}] lr={optimizer.lr:.5f}  train loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val loss={va_loss:.4f} acc={va_acc:.3f}  ({dt:.1f}s)\")\n",
    "\n",
    "    if early.stopped:\n",
    "        print(f\"Early stopped at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"Done.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
