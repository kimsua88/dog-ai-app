{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24e432f",
   "metadata": {},
   "source": [
    "### 자 딥러닝 시작해 보자고~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823daf4",
   "metadata": {},
   "source": [
    "- 일단 가상환경부터 만들어줌! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd814f5f",
   "metadata": {},
   "source": [
    "환경쒝!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ade55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suakim/Documents/프로젝트/flutter 딥러닝 플젝/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.6\n",
      "TF: 2.16.1\n",
      "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "CWD: /Users/suakim/Documents/프로젝트/flutter 딥러닝 플젝/Model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, platform, pathlib\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"CWD:\", pathlib.Path().resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3131276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dogs images: 4978, breeds: 25\n",
      "예시 품종 10개: ['american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees']\n",
      "Oxford root: /Users/suakim/Documents/프로젝트/flutter 딥러닝 플젝/data_sources/oxford\n",
      "Export dir: /Users/suakim/Documents/프로젝트/flutter 딥러닝 플젝/Model/data\n"
     ]
    }
   ],
   "source": [
    "import pathlib, re, os, shutil\n",
    "\n",
    "# 1) Oxford 루트 자동 탐색\n",
    "def find_oxford_root():\n",
    "    here = pathlib.Path().resolve()\n",
    "    for p in [\n",
    "        here / \"data_sources\" / \"oxford\",\n",
    "        here.parent / \"data_sources\" / \"oxford\",\n",
    "        here.parent.parent / \"data_sources\" / \"oxford\",\n",
    "        here.parent.parent.parent / \"data_sources\" / \"oxford\",\n",
    "    ]:\n",
    "        if (p / \"annotations\" / \"list.txt\").exists() and (p / \"images\").exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "root = find_oxford_root()\n",
    "if root is None:\n",
    "    raise FileNotFoundError(\"Oxford-IIIT Pet 경로를 못 찾았어요. data_sources/oxford 아래에 images/와 annotations/list.txt가 있어야 합니다.\")\n",
    "\n",
    "img_dir, ann_dir = root/\"images\", root/\"annotations\"\n",
    "dest = pathlib.Path(\"data\"); dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "USE_HARDLINKS = True   # 같은 디스크면 용량 절약\n",
    "count = 0; breeds = set()\n",
    "\n",
    "with open(ann_dir/\"list.txt\") as f:\n",
    "    for raw in f:\n",
    "        # 2) 주석/여분 토큰 방어 코드\n",
    "        line = raw.split('#', 1)[0].strip()   # # 뒤 주석 제거\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        name, cls_id, species, breed_id = parts[:4]  # 앞 4개만 사용\n",
    "\n",
    "        # 3) 강아지(species==2)만 선별\n",
    "        try:\n",
    "            if int(species) != 2:   # 1=cat, 2=dog\n",
    "                continue\n",
    "        except ValueError:\n",
    "            continue  # 비정상 줄 방어\n",
    "\n",
    "        # 4) 파일명 → 품종명 추출 (맨 끝 _숫자 제거)\n",
    "        breed = re.sub(r\"_(\\d+)$\", \"\", name)\n",
    "        src = img_dir / (name + \".jpg\")\n",
    "        if not src.exists():\n",
    "            continue\n",
    "\n",
    "        (dest / breed).mkdir(parents=True, exist_ok=True)\n",
    "        dst = dest / breed / (name + \".jpg\")\n",
    "        try:\n",
    "            if USE_HARDLINKS:\n",
    "                if dst.exists(): dst.unlink()\n",
    "                os.link(src, dst)\n",
    "            else:\n",
    "                shutil.copy2(src, dst)\n",
    "        except OSError:\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "        breeds.add(breed); count += 1\n",
    "\n",
    "print(f\"✅ dogs images: {count}, breeds: {len(breeds)}\")\n",
    "print(\"예시 품종 10개:\", sorted(list(breeds))[:10])\n",
    "print(\"Oxford root:\", root)\n",
    "print(\"Export dir:\", dest.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5573f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /Users/suakim/Documents/프로젝트/flutter 딥러닝 플젝/Model/data\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "# Model 폴더에서 작업 중이면 \"data\", 루트면 \"Model/data\"\n",
    "candidates = [pathlib.Path(\"data\"), pathlib.Path(\"Model/data\")]\n",
    "for p in candidates:\n",
    "    if p.exists() and any(p.iterdir()):\n",
    "        DATA_DIR = p\n",
    "        break\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cfbb6",
   "metadata": {},
   "source": [
    "## 품종마다 갯수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae85a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 이미지: 4978\n",
      "클래스 수: 25\n",
      "상위 10개: [('wheaten_terrier', 200), ('great_pyrenees', 200), ('american_bulldog', 200), ('pomeranian', 200), ('american_pit_bull_terrier', 200), ('yorkshire_terrier', 200), ('japanese_chin', 200), ('miniature_pinscher', 200), ('basset_hound', 200), ('saint_bernard', 200)]\n"
     ]
    }
   ],
   "source": [
    "import pathlib, collections\n",
    "root = DATA_DIR\n",
    "cnt = {d.name: len(list(d.glob(\"*.jpg\"))) for d in root.iterdir() if d.is_dir()}\n",
    "print(\"총 이미지:\", sum(cnt.values()))\n",
    "print(\"클래스 수:\", len(cnt))\n",
    "print(\"상위 10개:\", sorted(cnt.items(), key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcedfc",
   "metadata": {},
   "source": [
    "## 자 이제 딥러닝을 내가 만들어보자\n",
    "- 선생님이 없이 혼자하려니까 정말 어렵다 일단 해보자 \n",
    "- import tensorflow as tf가 뭔데?\n",
    "- tensorflow: 구글이 만든 머신러닝/딥러닝 라이브러리.\n",
    "- 걍 내가 tensorflow 만든다 구글미안하다 쑤아가 간다 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n",
    "- ssuatensorflow gogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdacc1",
   "metadata": {},
   "source": [
    "### 역전파란? 멀까?\n",
    "\n",
    "- 결과(손실)가 조금 바뀔 때, 각 입력/가중치가 얼마나 ‘책임’이 있는지 거꾸로 계산해 주는 절차\n",
    "- 수학 규칙이 연쇄법칙(Chain Rule).\n",
    "- 여기서 말하는 손실은 (실제값 - 예측값)^ 을 의미함 (=MSE)\n",
    "- mse를 줄이기 위해서는 기울기를 줄여야 하니까 그래서 미분이 되어야 함\n",
    "- 미분이 되어야 되기 때문에 연쇄법칙이 이용됨\n",
    "- 연쇄법칙이 이용되는 이유는 모델의 손실 𝐿, L은 보통 여러 연산이 겹겹이 합성된 함수\n",
    "- 이때 dy/dx는 바로 못 구하니까 조각조각 내서 곱해준다고 생각하면 됨\n",
    "- 그래서 결론은 역전파 = 연쇄법칙 으로 보면 돼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0976125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssuatensortflow/tensor.py 갱신 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "code = r'''\n",
    "# ssuatensortflow/tensor.py\n",
    "# class Tensor : 자동미분 코어\n",
    "# ------------------------------------------------------------\n",
    "# \"값(data) + 그라디언트(grad) + 연산그래프(parent 연결) + 역전파 규칙(_backward)\"\n",
    "# 을 묶어서 다루는 아주 작은 자동미분 엔진의 코어입니다.\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"\n",
    "    숫자 배열(np.ndarray)을 감싸는 클래스.\n",
    "    - data: 실제 값 (float32 배열)\n",
    "    - requires_grad: 이 텐서에 대해 grad(∂L/∂self)를 추적할지 여부\n",
    "    - grad: backward() 후 채워지는 그라디언트 버퍼 (data와 같은 shape)\n",
    "    - _prev: 이 텐서를 만든 부모 텐서들(연산 그래프 연결용)\n",
    "    - _op: 어떤 연산으로 만들어졌는지(디버깅/시각화용 라벨)\n",
    "    - _backward: 이 노드에서 부모로 grad를 보내는 함수(연산별 규칙 저장)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, requires_grad=False, _children=(), _op=\"\"):\n",
    "        # 1. 값을 float32로 통일(연산 일관성/속도)\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "\n",
    "        # 2. 이 텐서에 대해 미분을 추적할지\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        # 3. grad 버퍼: 추적할 때만 0으로 초기화, 아니면 None\n",
    "        self.grad = np.zeros_like(self.data) if requires_grad else None\n",
    "\n",
    "        # 4. 부모 연결: 연산이 결과 텐서를 만들 때 (부모들)을 넘겨줌\n",
    "        self._prev = set(_children)\n",
    "\n",
    "        # 5. 어떤 연산의 결과인지 라벨(디버깅용)\n",
    "        self._op = _op\n",
    "\n",
    "        # 6. 역전파 규칙 슬롯: 각 연산이 out을 만들 때 여기에 규칙을 주입\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_tensor(x):\n",
    "        \"\"\"스칼라/ndarray도 Tensor로 통일해서 연산을 단순화.\"\"\"\n",
    "        return x if isinstance(x, Tensor) else Tensor(x)\n",
    "\n",
    "    # -------------------- 기본 연산들 --------------------\n",
    "    # +\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        덧셈: z = x + y\n",
    "        grad 규칙: ∂z/∂x = 1, ∂z/∂y = 1  → 부모의 grad에 out.grad를 그대로 더함\n",
    "        \"\"\"\n",
    "        other = Tensor._to_tensor(other)\n",
    "        out = Tensor(self.data + other.data,\n",
    "                     requires_grad=self.requires_grad or other.requires_grad,\n",
    "                     _children=(self, other), _op=\"+\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad\n",
    "            if other.requires_grad:\n",
    "                other.grad = other.grad + out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # 교환법칙임 그니까 반대로 들어오는거 자리 바꿔서 인식 한다고 \n",
    "    #  3 + x ==> x + 3 요런 느낌쓰 ㅇㅋ?\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"\n",
    "        전부합: s = x.sum()  (스칼라)\n",
    "        grad 규칙: ∂s/∂x = 1(같은 shape) → out.grad * 1 을 부모로 보냄\n",
    "        \"\"\"\n",
    "        out = Tensor(self.data.sum(),\n",
    "                     requires_grad=self.requires_grad,\n",
    "                     _children=(self,), _op=\"sum\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + np.ones_like(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # 부호반전: y = -x\n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        grad 규칙: ∂y/∂x = -1 → 부모 grad에 -out.grad 더하기\n",
    "        \"\"\"\n",
    "        out = Tensor(-self.data, requires_grad=self.requires_grad, _children=(self,), _op=\"neg\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad - out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # - 뺄셈: x - y = x + (-y)\n",
    "    def __sub__(self, other):\n",
    "        \n",
    "        other = Tensor._to_tensor(other)\n",
    "        return self.__add__(-other)\n",
    "    # 자리 빠꾼 - \n",
    "    def __rsub__(self, other):\n",
    "        other = Tensor._to_tensor(other)\n",
    "        return other.__sub__(self)\n",
    "    #  원소곱: z = x * y\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        grad 규칙: ∂z/∂x = y, ∂z/∂y = x → 부모 grad에 상대방 data를 곱해 누적\n",
    "        \"\"\"\n",
    "        other = Tensor._to_tensor(other)\n",
    "        out = Tensor(self.data * other.data,\n",
    "                     requires_grad=self.requires_grad or other.requires_grad,\n",
    "                     _children=(self, other), _op=\"*\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad * other.data\n",
    "            if other.requires_grad:\n",
    "                other.grad = other.grad + out.grad * self.data\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # 자리 바꾼 곱곱\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "\n",
    "    # 평균: m = x.mean() (스칼라)\n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        grad 규칙: ∂m/∂x = 1/N → ones_like(x) * (out.grad/N)\n",
    "        \"\"\"\n",
    "        n = self.data.size\n",
    "        out = Tensor(self.data.mean(),\n",
    "                     requires_grad=self.requires_grad,\n",
    "                     _children=(self,), _op=\"mean\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + (np.ones_like(self.data) * (out.grad / n))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    #  거듭제곱: y = x ** p  (p는 스칼라)\n",
    "    def pow(self, p: float):\n",
    "        \"\"\"\n",
    "        grad 규칙: ∂y/∂x = p * x^(p-1)\n",
    "        \"\"\"\n",
    "        out = Tensor(self.data ** p,\n",
    "                     requires_grad=self.requires_grad,\n",
    "                     _children=(self,), _op=f\"pow{p}\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad * (p * (self.data ** (p - 1)))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        행렬곱: Z = A @ B  (단순화: 2D @ 2D만 지원)\n",
    "        grad 규칙:\n",
    "          dL/dA = dL/dZ @ B^T\n",
    "          dL/dB = A^T @ dL/dZ\n",
    "        \"\"\"\n",
    "        other = Tensor._to_tensor(other)\n",
    "        A, B = self.data, other.data\n",
    "        out = Tensor(A @ B,\n",
    "                     requires_grad=self.requires_grad or other.requires_grad,\n",
    "                     _children=(self, other), _op=\"@\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad @ B.T\n",
    "            if other.requires_grad:\n",
    "                other.grad = other.grad + A.T @ out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # 왜 음수값은 뱉어내고 양수 값만 받을까\n",
    "    # z > 0 : “그 특징이 있다고 판단” → 값을 그대로 다음 층에 전달\n",
    "    # z ≤ 0 : “그 특징이 없다/약하다” → 0(침묵) 으로 보냄\n",
    "    # 특징이 있냐 없냐를 보고 판단 하는 함수라서 그런거임 ㅇㅇ\n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        ReLU: y = max(0, x)\n",
    "        grad 규칙: ∂y/∂x = 1(x>0), else 0  → 마스크로 곱해 전달\n",
    "        \"\"\"\n",
    "        mask = (self.data > 0).astype(np.float32)\n",
    "        out = Tensor(self.data * mask,\n",
    "                     requires_grad=self.requires_grad,\n",
    "                     _children=(self,), _op=\"relu\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad = self.grad + out.grad * mask\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # -------------------- 역전파 엔진 --------------------\n",
    "    # 연쇄법칙(Chain Rule)을 이용해, 현재 계산 그래프의 모든 텐서에 대한 기울기(gradient)를 자동으로 계산해서 Tensor.grad에 채워넣는 함수\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        loss(보통 스칼라)에서 시작해 부모 방향으로 grad를 전파.\n",
    "        # 절차:\n",
    "        # 1) 위상정렬\n",
    "        # 2) 모든 grad 0으로 초기화\n",
    "        # 3) 루트(self)의 grad = 1 설정 (dL/dL = 1)\n",
    "        # 4) 리스트를 뒤집어 각 노드의 _backward() 실행 → 부모 grad 누적\n",
    "        \"\"\"\n",
    "        # 1. 그래프 순서 만들기  위상정렬(DFS)\n",
    "        #    자식이 뒤, 부모가 앞\n",
    "        topo, visited = [], set()\n",
    "        def build(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for p in v._prev:\n",
    "                    build(p)\n",
    "                topo.append(v)\n",
    "        build(self)\n",
    "\n",
    "        # 2. 전파는 누적(+=) 이므로, 이전 값이 남아있지 않게 0에서 시작.\n",
    "        for v in topo:\n",
    "            if v.requires_grad:\n",
    "                if v.grad is None:\n",
    "                    v.grad = np.zeros_like(v.data)\n",
    "                else:\n",
    "                    v.grad[...] = 0.0\n",
    "\n",
    "        # (3) 루트 grad = 1 (loss에서 시작)\n",
    "        if self.grad is None:\n",
    "            self.grad = np.ones_like(self.data)\n",
    "        else:\n",
    "            self.grad[...] = 1.0\n",
    "\n",
    "        # (4) 자식 - >부모로 역전파\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "'''\n",
    "Path(\"ssuatensortflow/tensor.py\").write_text(code, encoding=\"utf-8\")\n",
    "print(\"ssuatensortflow/tensor.py 갱신 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334655ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ssuatensortflow/nn.py 갱신 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "code = r'''\n",
    "# ssuatensortflow/nn.py\n",
    "# 손실 함수\n",
    "# ------------------------------------------------------------\n",
    "# 분류용 손실: 소프트맥스 + 크로스엔트로피(수치안정형)\n",
    "# 다중분류(multi-class classification) 문제에서 사용되는 손실(loss) 함수\n",
    "# logits: (B, C) Tensor, y_int: (B,) int64 numpy array\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "from .tensor import Tensor\n",
    "\n",
    "def softmax_cross_entropy(logits: Tensor, y_int: np.ndarray) -> Tensor:\n",
    "    \"\"\"\n",
    "    안정형 softmax:\n",
    "    # 약간 softmax = 값을 확률로 바꿔준다고 생각하면 됨 \n",
    "    # 모든 클래스의 합은 1이다\n",
    "    # croos_sentropy = 예측값과 실제 값과의 불일치 정도라고 보면 되는데 \n",
    "    # 정답 클래스의 확률을 높이고, 오답 클래스의 확률을 낮추도록\n",
    "      z = logits - max(logits, axis=1)\n",
    "      probs = exp(z) / sum(exp(z), axis=1)\n",
    "\n",
    "    교차엔트로피:\n",
    "      L = - 평균( log( probs[range(B), y] ) )\n",
    "\n",
    "    역전파 규칙(유도 결과):\n",
    "      dL/dlogits = (probs - onehot(y)) / B\n",
    "    \"\"\"\n",
    "    assert logits.data.ndim == 2, \"logits must be (B, C)\"\n",
    "    B, C = logits.data.shape\n",
    "    y_int = y_int.astype(np.int64)\n",
    "\n",
    "    # (1) 안정형 softmax\n",
    "    z = logits.data - logits.data.max(axis=1, keepdims=True)  # overflow 방지\n",
    "    exp = np.exp(z)\n",
    "    probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # (2) cross-entropy\n",
    "    eps = 1e-12\n",
    "    loss_val = -np.mean(np.log(probs[np.arange(B), y_int] + eps))\n",
    "\n",
    "    # (3) 결과 텐서(스칼라)\n",
    "    out = Tensor(loss_val, requires_grad=logits.requires_grad,\n",
    "                 _children=(logits,), _op=\"softmax_ce\")\n",
    "\n",
    "    # (4) 역전파: dL/dlogits = (probs - onehot)/B\n",
    "    y_onehot = np.eye(C, dtype=np.float32)[y_int]\n",
    "    def _backward():\n",
    "        if logits.requires_grad:\n",
    "            grad_logits = (probs - y_onehot) / B\n",
    "            logits.grad = logits.grad + grad_logits\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "'''\n",
    "Path(\"ssuatensortflow/nn.py\").write_text(code, encoding=\"utf-8\")\n",
    "print(\" ssuatensortflow/nn.py 갱신 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "79b2d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# ssuatensortflow/layers.py\n",
    "# ------------------------------------------------------------\n",
    "# 간단 레이어/옵티마이저 (개선판):\n",
    "#   - add_bias: (B,F) + (1,F) 전용 덧셈(브로드캐스팅 없이 역전파 안전)\n",
    "#   - dropout(x,p,train): 함수형 드롭아웃 (inverted 방식)\n",
    "#   - Linear(in,out, init=...): Xavier/He 초기화 선택\n",
    "#   - SGD: momentum / weight_decay(L2) / grad clipping 지원\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "code = r'''\n",
    "# Tensor 임포트 (노트북/스크립트 실행 맥락 모두 지원)\n",
    "import numpy as np\n",
    "from .tensor import Tensor\n",
    "\n",
    "\n",
    "# ======= 유틸: fan_in/out & 초기화 =======\n",
    "\n",
    "def _fan_in_out(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    else:\n",
    "        # Conv 등 고려할 때의 일반식(여긴 2D만 쓰지만 안전하게)\n",
    "        fan_in = shape[0] if len(shape) > 0 else 1\n",
    "        fan_out = shape[1] if len(shape) > 1 else 1\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _xavier_uniform(shape, rng):\n",
    "    fan_in, fan_out = _fan_in_out(shape)\n",
    "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return rng.uniform(-limit, limit, size=shape).astype(np.float32)\n",
    "\n",
    "def _he_uniform(shape, rng):\n",
    "    fan_in, _ = _fan_in_out(shape)\n",
    "    limit = np.sqrt(6.0 / max(1, fan_in))\n",
    "    return rng.uniform(-limit, limit, size=shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# ======= Op: add_bias =======\n",
    "\n",
    "def add_bias(z: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    (B,F) + (1,F) 형태의 편향 덧셈을 전용 op로 구현.\n",
    "    역전파 시 b.grad는 배치 축으로 합산해 (1,F) 모양으로 누적.\n",
    "    \"\"\"\n",
    "    out = Tensor(z.data + b.data,\n",
    "                 requires_grad=z.requires_grad or b.requires_grad,\n",
    "                 _children=(z, b), _op=\"add_bias\")\n",
    "    def _backward():\n",
    "        if z.requires_grad:\n",
    "            if z.grad is None:\n",
    "                z.grad = np.zeros_like(z.data, dtype=z.data.dtype)\n",
    "            z.grad = z.grad + out.grad\n",
    "        if (b is not None) and b.requires_grad:\n",
    "            if b.grad is None:\n",
    "                b.grad = np.zeros_like(b.data, dtype=b.data.dtype)\n",
    "            b.grad = b.grad + out.grad.sum(axis=0, keepdims=True)\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======= Op: dropout (inverted) =======\n",
    "\n",
    "def dropout(x: Tensor, p: float = 0.5, train: bool = True) -> Tensor:\n",
    "    \"\"\"\n",
    "    함수형 드롭아웃 (학습시만 적용).\n",
    "    inverted dropout: 학습 시 1/(1-p)로 스케일 → 추론 시 별도 보정 불필요.\n",
    "    \"\"\"\n",
    "    assert 0.0 <= p < 1.0, \"dropout p must be in [0,1)\"\n",
    "    if (not train) or p == 0.0:\n",
    "        # 추론 모드 또는 p=0: 그래프는 유지\n",
    "        out = Tensor(x.data.copy(), requires_grad=x.requires_grad, _children=(x,), _op=\"dropout_pass\")\n",
    "        def _backward():\n",
    "            if x.requires_grad:\n",
    "                if x.grad is None:\n",
    "                    x.grad = np.zeros_like(x.data, dtype=x.data.dtype)\n",
    "                x.grad = x.grad + out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    keep = 1.0 - p\n",
    "    rng = np.random.default_rng()\n",
    "    mask = (rng.random(x.data.shape) < keep).astype(x.data.dtype)\n",
    "    y_data = (x.data * mask) / keep\n",
    "\n",
    "    out = Tensor(y_data, requires_grad=x.requires_grad, _children=(x,), _op=\"dropout\")\n",
    "    def _backward():\n",
    "        if x.requires_grad:\n",
    "            if x.grad is None:\n",
    "                x.grad = np.zeros_like(x.data, dtype=x.data.dtype)\n",
    "            x.grad = x.grad + (out.grad * mask) / keep\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======= Layer: Linear =======\n",
    "\n",
    "class Linear:\n",
    "    \"\"\"\n",
    "    완전연결층: y = x @ W (+ b)\n",
    "      - W: (in_features, out_features)\n",
    "      - b: (1, out_features)\n",
    "    init:\n",
    "      - 'xavier' (기본)\n",
    "      - 'he'     (ReLU 계열에서 권장)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int,\n",
    "                 bias: bool = True, rng=None, init: str = \"xavier\"):\n",
    "        rng = np.random.default_rng() if rng is None else rng\n",
    "        shape = (in_features, out_features)\n",
    "        init = (init or \"xavier\").lower()\n",
    "\n",
    "        if init == \"he\":\n",
    "            W_init = _he_uniform(shape, rng)\n",
    "        else:\n",
    "            W_init = _xavier_uniform(shape, rng)\n",
    "\n",
    "        self.W = Tensor(W_init, requires_grad=True)\n",
    "        self.b = Tensor(np.zeros((1, out_features), np.float32), requires_grad=True) if bias else None\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"학습 가능한 파라미터 리스트(W, b).\"\"\"\n",
    "        return [p for p in (self.W, self.b) if p is not None]\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"순전파: y = x @ W (+ b)\"\"\"\n",
    "        y = x @ self.W\n",
    "        if self.b is not None:\n",
    "            y = add_bias(y, self.b)\n",
    "        return y\n",
    "\n",
    "\n",
    "# ======= Optimizer: SGD (momentum / L2 / grad clip) =======\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    확장 SGD:\n",
    "      - momentum: v = m*v - lr*g ; p += v\n",
    "      - weight_decay: g <- g + wd * p.data (L2 regularization)\n",
    "      - max_norm: 전체 grad L2를 max_norm으로 클립\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.1, momentum=0.0, weight_decay=0.0, max_norm=None):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_norm = max_norm\n",
    "        self._vel = {id(p): np.zeros_like(p.data, dtype=p.data.dtype) for p in self.params}\n",
    "\n",
    "    def _apply_weight_decay(self, p, g):\n",
    "        if self.weight_decay != 0.0:\n",
    "            g = g + self.weight_decay * p.data\n",
    "        return g\n",
    "\n",
    "    def _clip_grad_norm_(self):\n",
    "        if self.max_norm is None:\n",
    "            return\n",
    "        total_sq = 0.0\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                total_sq += float(np.sum(p.grad * p.grad))\n",
    "        total_norm = np.sqrt(total_sq)\n",
    "        if total_norm > self.max_norm and total_norm > 0:\n",
    "            scale = self.max_norm / (total_norm + 1e-12)\n",
    "            for p in self.params:\n",
    "                if p.grad is not None:\n",
    "                    p.grad *= scale\n",
    "\n",
    "    def step(self):\n",
    "        # 그라드 클리핑(옵션)\n",
    "        self._clip_grad_norm_()\n",
    "\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = self._apply_weight_decay(p, p.grad)\n",
    "\n",
    "            if self.momentum != 0.0:\n",
    "                v = self._vel[id(p)]\n",
    "                v[:] = self.momentum * v - self.lr * g\n",
    "                p.data += v\n",
    "            else:\n",
    "                p.data -= self.lr * g\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad[...] = 0.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711fa32",
   "metadata": {},
   "source": [
    "# importlib 쓰는 이유 \n",
    "- 노트북에서 우리가 ssuatensortflow/tensor.py 파일을 고쳐도 이해를 못 할 수가 있으니까 리로드 할려고 쓰임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fc431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.data: [11. 22. 33.]\n",
      "s.data: 66.0\n",
      "x.grad: [1. 1. 1.]\n",
      "y.grad: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import importlib, ssuatensortflow as stf\n",
    "# 함수 바뀐거 있으면 로드해라\n",
    "importlib.reload(stf ) # 내가 만들어 둔 stf 약어 ㅇㅇ \n",
    "\n",
    "\n",
    "x = stf.Tensor([1,2,3], requires_grad=True)\n",
    "y = stf.Tensor([10,20,30], requires_grad=True)\n",
    "\n",
    "z = x + y      # [11, 22, 33]\n",
    "s = z.sum()    # 66 (스칼라)\n",
    "s.backward()\n",
    "\n",
    "print(\"z.data:\", z.data)   # [11. 22. 33.]\n",
    "print(\"s.data:\", s.data)   # 66.0\n",
    "print(\"x.grad:\", x.grad)   # [1. 1. 1.]\n",
    "print(\"y.grad:\", y.grad)   # [1. 1. 1.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49820d27",
   "metadata": {},
   "source": [
    "파이썬은 x + y를 내부적으로 x.__add__(y) 로 바꿔 호출\n",
    "\n",
    "우리 Tensor.__add__ 메서드 안에서 직접 새 텐서를 만들 때 _op=\"+\"를 넣어줬어.\n",
    "\n",
    "그래서 결과 텐서의 _op가 \"+\"가 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db300704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x._op: \n",
      "z._op: +\n",
      "s._op: sum\n"
     ]
    }
   ],
   "source": [
    "import importlib, ssuatensortflow as stf\n",
    "importlib.reload(stf)\n",
    "\n",
    "x = stf.Tensor([1,2,3], requires_grad=True)   # 리프: _op == \"\"\n",
    "y = stf.Tensor([10,20,30], requires_grad=True)\n",
    "\n",
    "z = x + y     # z._op == \"+\"\n",
    "s = z.sum()   # s._op == \"sum\"\n",
    "\n",
    "print(\"x._op:\", x._op)  # \"\"\n",
    "print(\"z._op:\", z._op)  # \"+\"\n",
    "print(\"s._op:\", s._op)  # \"sum\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb3fba",
   "metadata": {},
   "source": [
    "### 모듈 리로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a5a06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, numpy as np, pathlib, random\n",
    "from PIL import Image\n",
    "import ssuatensortflow as stf\n",
    "import ssuatensortflow.nn as nn\n",
    "import ssuatensortflow.layers as L\n",
    "importlib.reload(stf); importlib.reload(nn); importlib.reload(L)\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "assert DATA_DIR.exists(), DATA_DIR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55a65159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경로: /Users/suakim/Documents/프로젝트/flutter 딥러닝 플젝/Model/data\n",
      "exists? True\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "print(\"경로:\", DATA_DIR.resolve())\n",
    "print(\"exists?\", DATA_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1801f6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 수: 25\n",
      "예시 품종 5개: ['american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer']\n",
      "총 이미지 개수: 4978\n",
      "Train: 3980, Val: 998\n",
      "입력 차원: 4096\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "IMG_SIZE = 64  # 한 이미지를 64x64로 축소\n",
    "\n",
    "# (1) 품종별 폴더 확인\n",
    "breeds = sorted([p.name for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "breed_to_idx = {b:i for i,b in enumerate(breeds)}\n",
    "num_classes = len(breeds)\n",
    "print(f\"클래스 수: {num_classes}\")\n",
    "print(\"예시 품종 5개:\", breeds[:5])\n",
    "\n",
    "# (2) 이미지 경로 수집\n",
    "files = []\n",
    "for b in breeds:\n",
    "    for jpg in (DATA_DIR/b).glob(\"*.jpg\"):\n",
    "        files.append((jpg, breed_to_idx[b]))\n",
    "print(f\"총 이미지 개수: {len(files)}\")\n",
    "\n",
    "# (3) train/val 8:2 분할\n",
    "random.seed(0)\n",
    "by_class = defaultdict(list)\n",
    "for fp, y in files:\n",
    "    by_class[y].append(fp)\n",
    "\n",
    "train, val = [], []\n",
    "for y, fps in by_class.items():\n",
    "    random.shuffle(fps)\n",
    "    k = int(len(fps)*0.8)\n",
    "    train += [(fp, y) for fp in fps[:k]]\n",
    "    val   += [(fp, y) for fp in fps[k:]]\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}\")\n",
    "\n",
    "# (4) 이미지를 로드해 numpy로 변환하는 함수\n",
    "# 흑백으로 바꾸는 거 \n",
    "# def load_img_as_vec(path, size=IMG_SIZE):\n",
    "#     im = Image.open(path).convert(\"L\").resize((size, size))\n",
    "#     arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "#     return arr.reshape(-1)  # (D,)\n",
    "#D = IMG_SIZE * IMG_SIZE\n",
    "\n",
    "def load_img_as_vec(path, size=IMG_SIZE):\n",
    "    im = Image.open(path).convert(\"RGB\").resize((size, size))\n",
    "    arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "    return arr.reshape(-1)  # (D,)\n",
    "\n",
    "\n",
    "print(f\"입력 차원: {D}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5349cf",
   "metadata": {},
   "source": [
    "## 전체 이미지 인덱스 리스트 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7a14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(pairs, batch_size=64, shuffle=True):\n",
    "    idxs = np.arange(len(pairs))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idxs)\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        j = idxs[i:i+batch_size]\n",
    "        xs = [load_img_as_vec(pairs[k][0]) for k in j]\n",
    "        ys = [pairs[k][1] for k in j]\n",
    "        X = np.stack(xs).astype(np.float32)     \n",
    "        y = np.array(ys, dtype=np.int64)        \n",
    "        yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61389369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ssuatensortflow.tensor as T\n",
    "import ssuatensortflow.layers as L\n",
    "import ssuatensortflow.nn as nn\n",
    "import ssuatensortflow as stf\n",
    "\n",
    "# 1) 모듈을 'tensor -> layers -> nn -> stf' 순으로 리로드\n",
    "importlib.reload(T)\n",
    "importlib.reload(L)\n",
    "importlib.reload(nn)\n",
    "importlib.reload(stf)\n",
    "\n",
    "# 2) (중요) 기존 모델/옵티마이저 객체 버리고 새로 생성\n",
    "# 흑백일 때 차원이 적어서 \n",
    "# D = IMG_SIZE * IMG_SIZE         # 이미 위에서 정의했음(64x64=4096)\n",
    "# fc1 = L.Linear(D, 256, bias=True)\n",
    "# fc2 = L.Linear(256, len(breeds), bias=True)\n",
    "\n",
    "# 칼라로 바꾸고 차원이 늘었음 \n",
    "D = IMG_SIZE * IMG_SIZE * 3  # 이제 3채널\n",
    "fc1 = L.Linear(D, 1024)       # 차원 늘어나서 은닉층도 조금 크게!\n",
    "fc2 = L.Linear(1024, num_classes)\n",
    "\n",
    "\n",
    "def forward(X_np):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)  # 이제 '새 Tensor' 클래스\n",
    "    h = fc1(x).relu()\n",
    "    logits = fc2(h)\n",
    "    return logits\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(), lr=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "658629e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] loss=4.6506\n",
      "[02] loss=3.1939\n",
      "[03] loss=3.2089\n",
      "[04] loss=3.1773\n",
      "[05] loss=3.1771\n",
      "[06] loss=3.1775\n",
      "[07] loss=3.2173\n",
      "[08] loss=3.1760\n",
      "[09] loss=3.1726\n",
      "[10] loss=3.2023\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH  = 64\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "    for X, y in make_batches(train, batch_size=BATCH, shuffle=True):\n",
    "        logits = forward(X)\n",
    "        loss   = nn.softmax_cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(float(loss.data))\n",
    "    print(f\"[{ep:02d}] loss={np.mean(losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b404fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ssuatensortflow.tensor as T\n",
    "import ssuatensortflow.layers as L\n",
    "import ssuatensortflow.nn as nn\n",
    "import ssuatensortflow as stf\n",
    "\n",
    "# 1) 모듈을 'tensor -> layers -> nn -> stf' 순으로 리로드\n",
    "importlib.reload(T)\n",
    "importlib.reload(L)\n",
    "importlib.reload(nn)\n",
    "importlib.reload(stf)\n",
    "\n",
    "# D = IMG_SIZE * IMG_SIZE * 3\n",
    "# num_classes = len(breeds)\n",
    "\n",
    "fc1 = L.Linear(D, 1024, init=\"he\")\n",
    "fc2 = L.Linear(1024, num_classes, init=\"xavier\")\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(),\n",
    "            lr=0.01, momentum=0.9, weight_decay=1e-4, max_norm=5.0)\n",
    "\n",
    "def forward(X_np, train=True):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x).relu()\n",
    "    h = L.dropout(h, p=0.3, train=train)  # 학습 시만 활성\n",
    "    logits = fc2(h)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2e964c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 테스트 성공, loss = 2.048614501953125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ssuatensortflow as stf\n",
    "import ssuatensortflow.layers as L\n",
    "import ssuatensortflow.nn as nn\n",
    "\n",
    "D, num_classes = 64*64*3, 10  # 예시용\n",
    "X_dummy = np.random.rand(8, D).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, num_classes, size=(8,))\n",
    "\n",
    "fc1 = L.Linear(D, 1024, init=\"he\")\n",
    "fc2 = L.Linear(1024, num_classes, init=\"xavier\")\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(),\n",
    "            lr=0.01, momentum=0.9, weight_decay=1e-4, max_norm=5.0)\n",
    "\n",
    "def forward(X_np, train=True):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x).relu()\n",
    "    h = L.dropout(h, p=0.3, train=train)\n",
    "    logits = fc2(h)\n",
    "    return logits\n",
    "\n",
    "# 순전파 + 역전파 테스트\n",
    "logits = forward(X_dummy)\n",
    "loss = nn.softmax_cross_entropy(logits, y_dummy)\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "print(\"✅ 테스트 성공, loss =\", float(loss.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb9c99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오버핏 테스트 전용\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(), lr=0.05)  # momentum=0, wd=0\n",
    "def forward_overfit(X_np):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x).relu()\n",
    "    logits = fc2(h)  # dropout 끔\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "efc2ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] classes=25, train=3980, val=998, D=12288, mean=[0.48045987 0.45337072 0.39187315], std=[0.2551374  0.25123423 0.2606304 ]\n"
     ]
    }
   ],
   "source": [
    "# train_rgb_mlp.py  (1) 데이터 준비\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import numpy as np, random\n",
    "\n",
    "DATA_DIR = Path(\"data\")          # 품종 폴더들이 들어있는 루트\n",
    "IMG_SIZE = 64\n",
    "random.seed(0); np.random.seed(0)\n",
    "\n",
    "# 1) 클래스/라벨\n",
    "breeds = sorted([p.name for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "breed_to_idx = {b:i for i,b in enumerate(breeds)}\n",
    "num_classes = len(breeds)\n",
    "\n",
    "files = []\n",
    "for b in breeds:\n",
    "    for jpg in (DATA_DIR/b).glob(\"*.jpg\"):\n",
    "        files.append((jpg, breed_to_idx[b]))\n",
    "\n",
    "# 2) 클래스별 8:2 분할\n",
    "by_class = defaultdict(list)\n",
    "for fp, y in files: by_class[y].append(fp)\n",
    "\n",
    "train, val = [], []\n",
    "for y, fps in by_class.items():\n",
    "    random.shuffle(fps)\n",
    "    k = int(len(fps)*0.8)\n",
    "    train += [(fp, y) for fp in fps[:k]]\n",
    "    val   += [(fp, y) for fp in fps[k:]]\n",
    "\n",
    "# 3) 채널별 mean/std 추정\n",
    "def compute_mean_std(pairs, n_samples=1024):\n",
    "    sel = pairs if len(pairs)<=n_samples else random.sample(pairs, n_samples)\n",
    "    xs = []\n",
    "    for fp,_ in sel:\n",
    "        im = Image.open(fp).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "        arr = np.asarray(im, np.float32)/255.0   # (H,W,3)\n",
    "        xs.append(arr.reshape(-1,3))\n",
    "    X = np.concatenate(xs, axis=0)\n",
    "    mean = X.mean(axis=0); std = X.std(axis=0) + 1e-6\n",
    "    return mean, std\n",
    "\n",
    "mean3, std3 = compute_mean_std(train)\n",
    "\n",
    "# 4) 로더 & 미니배치\n",
    "def load_img_as_vec(path):\n",
    "    im = Image.open(path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR)\n",
    "    arr = np.asarray(im, np.float32)/255.0\n",
    "    arr = (arr - mean3)/std3\n",
    "    return arr.reshape(-1)  # (D,)\n",
    "\n",
    "def iterate_minibatches(pairs, batch_size=64, shuffle=True):\n",
    "    idx = np.arange(len(pairs))\n",
    "    if shuffle: np.random.shuffle(idx)\n",
    "    for s in range(0, len(pairs), batch_size):\n",
    "        sel = idx[s:s+batch_size]\n",
    "        Xb = np.stack([load_img_as_vec(pairs[i][0]) for i in sel], axis=0)\n",
    "        yb = np.array([pairs[i][1] for i in sel], dtype=np.int64)\n",
    "        yield Xb, yb\n",
    "\n",
    "D = IMG_SIZE*IMG_SIZE*3\n",
    "print(f\"[DATA] classes={num_classes}, train={len(train)}, val={len(val)}, D={D}, mean={mean3}, std={std3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "30369b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rgb_mlp.py  (2) 모델/옵티마이저/forward\n",
    "import importlib\n",
    "import ssuatensortflow as stf\n",
    "import ssuatensortflow.layers as L\n",
    "import ssuatensortflow.nn as nn\n",
    "\n",
    "importlib.reload(L); importlib.reload(nn)\n",
    "\n",
    "fc1 = L.Linear(D, 1024, init=\"he\")\n",
    "fc2 = L.Linear(1024, num_classes, init=\"xavier\")\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + fc2.parameters(),\n",
    "            lr=0.01, momentum=0.9, weight_decay=1e-4, max_norm=5.0)\n",
    "\n",
    "def forward(X_np, train=True):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x).relu()\n",
    "    h = L.dropout(h, p=0.3, train=train)   # 학습 때만 활성\n",
    "    logits = fc2(h)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9154e60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train_loss=4.1572 acc=0.093 | val_loss=3.8882 acc=0.122 | lr=0.01\n",
      "[02] train_loss=3.2960 acc=0.220 | val_loss=3.9292 acc=0.118 | lr=0.01\n",
      "[03] train_loss=2.6762 acc=0.312 | val_loss=4.0136 acc=0.142 | lr=0.01\n",
      "[04] train_loss=2.3449 acc=0.395 | val_loss=3.8690 acc=0.138 | lr=0.01\n",
      "[05] train_loss=1.9013 acc=0.470 | val_loss=4.1558 acc=0.144 | lr=0.01\n",
      "[06] train_loss=1.6769 acc=0.535 | val_loss=4.4493 acc=0.147 | lr=0.01\n",
      "[07] train_loss=1.4803 acc=0.591 | val_loss=4.4037 acc=0.152 | lr=0.01\n",
      "  ↳ plateau detected: reduce lr to 0.005\n",
      "[08] train_loss=0.9989 acc=0.712 | val_loss=4.0707 acc=0.156 | lr=0.005\n",
      "[09] train_loss=0.7182 acc=0.787 | val_loss=4.2060 acc=0.171 | lr=0.005\n",
      "[10] train_loss=0.6224 acc=0.831 | val_loss=4.2276 acc=0.161 | lr=0.005\n",
      "  ↳ plateau detected: reduce lr to 0.0025\n"
     ]
    }
   ],
   "source": [
    "# train_rgb_mlp.py  (3) 평가 + 학습 루프\n",
    "def evaluate(pairs, batch_size=128):\n",
    "    losses, correct, total = [], 0, 0\n",
    "    for Xb, yb in iterate_minibatches(pairs, batch_size=batch_size, shuffle=False):\n",
    "        logits = forward(Xb, train=False)\n",
    "        loss = nn.softmax_cross_entropy(logits, yb)\n",
    "        losses.append(float(loss.data))\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        correct += int((pred == yb).sum())\n",
    "        total   += int(yb.size)\n",
    "    return (np.mean(losses) if losses else float(\"nan\")), (correct/total if total else 0.0)\n",
    "\n",
    "EPOCHS = 10\n",
    "best_val = float(\"inf\")\n",
    "patience, bad = 3, 0\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    # ---- Train ----\n",
    "    train_losses = []\n",
    "    train_correct, train_total = 0, 0\n",
    "\n",
    "    for Xb, yb in iterate_minibatches(train, batch_size=64, shuffle=True):\n",
    "        logits = forward(Xb, train=True)\n",
    "        loss = nn.softmax_cross_entropy(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        train_losses.append(float(loss.data))\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        train_correct += int((pred == yb).sum())\n",
    "        train_total   += int(yb.size)\n",
    "\n",
    "    tr_loss = np.mean(train_losses)\n",
    "    tr_acc  = train_correct / train_total\n",
    "\n",
    "    # ---- Val ----\n",
    "    val_loss, val_acc = evaluate(val)\n",
    "    print(f\"[{ep:02d}] train_loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.3f} | lr={opt.lr:g}\")\n",
    "\n",
    "    # plateau → lr 반감\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            opt.lr *= 0.5\n",
    "            bad = 0\n",
    "            print(f\"  ↳ plateau detected: reduce lr to {opt.lr:g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835a853",
   "metadata": {},
   "source": [
    "transforms: 데이터 다양화 → 과적합 감소, 일반화↑\n",
    "\n",
    "BatchNorm: 내부 공변량 변화 감소 → 수렴 빨라짐 + 초기화/학습률에 덜 민감\n",
    "\n",
    "ReduceLROnPlateau: 멈추면 자동으로 lr 낮춤 → 더 낮은 최소점 탐색\n",
    "\n",
    "L2/Dropout 강화: 학습셋 암기(오버핏) 억제 → val 성능 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839805ed",
   "metadata": {},
   "source": [
    "Tensor 연산 구조\n",
    "\n",
    "Linear, BatchNorm1d, SGD 같은 기본 레이어와 옵티마이저\n",
    "\n",
    "softmax_cross_entropy 손실\n",
    "\n",
    "DataLoader, Compose 등 데이터 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "23d71997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssuatensortflow.transforms as Tfm\n",
    "import ssuatensortflow.utils as U\n",
    "import ssuatensortflow.schedulers as S\n",
    "U.seed_everything(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d5b04429",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = Tfm.Compose([\n",
    "    Tfm.RandomHorizontalFlip(p=0.5),\n",
    "    Tfm.RandomPadCrop(IMG_SIZE, pad=4),\n",
    "    Tfm.ToRGBResize(IMG_SIZE),\n",
    "    Tfm.Normalize01(mean3, std3)\n",
    "])\n",
    "val_tf = Tfm.Compose([\n",
    "    Tfm.ToRGBResize(IMG_SIZE),\n",
    "    Tfm.Normalize01(mean3, std3)\n",
    "])\n",
    "\n",
    "def load_img_as_vec(path, train_mode=False):\n",
    "    img = Image.open(path)\n",
    "    tfm = train_tf if train_mode else val_tf\n",
    "    arr = tfm(img)                 # (H,W,3)\n",
    "    return Tfm.to_vec(arr)         # (D,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2278f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(pairs, batch_size=64, shuffle=True, train_mode=False):\n",
    "    idx = np.arange(len(pairs))\n",
    "    if shuffle: np.random.shuffle(idx)\n",
    "    for s in range(0, len(pairs), batch_size):\n",
    "        sel = idx[s:s+batch_size]\n",
    "        Xb = np.stack([load_img_as_vec(pairs[i][0], train_mode=train_mode) for i in sel], axis=0)\n",
    "        yb = np.array([pairs[i][1] for i in sel], dtype=np.int64)\n",
    "        yield Xb, yb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a30cc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = L.Linear(D, 512, init=\"he\")\n",
    "bn1 = L.BatchNorm1d(512)          # ★ 추가\n",
    "fc2 = L.Linear(512, num_classes, init=\"xavier\")\n",
    "\n",
    "opt = L.SGD(fc1.parameters() + bn1.parameters() + fc2.parameters(),\n",
    "            lr=0.005, momentum=0.9, weight_decay=3e-4, max_norm=5.0)\n",
    "\n",
    "def forward(X_np, train=True):\n",
    "    x = stf.Tensor(X_np, requires_grad=False)\n",
    "    h = fc1(x)\n",
    "    h = bn1(h, train=train)       # ★ BN\n",
    "    h = h.relu()\n",
    "    h = L.dropout(h, p=0.5, train=train)\n",
    "    logits = fc2(h)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "15407b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = S.ReduceLROnPlateau(opt, factor=0.5, patience=2, min_lr=1e-6, threshold=1e-4)\n",
    "best_path = \"best_fc_bn.npz\"\n",
    "best_val = float(\"inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "126b2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "if val_loss < best_val - 1e-4:\n",
    "    best_val = val_loss\n",
    "    U.save_checkpoint(best_path, [fc1, fc2])  # bn1은 러닝통계만 있고 가중치 gamma/beta는 bn1.parameters()로 포함(이미 opt에 포함)\n",
    "# 스케줄\n",
    "reduced = scheduler.step(val_loss)\n",
    "if reduced:\n",
    "    print(f\"  ↳ ReduceLROnPlateau: lr -> {opt.lr:g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4f694e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1d 존재? -> True\n",
      "layers.py 경로: /Users/suakim/Documents/프로젝트/flutter 딥러닝 플젝/Model/ssuatensortflow/layers.py\n"
     ]
    }
   ],
   "source": [
    "import importlib, inspect\n",
    "import ssuatensortflow.layers as L\n",
    "importlib.reload(L)\n",
    "print(\"BatchNorm1d 존재? ->\", hasattr(L, \"BatchNorm1d\"))\n",
    "print(\"layers.py 경로:\", inspect.getfile(L))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3e36fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train_loss=3.7451 acc=0.048 | val_loss=3.2558 acc=0.054 | lr=0.005\n",
      "[02] train_loss=3.5702 acc=0.058 | val_loss=3.2366 acc=0.044 | lr=0.005\n",
      "[03] train_loss=3.5212 acc=0.052 | val_loss=3.2271 acc=0.053 | lr=0.005\n",
      "[04] train_loss=3.4932 acc=0.065 | val_loss=3.2243 acc=0.049 | lr=0.005\n",
      "[05] train_loss=3.4488 acc=0.059 | val_loss=3.2216 acc=0.061 | lr=0.005\n",
      "[06] train_loss=3.4062 acc=0.059 | val_loss=3.2326 acc=0.051 | lr=0.005\n",
      "[07] train_loss=3.3662 acc=0.064 | val_loss=3.2158 acc=0.056 | lr=0.005\n",
      "[08] train_loss=3.3643 acc=0.066 | val_loss=3.2213 acc=0.056 | lr=0.005\n",
      "[09] train_loss=3.3542 acc=0.067 | val_loss=3.2205 acc=0.047 | lr=0.005\n",
      "  ↳ ReduceLROnPlateau: lr -> 0.0025\n",
      "[10] train_loss=3.2980 acc=0.069 | val_loss=3.2130 acc=0.057 | lr=0.0025\n"
     ]
    }
   ],
   "source": [
    "for Xb, yb in iterate_minibatches(train, batch_size=64, shuffle=True, train_mode=True):\n",
    "    ...\n",
    "def evaluate(pairs, batch_size=128):\n",
    "    losses, correct, total = [], 0, 0\n",
    "    for Xb, yb in iterate_minibatches(pairs, batch_size=batch_size, shuffle=False, train_mode=False):\n",
    "        logits = forward(Xb, train=False)\n",
    "        loss = nn.softmax_cross_entropy(logits, yb)\n",
    "        losses.append(float(loss.data))\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        correct += int((pred == yb).sum()); total += int(yb.size)\n",
    "    return (np.mean(losses) if losses else float(\"nan\")), (correct/total if total else 0.0)\n",
    "\n",
    "EPOCHS = 10\n",
    "best_path = \"best_fc_bn.npz\"\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    # ---- Train (증강 ON) ----\n",
    "    train_losses = []; train_correct = train_total = 0\n",
    "    for Xb, yb in iterate_minibatches(train, batch_size=64, shuffle=True, train_mode=True):\n",
    "        logits = forward(Xb, train=True)\n",
    "        loss = nn.softmax_cross_entropy(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        train_losses.append(float(loss.data))\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        train_correct += int((pred == yb).sum()); train_total += int(yb.size)\n",
    "\n",
    "    tr_loss = float(np.mean(train_losses)); tr_acc = train_correct/train_total\n",
    "\n",
    "    # ---- Val ----\n",
    "    val_loss, val_acc = evaluate(val)\n",
    "\n",
    "    print(f\"[{ep:02d}] train_loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.3f} | lr={opt.lr:g}\")\n",
    "\n",
    "    # ---- 저장 ----\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        U.save_checkpoint(best_path, [fc1, fc2])   # bn1의 gamma/beta는 opt에 이미 포함되어 학습됨\n",
    "\n",
    "    # ---- 스케줄 ----\n",
    "    reduced = scheduler.step(val_loss)\n",
    "    if reduced:\n",
    "        print(f\"  ↳ ReduceLROnPlateau: lr -> {opt.lr:g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1312c800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장: prep.npz best_fc_bn.npz\n"
     ]
    }
   ],
   "source": [
    "np.savez(\"prep.npz\", mean3=mean3, std3=std3, breeds=np.array(breeds, dtype=object))\n",
    "print(\"저장:\", \"prep.npz\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ab4f513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss=2.4066 acc=0.125\n",
      "[100] loss=0.0056 acc=1.000\n",
      "[200] loss=0.0033 acc=1.000\n",
      "[300] loss=0.0023 acc=1.000\n",
      "[400] loss=0.0018 acc=1.000\n",
      "[500] loss=0.0014 acc=1.000\n",
      "[600] loss=0.0012 acc=1.000\n",
      "[700] loss=0.0010 acc=1.000\n",
      "[800] loss=0.0009 acc=1.000\n",
      "[900] loss=0.0008 acc=1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ssuatensortflow.tensor import Tensor\n",
    "from ssuatensortflow.layers import Linear, SGD\n",
    "from ssuatensortflow.nn import softmax_cross_entropy\n",
    "from ssuatensortflow.utils import seed_everything\n",
    "from ssuatensortflow.transforms import to_vec\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "# 가짜 데이터 (B=16, C=10), 입력 32x32x3\n",
    "B, C = 16, 10\n",
    "X_np = np.random.randn(B, 32*32*3).astype(np.float32) * 0.2\n",
    "y_np = np.random.randint(0, C, size=(B,), dtype=np.int64)\n",
    "\n",
    "# 아주 작은 MLP (BN 빼고 먼저 확인)\n",
    "l1 = Linear(32*32*3, 128, init=\"he\")\n",
    "l2 = Linear(128, C, init=\"xavier\")\n",
    "layers = [l1, l2]\n",
    "\n",
    "opt = SGD([p for L in layers for p in L.parameters()], lr=1e-2, momentum=0.9, weight_decay=0.0)\n",
    "\n",
    "for t in range(1000):\n",
    "    x = Tensor(X_np, requires_grad=False)           # (B, D)\n",
    "    h = l1(x).relu()\n",
    "    logits = l2(h)\n",
    "    loss = softmax_cross_entropy(logits, y_np)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        acc = (pred == y_np).mean()\n",
    "        print(f\"[{t}] loss={loss.data:.4f} acc={acc:.3f}\")\n",
    "\n",
    "# 기대: acc가 0.9~1.0 가까이 올라감(과적합)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "80b98e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib, sys, os\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "importlib.invalidate_caches()\n",
    "import ssuatensortflow\n",
    "importlib.reload(ssuatensortflow)\n",
    "from ssuatensortflow.data import build_index, split_train_val, ImageFolderDataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4dfa2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, sys, os\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "importlib.invalidate_caches()\n",
    "import ssuatensortflow\n",
    "importlib.reload(ssuatensortflow)\n",
    "\n",
    "from ssuatensortflow.data import build_index, split_train_val, ImageFolderDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8dd28",
   "metadata": {},
   "source": [
    "## 데이터 로드 + DataLoader 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8f207b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 수: 25\n",
      "예시 클래스 5개: ['american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer']\n",
      "train: 4481  | val: 497\n"
     ]
    }
   ],
   "source": [
    "from ssuatensortflow.transforms import Compose, ToRGBResize, RandomHorizontalFlip, RandomPadCrop, Normalize01, to_chw\n",
    "from ssuatensortflow.utils import seed_everything\n",
    "import numpy as np\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "data_root = \"data\"\n",
    "\n",
    "paths, labels, classes = build_index(data_root)\n",
    "print(\"클래스 수:\", len(classes))\n",
    "print(\"예시 클래스 5개:\", classes[:5])\n",
    "\n",
    "(train_paths, train_labels), (val_paths, val_labels) = split_train_val(paths, labels, val_ratio=0.1, seed=0)\n",
    "print(\"train:\", len(train_paths), \" | val:\", len(val_paths))\n",
    "\n",
    "# 변환 정의\n",
    "train_tf = Compose([\n",
    "    ToRGBResize(64),\n",
    "    RandomPadCrop(64, pad=4),\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    Normalize01([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
    "])\n",
    "val_tf = Compose([\n",
    "    ToRGBResize(64),\n",
    "    Normalize01([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "train_ds = ImageFolderDataset(train_paths, train_labels, transform=train_tf)\n",
    "val_ds   = ImageFolderDataset(val_paths, val_labels, transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6dbc6",
   "metadata": {},
   "source": [
    "## 모델 구성 + 학습 루프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "854e10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: data\n",
      "하위 폴더 목록: ['wheaten_terrier', 'great_pyrenees', 'american_bulldog', 'pomeranian', 'american_pit_bull_terrier', 'yorkshire_terrier', 'japanese_chin', 'english_cocker_spaniel', 'miniature_pinscher', 'basset_hound', 'saint_bernard', 'chihuahua', 'newfoundland', 'pug', 'havanese', 'beagle', 'german_shorthaired', 'staffordshire_bull_terrier', 'samoyed', 'scottish_terrier', 'leonberger', 'keeshond', 'boxer', 'english_setter', 'shiba_inu']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_ROOT = \"data\"\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"하위 폴더 목록:\", os.listdir(DATA_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359dc403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 25, Train: 4229, Val: 749\n",
      "[001] lr=0.00500  train loss=3.6698 acc=0.075 | val loss=3.0866 acc=0.123  (28.0s)\n",
      "[002] lr=0.00500  train loss=3.3722 acc=0.092 | val loss=3.0427 acc=0.142  (23.4s)\n",
      "[003] lr=0.00500  train loss=3.2689 acc=0.113 | val loss=3.0283 acc=0.147  (22.9s)\n",
      "[004] lr=0.00500  train loss=3.1831 acc=0.120 | val loss=3.0216 acc=0.148  (23.5s)\n",
      "[005] lr=0.00500  train loss=3.1407 acc=0.136 | val loss=3.0159 acc=0.146  (22.9s)\n",
      "[006] lr=0.00500  train loss=3.1014 acc=0.145 | val loss=3.0095 acc=0.152  (23.5s)\n",
      "[007] lr=0.00450  train loss=3.0789 acc=0.143 | val loss=3.0090 acc=0.152  (23.5s)\n",
      "[008] lr=0.00450  train loss=3.0554 acc=0.150 | val loss=2.9981 acc=0.150  (23.5s)\n",
      "[009] lr=0.00450  train loss=3.0360 acc=0.152 | val loss=2.9957 acc=0.166  (22.7s)\n",
      "[010] lr=0.00450  train loss=3.0181 acc=0.162 | val loss=2.9921 acc=0.159  (26.3s)\n",
      "[011] lr=0.00450  train loss=3.0042 acc=0.163 | val loss=2.9844 acc=0.164  (23.2s)\n",
      "[012] lr=0.00405  train loss=2.9970 acc=0.162 | val loss=2.9838 acc=0.156  (26.6s)\n",
      "[013] lr=0.00405  train loss=2.9833 acc=0.173 | val loss=2.9791 acc=0.160  (29.7s)\n",
      "[014] lr=0.00405  train loss=2.9673 acc=0.179 | val loss=2.9777 acc=0.170  (28.1s)\n",
      "[015] lr=0.00405  train loss=2.9526 acc=0.190 | val loss=2.9841 acc=0.158  (32.5s)\n",
      "[016] lr=0.00365  train loss=2.9438 acc=0.182 | val loss=2.9845 acc=0.179  (28.2s)\n",
      "[017] lr=0.00365  train loss=2.9448 acc=0.176 | val loss=2.9805 acc=0.160  (24.2s)\n",
      "[018] lr=0.00365  train loss=2.9319 acc=0.195 | val loss=2.9818 acc=0.163  (22.1s)\n",
      "[019] lr=0.00328  train loss=2.9158 acc=0.198 | val loss=2.9816 acc=0.160  (22.7s)\n",
      "[020] lr=0.00295  train loss=2.9142 acc=0.199 | val loss=2.9817 acc=0.152  (23.1s)\n",
      "[021] lr=0.00295  train loss=2.9197 acc=0.199 | val loss=2.9763 acc=0.175  (26.6s)\n",
      "[022] lr=0.00295  train loss=2.9099 acc=0.207 | val loss=2.9724 acc=0.160  (27.1s)\n",
      "[023] lr=0.00295  train loss=2.9036 acc=0.205 | val loss=2.9702 acc=0.158  (48.5s)\n",
      "[024] lr=0.00295  train loss=2.9018 acc=0.202 | val loss=2.9688 acc=0.174  (27.5s)\n",
      "[025] lr=0.00295  train loss=2.8950 acc=0.212 | val loss=2.9722 acc=0.172  (31.2s)\n",
      "[026] lr=0.00266  train loss=2.8981 acc=0.212 | val loss=2.9716 acc=0.182  (54.1s)\n",
      "[027] lr=0.00266  train loss=2.8937 acc=0.205 | val loss=2.9684 acc=0.176  (22.9s)\n",
      "[028] lr=0.00266  train loss=2.8900 acc=0.214 | val loss=2.9631 acc=0.175  (21.4s)\n",
      "[029] lr=0.00266  train loss=2.8643 acc=0.223 | val loss=2.9698 acc=0.168  (21.6s)\n",
      "[030] lr=0.00266  train loss=2.8739 acc=0.225 | val loss=2.9660 acc=0.182  (21.4s)\n",
      "[031] lr=0.00239  train loss=2.8702 acc=0.214 | val loss=2.9656 acc=0.175  (21.5s)\n",
      "[032] lr=0.00239  train loss=2.8658 acc=0.226 | val loss=2.9682 acc=0.167  (21.8s)\n",
      "[033] lr=0.00239  train loss=2.8514 acc=0.231 | val loss=2.9625 acc=0.174  (21.6s)\n",
      "[034] lr=0.00239  train loss=2.8565 acc=0.225 | val loss=2.9653 acc=0.168  (23.2s)\n",
      "[035] lr=0.00239  train loss=2.8524 acc=0.234 | val loss=2.9578 acc=0.187  (23.0s)\n",
      "[036] lr=0.00215  train loss=2.8527 acc=0.235 | val loss=2.9580 acc=0.172  (22.5s)\n",
      "[037] lr=0.00194  train loss=2.8412 acc=0.229 | val loss=2.9579 acc=0.192  (28.2s)\n",
      "[038] lr=0.00194  train loss=2.8509 acc=0.238 | val loss=2.9541 acc=0.186  (29.1s)\n",
      "[039] lr=0.00194  train loss=2.8385 acc=0.233 | val loss=2.9597 acc=0.190  (26.2s)\n",
      "[040] lr=0.00194  train loss=2.8289 acc=0.237 | val loss=2.9584 acc=0.191  (24.3s)\n",
      "[041] lr=0.00194  train loss=2.8263 acc=0.238 | val loss=2.9633 acc=0.188  (27.4s)\n",
      "[042] lr=0.00194  train loss=2.8304 acc=0.237 | val loss=2.9608 acc=0.191  (27.2s)\n",
      "[043] lr=0.00174  train loss=2.8279 acc=0.240 | val loss=2.9601 acc=0.186  (27.6s)\n",
      "[044] lr=0.00174  train loss=2.8258 acc=0.244 | val loss=2.9614 acc=0.179  (27.3s)\n",
      "[045] lr=0.00174  train loss=2.8198 acc=0.245 | val loss=2.9650 acc=0.180  (27.3s)\n",
      "[046] lr=0.00174  train loss=2.8238 acc=0.235 | val loss=2.9571 acc=0.187  (27.7s)\n",
      "[047] lr=0.00174  train loss=2.8165 acc=0.245 | val loss=2.9557 acc=0.190  (28.8s)\n",
      "[048] lr=0.00174  train loss=2.8096 acc=0.257 | val loss=2.9574 acc=0.195  (26.9s)\n",
      "[049] lr=0.00174  train loss=2.8099 acc=0.243 | val loss=2.9609 acc=0.188  (22.6s)\n",
      "[050] lr=0.00157  train loss=2.8112 acc=0.247 | val loss=2.9607 acc=0.190  (812.6s)\n",
      "Early stopped at epoch 50\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# train_ssua_standalone.py (v2: 과적합 완화 튜닝 반영)\n",
    "# ------------------------------------------------------------\n",
    "# - transforms / dataset / dataloader / split 내장\n",
    "# - ssuatensortflow: Tensor / Linear / BatchNorm1d / SGD / dropout / LS-CE / EarlyStopping 사용\n",
    "# - 변경점: 입력/히든 축소, weight_decay↑, label smoothing↑, dropout↑, PadCrop 증강, 간단 LR decay\n",
    "# - DataLoader에서 (B,H,W,C) → (B,F) 평탄화\n",
    "# ------------------------------------------------------------\n",
    "import os, sys, time, random, glob\n",
    "import numpy as np\n",
    "\n",
    "# 0) ssuatensortflow 경로 추가\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"Model\"))\n",
    "\n",
    "# 1) 라이브러리 임포트 (공주님 라이브러리)\n",
    "from ssuatensortflow.tensor import Tensor\n",
    "from ssuatensortflow.layers import Linear, BatchNorm1d, SGD, dropout  # 함수형 dropout\n",
    "from ssuatensortflow.nn import cross_entropy_with_label_smoothing\n",
    "from ssuatensortflow.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# A. 이 파일 안에서만 쓰는 전처리/데이터 유틸 (버전차이 방지용)\n",
    "# ============================================================\n",
    "from PIL import Image\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms): self.transforms = transforms\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms: img = t(img)\n",
    "        return img\n",
    "\n",
    "class ToRGBResize:\n",
    "    def __init__(self, width, height):\n",
    "        self.w, self.h = width, height\n",
    "    def __call__(self, img):\n",
    "        # img 타입에 상관없이 안전하게 PIL.Image로 변환\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img)\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            arr = img\n",
    "            if arr.dtype != np.uint8:\n",
    "                arr = np.clip(arr * (255.0 if arr.max() <= 1.0 else 1.0), 0, 255).astype(np.uint8)\n",
    "            img = Image.fromarray(arr)\n",
    "        img = img.convert(\"RGB\").resize((self.w, self.h))\n",
    "        return np.array(img, dtype=np.float32) / 255.0  # 0~1 스케일\n",
    "\n",
    "class Normalize01:\n",
    "    def __init__(self, mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)):\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "        self.std  = np.array(std,  dtype=np.float32)\n",
    "    def __call__(self, img):\n",
    "        x = img.data if isinstance(img, Tensor) else img\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str):\n",
    "            return img  # 경로면 다음 변환에서 처리\n",
    "        if np.random.rand() < self.p:\n",
    "            if isinstance(img, np.ndarray):\n",
    "                return np.fliplr(img)\n",
    "        return img\n",
    "\n",
    "class RandomPadCrop:\n",
    "    \"\"\"이미지 가장자리를 반사 패딩 후 원래 크기로 랜덤 크롭\"\"\"\n",
    "    def __init__(self, padding=8):\n",
    "        self.padding = padding\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str):\n",
    "            return img  # 경로 문자열은 다음 변환에서 처리\n",
    "        if self.padding <= 0:\n",
    "            return img\n",
    "        h, w, c = img.shape\n",
    "        pad = self.padding\n",
    "        padded = np.pad(img, ((pad,pad),(pad,pad),(0,0)), mode=\"reflect\")\n",
    "        top  = np.random.randint(0, 2*pad)\n",
    "        left = np.random.randint(0, 2*pad)\n",
    "        return padded[top:top+h, left:left+w]\n",
    "\n",
    "def stratified_split(data_root, val_ratio=0.15, seed=42):\n",
    "    \"\"\"data_root/{class}/*.jpg 구조를 클래스 비율 유지하며 분할\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    classes = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
    "    cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    train, val = [], []\n",
    "    for c in classes:\n",
    "        files = []\n",
    "        for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\"):\n",
    "            files += glob.glob(os.path.join(data_root, c, ext))\n",
    "        files.sort(); rng.shuffle(files)\n",
    "        k = int(len(files) * (1 - val_ratio))\n",
    "        train += [(f, cls_to_idx[c]) for f in files[:k]]\n",
    "        val   += [(f, cls_to_idx[c]) for f in files[k:]]\n",
    "    rng.shuffle(train); rng.shuffle(val)\n",
    "    return train, val\n",
    "\n",
    "class ImageFolderDataset:\n",
    "    \"\"\"(path, class_idx) 리스트를 받아 이미지를 로드/전처리\"\"\"\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(set([y for _, y in samples]))\n",
    "        self.num_classes = len(self.classes)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, y = self.samples[idx]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(path)  # 변환 파이프라인에 로드 포함\n",
    "        else:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            x = np.array(img, dtype=np.float32) / 255.0\n",
    "        if not isinstance(x, Tensor):\n",
    "            x = Tensor(x.astype(np.float32))  # Tensor로 감싸기 (H,W,C)\n",
    "        return x, np.int64(y)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=64, shuffle=True, drop_last=False):\n",
    "        self.ds, self.bs, self.shuffle, self.drop_last = dataset, batch_size, shuffle, drop_last\n",
    "    def __iter__(self):\n",
    "        idxs = list(range(len(self.ds)))\n",
    "        if self.shuffle: random.shuffle(idxs)\n",
    "        batch = []\n",
    "        for i in idxs:\n",
    "            batch.append(self.ds[i])\n",
    "            if len(batch) == self.bs:\n",
    "                yield self._collate(batch); batch = []\n",
    "        if batch and not self.drop_last:\n",
    "            yield self._collate(batch)\n",
    "    def __len__(self):\n",
    "        n = len(self.ds) // self.bs\n",
    "        if len(self.ds) % self.bs and not self.drop_last: n += 1\n",
    "        return max(1, n)\n",
    "    def _collate(self, batch):\n",
    "        # (B,H,W,C) 스택 후 (B,F) 평탄화\n",
    "        xs, ys = zip(*batch)\n",
    "        x_np = np.stack([x.data for x in xs], axis=0).astype(np.float32)\n",
    "        if x_np.ndim == 4:\n",
    "            if x_np.shape[-1] == 3:  # (B,H,W,C)\n",
    "                B = x_np.shape[0]\n",
    "                x_np = x_np.reshape(B, -1)\n",
    "            else:  # (B,3,H,W)\n",
    "                B, C, H, W = x_np.shape\n",
    "                x_np = x_np.reshape(B, C*H*W)\n",
    "        return Tensor(x_np), np.array(ys, dtype=np.int64)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def save_checkpoint(path, state: dict):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    np.savez_compressed(path, **state)\n",
    "\n",
    "# ============================================================\n",
    "# B. 설정 (v2 튜닝)\n",
    "# ============================================================\n",
    "# ★ 공주님 실제 데이터 경로 (캡처 기준 'Model/data')\n",
    "DATA_ROOT = \"data\"\n",
    "\n",
    "IMG_SIZE  = 160      # 224 -> 160\n",
    "BATCH     = 64\n",
    "EPOCHS    = 60\n",
    "LR        = 5e-3     # 1e-2 -> 5e-3\n",
    "MOM       = 0.9\n",
    "WD        = 5e-4     # 1e-4 -> 5e-4\n",
    "SMOOTH    = 0.2      # 0.1 -> 0.2\n",
    "PATIENCE  = 12       # 8  -> 12\n",
    "VAL_RATIO = 0.15\n",
    "SEED      = 42\n",
    "\n",
    "HIDDEN    = 256      # 512 -> 256\n",
    "\n",
    "# ============================================================\n",
    "# C. 데이터\n",
    "# ============================================================\n",
    "seed_everything(SEED)\n",
    "\n",
    "def build_transforms(split):\n",
    "    if split == \"train\":\n",
    "        return Compose([\n",
    "            ToRGBResize(IMG_SIZE, IMG_SIZE),\n",
    "            RandomPadCrop(padding=8),     # ★ 추가 증강\n",
    "            RandomHorizontalFlip(0.5),\n",
    "            Normalize01(),\n",
    "        ])\n",
    "    else:\n",
    "        return Compose([\n",
    "            ToRGBResize(IMG_SIZE, IMG_SIZE),\n",
    "            Normalize01(),\n",
    "        ])\n",
    "\n",
    "if not os.path.isdir(DATA_ROOT):\n",
    "    raise FileNotFoundError(f\"DATA_ROOT가 없습니다: {DATA_ROOT}\")\n",
    "\n",
    "train_files, val_files = stratified_split(DATA_ROOT, val_ratio=VAL_RATIO, seed=SEED)\n",
    "train_ds = ImageFolderDataset(train_files, transform=build_transforms(\"train\"))\n",
    "val_ds   = ImageFolderDataset(val_files,   transform=build_transforms(\"val\"))\n",
    "\n",
    "num_classes = train_ds.num_classes\n",
    "in_features = IMG_SIZE * IMG_SIZE * 3\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, drop_last=False)\n",
    "\n",
    "# ============================================================\n",
    "# D. 모델\n",
    "# ============================================================\n",
    "class FCNet:\n",
    "    def __init__(self, in_features, hidden=HIDDEN, num_classes=10):\n",
    "        self.fc1 = Linear(in_features, hidden, init=\"he\")\n",
    "        self.bn1 = BatchNorm1d(hidden)\n",
    "        self.fc2 = Linear(hidden, num_classes, init=\"xavier\")\n",
    "        self.training = True\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.fc1.parameters() + self.bn1.parameters() + self.fc2.parameters()\n",
    "\n",
    "    def train(self): self.training = True\n",
    "    def eval(self):  self.training = False\n",
    "\n",
    "    def relu(self, x):\n",
    "        mask = (x.data > 0).astype(x.data.dtype)\n",
    "        return x * mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DataLoader에서 이미 (B,F)\n",
    "        y = self.fc1(x)\n",
    "        y = self.bn1(y, train=self.training)\n",
    "        y = self.relu(y)\n",
    "        y = dropout(y, p=0.6, train=self.training)  # 0.4 -> 0.6\n",
    "        y = self.fc2(y)\n",
    "        return y\n",
    "\n",
    "    # 얼리 스탑 저장/복원\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"fc1.W\": self.fc1.W.data.copy(),\n",
    "            \"fc1.b\": None if self.fc1.b is None else self.fc1.b.data.copy(),\n",
    "            \"bn1.gamma\": self.bn1.gamma.data.copy(),\n",
    "            \"bn1.beta\":  self.bn1.beta.data.copy(),\n",
    "            \"bn1.running_mean\": self.bn1.running_mean.copy(),\n",
    "            \"bn1.running_var\":  self.bn1.running_var.copy(),\n",
    "            \"fc2.W\": self.fc2.W.data.copy(),\n",
    "            \"fc2.b\": None if self.fc2.b is None else self.fc2.b.data.copy(),\n",
    "        }\n",
    "    def load_state_dict(self, sd):\n",
    "        self.fc1.W.data[:] = sd[\"fc1.W\"]\n",
    "        if self.fc1.b is not None and sd[\"fc1.b\"] is not None:\n",
    "            self.fc1.b.data[:] = sd[\"fc1.b\"]\n",
    "        self.bn1.gamma.data[:] = sd[\"bn1.gamma\"]\n",
    "        self.bn1.beta.data[:]  = sd[\"bn1.beta\"]\n",
    "        self.bn1.running_mean[:] = sd[\"bn1.running_mean\"]\n",
    "        self.bn1.running_var[:]  = sd[\"bn1.running_var\"]\n",
    "        self.fc2.W.data[:] = sd[\"fc2.W\"]\n",
    "        if self.fc2.b is not None and sd[\"fc2.b\"] is not None:\n",
    "            self.fc2.b.data[:] = sd[\"fc2.b\"]\n",
    "\n",
    "model = FCNet(in_features, hidden=HIDDEN, num_classes=num_classes)\n",
    "optimizer = SGD(model.parameters(), lr=LR, momentum=MOM, weight_decay=WD)\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "# ============================================================\n",
    "# E. 학습/평가 루프 (+ 간단 LR decay)\n",
    "# ============================================================\n",
    "def run_one_epoch(loader, train=True):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        logits = model.forward(x)\n",
    "        loss = cross_entropy_with_label_smoothing(logits, y, eps=SMOOTH)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.data)\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        total_correct += int((pred == y).sum())\n",
    "        total_count  += y.shape[0]\n",
    "\n",
    "    return total_loss / max(1, len(loader)), total_correct / max(1, total_count)\n",
    "\n",
    "# ============================================================\n",
    "# F. 실행\n",
    "# ============================================================\n",
    "best_path = os.path.join(\"Model\", \"checkpoints\", \"best_fc_bn.npz\")\n",
    "os.makedirs(os.path.dirname(best_path), exist_ok=True)\n",
    "\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "print(f\"Classes: {num_classes}, Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = run_one_epoch(train_loader, True)\n",
    "    va_loss, va_acc = run_one_epoch(val_loader, False)\n",
    "\n",
    "    # 간단한 plateau 감지 → lr 0.9배\n",
    "    if epoch > 3 and abs(prev_val_loss - va_loss) < 1e-3:\n",
    "        optimizer.lr *= 0.9\n",
    "    prev_val_loss = va_loss\n",
    "\n",
    "    early.update(va_loss, model.state_dict, model.load_state_dict)\n",
    "\n",
    "    try: save_checkpoint(best_path, model.state_dict())\n",
    "    except Exception: pass\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[{epoch:03d}] lr={optimizer.lr:.5f}  train loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val loss={va_loss:.4f} acc={va_acc:.3f}  ({dt:.1f}s)\")\n",
    "\n",
    "    if early.stopped:\n",
    "        print(f\"Early stopped at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d6f72",
   "metadata": {},
   "source": [
    "## 자꾸 과적합 아니면 정확도가 15% 밖에 안나옴\n",
    "- 이유 사진의 픽셀을 뭉텅이로 보는게 아니라 각자 따로 보기 때문에 그런거임 \n",
    "- 컨볼루션(Conv) + 풀링(MaxPool) 으로 = CNN 3*3 뭐 미런식으로 봐서 귀 형태를 잡는다거나 이런식으로 접근 하면 될듯?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 25, Train: 4229, Val: 749\n"
     ]
    }
   ],
   "source": [
    "# train_ssua_cnn_standalone.py\n",
    "# ------------------------------------------------------------\n",
    "# 강아지 종 분류: 순수 NumPy CNN (Conv2d + MaxPool2d + GAP + FC)\n",
    "# - 메모리-세이프 Conv2d (샘플별 im2col/grad 재계산)\n",
    "# - 축 안전 벡터화 MaxPool2d (2x2,stride=2 전용)\n",
    "# - Global Average Pooling으로 일반화↑, 파라미터↓\n",
    "# - EarlyStopping, Label Smoothing(초기 0), Dropout(0.2), BN1d\n",
    "# - DataLoader가 (B,C,H,W)로 변환해 CNN에 투입\n",
    "# ------------------------------------------------------------\n",
    "import os, sys, time, random, glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---- ssuatensortflow 경로 ----\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"Model\"))\n",
    "\n",
    "# ---- 라이브러리 임포트 ----\n",
    "from ssuatensortflow.tensor import Tensor\n",
    "from ssuatensortflow.layers import Linear, BatchNorm1d, SGD, dropout\n",
    "from ssuatensortflow.nn import cross_entropy_with_label_smoothing\n",
    "from ssuatensortflow.callbacks import EarlyStopping\n",
    "\n",
    "# ================= 전처리/데이터 유틸 =================\n",
    "class Compose:\n",
    "    def __init__(self, transforms): self.transforms = transforms\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms: img = t(img)\n",
    "        return img\n",
    "\n",
    "class ToRGBResize:\n",
    "    def __init__(self, width, height):\n",
    "        self.w, self.h = width, height\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img)\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            arr = img\n",
    "            if arr.dtype != np.uint8:\n",
    "                arr = np.clip(arr * (255.0 if arr.max() <= 1.0 else 1.0), 0, 255).astype(np.uint8)\n",
    "            img = Image.fromarray(arr)\n",
    "        img = img.convert(\"RGB\").resize((self.w, self.h))\n",
    "        return np.array(img, dtype=np.float32) / 255.0  # (H,W,3)\n",
    "\n",
    "class Normalize01:\n",
    "    def __init__(self, mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)):\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "        self.std  = np.array(std,  dtype=np.float32)\n",
    "    def __call__(self, img):\n",
    "        x = img.data if isinstance(img, Tensor) else img\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str): return img\n",
    "        if np.random.rand() < self.p and isinstance(img, np.ndarray):\n",
    "            return np.fliplr(img)\n",
    "        return img\n",
    "\n",
    "class RandomPadCrop:\n",
    "    def __init__(self, padding=8): self.padding = padding\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, str): return img\n",
    "        if self.padding <= 0: return img\n",
    "        h, w, c = img.shape\n",
    "        pad = self.padding\n",
    "        padded = np.pad(img, ((pad,pad),(pad,pad),(0,0)), mode=\"reflect\")\n",
    "        top  = np.random.randint(0, 2*pad)\n",
    "        left = np.random.randint(0, 2*pad)\n",
    "        return padded[top:top+h, left:left+w]\n",
    "\n",
    "def stratified_split(data_root, val_ratio=0.15, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    classes = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
    "    cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    train, val = [], []\n",
    "    for c in classes:\n",
    "        files = []\n",
    "        for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\"):\n",
    "            files += glob.glob(os.path.join(data_root, c, ext))\n",
    "        files.sort(); rng.shuffle(files)\n",
    "        k = int(len(files) * (1 - val_ratio))\n",
    "        train += [(f, cls_to_idx[c]) for f in files[:k]]\n",
    "        val   += [(f, cls_to_idx[c]) for f in files[k:]]\n",
    "    rng.shuffle(train); rng.shuffle(val)\n",
    "    return train, val\n",
    "\n",
    "class ImageFolderDataset:\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(set([y for _, y in samples]))\n",
    "        self.num_classes = len(self.classes)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, y = self.samples[idx]\n",
    "        x = self.transform(path) if self.transform else np.array(Image.open(path).convert(\"RGB\"), dtype=np.float32)/255.0\n",
    "        if not isinstance(x, Tensor):\n",
    "            x = Tensor(x.astype(np.float32))  # (H,W,C)\n",
    "        return x, np.int64(y)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=64, shuffle=True, drop_last=False):\n",
    "        self.ds, self.bs, self.shuffle, self.drop_last = dataset, batch_size, shuffle, drop_last\n",
    "    def __iter__(self):\n",
    "        idxs = list(range(len(self.ds)))\n",
    "        if self.shuffle: random.shuffle(idxs)\n",
    "        batch=[]\n",
    "        for i in idxs:\n",
    "            batch.append(self.ds[i])\n",
    "            if len(batch)==self.bs:\n",
    "                yield self._collate(batch); batch=[]\n",
    "        if batch and not self.drop_last:\n",
    "            yield self._collate(batch)\n",
    "    def __len__(self):\n",
    "        n = len(self.ds)//self.bs\n",
    "        if len(self.ds)%self.bs and not self.drop_last: n+=1\n",
    "        return max(1,n)\n",
    "    def _collate(self, batch):\n",
    "        xs, ys = zip(*batch)\n",
    "        x_np = np.stack([x.data for x in xs], axis=0).astype(np.float32)  # (B,H,W,C)\n",
    "        x_np = np.transpose(x_np, (0,3,1,2)).copy()  # → (B,C,H,W)\n",
    "        return Tensor(x_np), np.array(ys, dtype=np.int64)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def save_checkpoint(path, state: dict):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    np.savez_compressed(path, **state)\n",
    "\n",
    "# ================= Conv / Pool (메모리-세이프) =================\n",
    "def _im2col(x, kh, kw, sh, sw, ph, pw):\n",
    "    # x: (B,C,H,W) numpy\n",
    "    B,C,H,W = x.shape\n",
    "    H_out = (H + 2*ph - kh)//sh + 1\n",
    "    W_out = (W + 2*pw - kw)//sw + 1\n",
    "    x_pad = np.pad(x, ((0,0),(0,0),(ph,ph),(pw,pw)), mode='constant')\n",
    "    cols = np.zeros((B, C, kh, kw, H_out, W_out), dtype=x.dtype)\n",
    "    for i in range(kh):\n",
    "        i_lim = i + sh*H_out\n",
    "        for j in range(kw):\n",
    "            j_lim = j + sw*W_out\n",
    "            cols[:, :, i, j, :, :] = x_pad[:, :, i:i_lim:sh, j:j_lim:sw]\n",
    "    cols = cols.reshape(B, C*kh*kw, H_out*W_out)\n",
    "    return cols, H_out, W_out\n",
    "\n",
    "def _col2im(dcols, x_shape, kh, kw, sh, sw, ph, pw):\n",
    "    B,C,H,W = x_shape\n",
    "    H_out = (H + 2*ph - kh)//sh + 1\n",
    "    W_out = (W + 2*pw - kw)//sw + 1\n",
    "    dcols = dcols.reshape(B, C, kh, kw, H_out, W_out)\n",
    "    dx_pad = np.zeros((B, C, H+2*ph, W+2*pw), dtype=dcols.dtype)\n",
    "    for i in range(kh):\n",
    "        i_lim = i + sh*H_out\n",
    "        for j in range(kw):\n",
    "            j_lim = j + sw*W_out\n",
    "            dx_pad[:, :, i:i_lim:sh, j:j_lim:sw] += dcols[:, :, i, j, :, :]\n",
    "    return dx_pad[:, :, ph:ph+H, pw:pw+W]\n",
    "\n",
    "class Conv2d:\n",
    "    \"\"\"메모리-세이프 Conv2d (샘플별 im2col)\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, init='he'):\n",
    "        kh = kw = kernel_size if isinstance(kernel_size, int) else kernel_size[0]\n",
    "        self.kh, self.kw = kh, kw\n",
    "        self.sh = stride if isinstance(stride, int) else stride[0]\n",
    "        self.sw = stride if isinstance(stride, int) else stride[1]\n",
    "        self.ph = padding if isinstance(padding, int) else padding[0]\n",
    "        self.pw = padding if isinstance(padding, int) else padding[1]\n",
    "        fan_in = in_channels*kh*kw\n",
    "        limit = np.sqrt(6.0/max(1,fan_in)) if init=='he' else np.sqrt(6.0/(fan_in+out_channels*kh*kw))\n",
    "        W = np.random.uniform(-limit, limit, size=(out_channels, in_channels, kh, kw)).astype(np.float32)\n",
    "        self.W = Tensor(W, requires_grad=True)\n",
    "        self.b = Tensor(np.zeros((1,out_channels,1,1), np.float32), requires_grad=True) if bias else None\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for p in (self.W, self.b) if p is not None]\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        X = x.data  # (B,C,H,W)\n",
    "        B, Cin, H, W = X.shape\n",
    "        Cout = self.W.data.shape[0]\n",
    "        # 출력 크기 계산\n",
    "        _, H_out, W_out = _im2col(X[:1], self.kh, self.kw, self.sh, self.sw, self.ph, self.pw)\n",
    "        Y = np.zeros((B, Cout, H_out, W_out), dtype=np.float32)\n",
    "        W_col = self.W.data.reshape(Cout, -1)  # (Cout, K)\n",
    "\n",
    "        # 샘플별로 cols 생성 → matmul → 버리기 (피크 메모리↓)\n",
    "        for b in range(B):\n",
    "            cols_b, _, _ = _im2col(X[b:b+1], self.kh, self.kw, self.sh, self.sw, self.ph, self.pw)  # (1,K,L)\n",
    "            cols_b = cols_b[0]  # (K, L)\n",
    "            Yb = W_col @ cols_b  # (Cout, L)\n",
    "            if self.b is not None: Yb += self.b.data.reshape(-1,1)\n",
    "            Y[b] = Yb.reshape(Cout, H_out, W_out)\n",
    "\n",
    "        out_t = Tensor(\n",
    "            Y,\n",
    "            requires_grad=x.requires_grad or self.W.requires_grad or (self.b is not None and self.b.requires_grad),\n",
    "            _children=(x, self.W, self.b) if self.b is not None else (x, self.W,),\n",
    "            _op=\"conv2d_memsafe\"\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            if out_t.grad is None: return\n",
    "            gy = out_t.grad  # (B,Cout,H_out,W_out)\n",
    "            Cout, Cin, kh, kw = self.W.data.shape\n",
    "            W_col = self.W.data.reshape(Cout, -1)\n",
    "            dW = np.zeros_like(self.W.data)\n",
    "            db = np.zeros((1, Cout, 1, 1), dtype=np.float32) if self.b is not None else None\n",
    "            dX = np.zeros_like(X) if x.requires_grad else None\n",
    "\n",
    "            # 샘플별로 cols 재계산 (메모리 절약)\n",
    "            for b in range(B):\n",
    "                cols_b, _, _ = _im2col(X[b:b+1], self.kh, self.kw, self.sh, self.sw, self.ph, self.pw)  # (1,K,L)\n",
    "                cols_b = cols_b[0]                 # (K, L)\n",
    "                gyb = gy[b].reshape(Cout, -1)     # (Cout, L)\n",
    "\n",
    "                dW += (gyb @ cols_b.T).reshape(dW.shape)\n",
    "                if db is not None:\n",
    "                    db += gyb.sum(axis=1).reshape(1, Cout, 1, 1)\n",
    "                if dX is not None:\n",
    "                    dx_cols = W_col.T @ gyb        # (K, L)\n",
    "                    dX[b:b+1] += _col2im(dx_cols.reshape(1, *dx_cols.shape), X[b:b+1].shape,\n",
    "                                         self.kh, self.kw, self.sh, self.sw, self.ph, self.pw)\n",
    "\n",
    "            if self.W.requires_grad:\n",
    "                self.W.grad = (self.W.grad if self.W.grad is not None else 0) + dW\n",
    "            if (self.b is not None) and self.b.requires_grad:\n",
    "                self.b.grad = (self.b.grad if self.b.grad is not None else 0) + db\n",
    "            if x.requires_grad:\n",
    "                x.grad = (x.grad if x.grad is not None else 0) + dX\n",
    "\n",
    "        out_t._backward = _backward\n",
    "        return out_t\n",
    "\n",
    "class MaxPool2d:\n",
    "    \"\"\"벡터화 MaxPool2d (2x2, stride=2 전용; 축 안전 버전)\"\"\"\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kh = self.kw = kernel_size if isinstance(kernel_size,int) else kernel_size[0]\n",
    "        self.sh = self.sw = stride if isinstance(stride,int) else stride[0]\n",
    "        assert self.kh==2 and self.kw==2 and self.sh==2 and self.sw==2, \"2x2,stride=2만 지원\"\n",
    "        self._mask = None\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        X = x.data  # (B,C,H,W)\n",
    "        B,C,H,W = X.shape\n",
    "        H2 = (H//2)*2; W2 = (W//2)*2\n",
    "        Xc = X[:, :, :H2, :W2]\n",
    "        Hh, Ww = H2//2, W2//2\n",
    "\n",
    "        # (B,C,H/2,2,W/2,2)\n",
    "        Xr = Xc.reshape(B, C, Hh, 2, Ww, 2)\n",
    "\n",
    "        # 두 축(3,5)에서 한 번에 max → (B,C,H/2,W/2)\n",
    "        out = Xr.max(axis=(3,5))\n",
    "\n",
    "        # argmax로 정확한 마스크 구성\n",
    "        flat = Xr.reshape(B, C, Hh, Ww, 4)   # 2x2 → 4\n",
    "        arg  = flat.argmax(axis=4)           # (B,C,H/2,W/2) ∈ {0..3}\n",
    "        mask = np.zeros_like(flat, dtype=bool)\n",
    "        b = np.arange(B)[:,None,None,None]\n",
    "        c = np.arange(C)[None,:,None,None]\n",
    "        h = np.arange(Hh)[None,None,:,None]\n",
    "        w = np.arange(Ww)[None,None,None,:]\n",
    "        mask[b, c, h, w, arg] = True\n",
    "        mask = mask.reshape(B, C, Hh, 2, Ww, 2)\n",
    "\n",
    "        out_t = Tensor(out, requires_grad=x.requires_grad, _children=(x,), _op=\"maxpool2d_vec\")\n",
    "        self._mask = (mask, X.shape)\n",
    "\n",
    "        def _backward():\n",
    "            if out_t.grad is None or not x.requires_grad: \n",
    "                return\n",
    "            mask, orig_shape = self._mask\n",
    "            B,C,H,W = orig_shape\n",
    "            H2 = (H//2)*2; W2 = (W//2)*2\n",
    "            Hh, Ww = H2//2, W2//2\n",
    "\n",
    "            grad_out = out_t.grad.reshape(B, C, Hh, 1, Ww, 1)   # (B,C,H/2,1,W/2,1)\n",
    "            go2 = np.broadcast_to(grad_out, mask.shape)         # (B,C,H/2,2,W/2,2)\n",
    "            gx_small = np.where(mask, go2, 0.0)\n",
    "\n",
    "            gx = np.zeros((B,C,H,W), dtype=out_t.grad.dtype)\n",
    "            gx[:, :, :H2, :W2] = gx_small.reshape(B, C, H2, W2)\n",
    "            x.grad = (x.grad if x.grad is not None else 0) + gx\n",
    "\n",
    "        out_t._backward = _backward\n",
    "        return out_t\n",
    "\n",
    "# ================= 설정 =================\n",
    "DATA_ROOT = \"data\"   # (대문자 M)\n",
    "IMG_SIZE  = 112\n",
    "BATCH     = 16\n",
    "EPOCHS    = 10          # ↑ 안정 모드\n",
    "LR        = 5e-3        # ↓ 발산 방지\n",
    "MOM       = 0.9\n",
    "WD        = 1e-4        # 규제 약간\n",
    "SMOOTH    = 0.0         # 초반은 0.0으로 신호 세게\n",
    "PATIENCE  = 12\n",
    "VAL_RATIO = 0.15\n",
    "SEED      = 42\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "def build_transforms(split):\n",
    "    if split == \"train\":\n",
    "        return Compose([\n",
    "            ToRGBResize(IMG_SIZE, IMG_SIZE),\n",
    "            # RandomPadCrop(padding=8),   # 속도 여유 생기면 켜기\n",
    "            RandomHorizontalFlip(0.5),\n",
    "            Normalize01(),\n",
    "        ])\n",
    "    else:\n",
    "        return Compose([\n",
    "            ToRGBResize(IMG_SIZE, IMG_SIZE),\n",
    "            Normalize01(),\n",
    "        ])\n",
    "\n",
    "if not os.path.isdir(DATA_ROOT):\n",
    "    raise FileNotFoundError(f\"DATA_ROOT가 없습니다: {DATA_ROOT}\")\n",
    "\n",
    "train_files, val_files = stratified_split(DATA_ROOT, val_ratio=VAL_RATIO, seed=SEED)\n",
    "train_ds = ImageFolderDataset(train_files, transform=build_transforms(\"train\"))\n",
    "val_ds   = ImageFolderDataset(val_files,   transform=build_transforms(\"val\"))\n",
    "\n",
    "num_classes = train_ds.num_classes\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, drop_last=False)\n",
    "\n",
    "# ================= 작은 CNN (GAP 사용) =================\n",
    "class SmallCNN:\n",
    "    def __init__(self, in_ch=3, num_classes=10, hidden=256):\n",
    "        # 앞단 stride=2로 다운샘플 → 메모리/속도 안정\n",
    "        self.conv1 = Conv2d(in_ch, 16, kernel_size=3, stride=2, padding=1, init='he')\n",
    "        self.conv2 = Conv2d(16,   32, kernel_size=3, stride=1, padding=1, init='he')\n",
    "        self.pool1 = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = Conv2d(32,   64, kernel_size=3, stride=1, padding=1, init='he')\n",
    "        self.conv4 = Conv2d(64,  128, kernel_size=3, stride=1, padding=1, init='he')\n",
    "        self.pool2 = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = None; self.bn1 = None; self.fc2 = None\n",
    "        self.hidden = hidden\n",
    "        self.num_classes = num_classes\n",
    "        self.training = True\n",
    "\n",
    "    def parameters(self):\n",
    "        ps = []\n",
    "        for m in [self.conv1, self.conv2, self.conv3, self.conv4]:\n",
    "            ps += m.parameters()\n",
    "        if self.fc1 is not None:\n",
    "            ps += self.fc1.parameters() + self.bn1.parameters() + self.fc2.parameters()\n",
    "        return ps\n",
    "\n",
    "    def train(self): self.training = True\n",
    "    def eval(self):  self.training = False\n",
    "\n",
    "    def relu(self, x):\n",
    "        mask = (x.data > 0).astype(x.data.dtype)\n",
    "        return x * mask\n",
    "\n",
    "    def _ensure_fc(self, feat_4d: Tensor):\n",
    "        B,C,H,W = feat_4d.data.shape\n",
    "        F = C              # ✅ GAP 쓰니까 채널 수만 입력\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = Linear(F, self.hidden, init=\"he\")\n",
    "            self.bn1 = BatchNorm1d(self.hidden)\n",
    "            self.fc2 = Linear(self.hidden, self.num_classes, init=\"xavier\")\n",
    "\n",
    "    def forward(self, x):  # x: (B,C,H,W)\n",
    "        y = self.conv1(x); y = self.relu(y)\n",
    "        y = self.conv2(y); y = self.relu(y)\n",
    "        y = self.pool1(y)\n",
    "        y = self.conv3(y); y = self.relu(y)\n",
    "        y = self.conv4(y); y = self.relu(y)\n",
    "        y = self.pool2(y)\n",
    "\n",
    "        # FC 구성은 GAP 전에\n",
    "        self._ensure_fc(y)\n",
    "\n",
    "        # ✅ Global Average Pool: (B,C,H,W) -> (B,C)\n",
    "        B, C, H, W = y.data.shape\n",
    "        avg = y.data.mean(axis=(2,3))  # (B,C)\n",
    "        y_gap = Tensor(avg, requires_grad=y.requires_grad, _children=(y,), _op=\"gap\")\n",
    "        def _bw_gap():\n",
    "            if y_gap.grad is None or not y.requires_grad: return\n",
    "            g = y_gap.grad[:, :, None, None] / (H*W)     # (B,C,1,1)\n",
    "            g = np.broadcast_to(g, (B,C,H,W))\n",
    "            y.grad = (y.grad if y.grad is not None else 0) + g\n",
    "        y_gap._backward = _bw_gap\n",
    "\n",
    "        z = self.fc1(y_gap)\n",
    "        z = self.bn1(z, train=self.training)\n",
    "        z = self.relu(z)\n",
    "        z = dropout(z, p=0.2, train=self.training)   # 0.3 -> 0.2\n",
    "        z = self.fc2(z)\n",
    "        return z\n",
    "\n",
    "# ================= 학습 루프 =================\n",
    "model = SmallCNN(in_ch=3, num_classes=num_classes, hidden=256)\n",
    "\n",
    "# 워밍업(FC 생성) + 옵티마이저 준비\n",
    "x_warm, y_warm = next(iter(train_loader))\n",
    "_ = model.forward(x_warm)\n",
    "optimizer = SGD(model.parameters(), lr=LR, momentum=MOM, weight_decay=WD)\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "def run_one_epoch(loader, train=True):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "    for bi, (x,y) in enumerate(loader, 1):\n",
    "        logits = model.forward(x)\n",
    "        loss = cross_entropy_with_label_smoothing(logits, y, eps=SMOOTH)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += float(loss.data)\n",
    "        pred = np.argmax(logits.data, axis=1)\n",
    "        total_correct += int((pred==y).sum())\n",
    "        total_count  += y.shape[0]\n",
    "    return total_loss/max(1,len(loader)), total_correct/max(1,total_count)\n",
    "\n",
    "best_path = os.path.join(\"Model\", \"checkpoints\", \"best_cnn.npz\")\n",
    "os.makedirs(os.path.dirname(best_path), exist_ok=True)\n",
    "\n",
    "print(f\"Classes: {num_classes}, Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "prev_val = float(\"inf\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0=time.time()\n",
    "    tr_loss, tr_acc = run_one_epoch(train_loader, True)\n",
    "    va_loss, va_acc = run_one_epoch(val_loader, False)\n",
    "\n",
    "    # plateau 감지는 5epoch 이후부터\n",
    "    if epoch > 5 and (prev_val - va_loss) < 5e-4:\n",
    "        optimizer.lr *= 0.9\n",
    "    prev_val = va_loss\n",
    "\n",
    "    # 얼리 스탑 체크포인트\n",
    "    early.update(va_loss,\n",
    "        lambda: {\n",
    "            \"conv1.W\": model.conv1.W.data.copy(),\n",
    "            \"conv1.b\": None if model.conv1.b is None else model.conv1.b.data.copy(),\n",
    "            \"conv2.W\": model.conv2.W.data.copy(),\n",
    "            \"conv2.b\": None if model.conv2.b is None else model.conv2.b.data.copy(),\n",
    "            \"conv3.W\": model.conv3.W.data.copy(),\n",
    "            \"conv3.b\": None if model.conv3.b is None else model.conv3.b.data.copy(),\n",
    "            \"conv4.W\": model.conv4.W.data.copy(),\n",
    "            \"conv4.b\": None if model.conv4.b is None else model.conv4.b.data.copy(),\n",
    "            \"fc1.W\":   model.fc1.W.data.copy(),\n",
    "            \"fc1.b\":   None if model.fc1.b is None else model.fc1.b.data.copy(),\n",
    "            \"bn1.gamma\": model.bn1.gamma.data.copy(),\n",
    "            \"bn1.beta\":  model.bn1.beta.data.copy(),\n",
    "            \"bn1.running_mean\": model.bn1.running_mean.copy(),\n",
    "            \"bn1.running_var\":  model.bn1.running_var.copy(),\n",
    "            \"fc2.W\":   model.fc2.W.data.copy(),\n",
    "            \"fc2.b\":   None if model.fc2.b is None else model.fc2.b.data.copy(),\n",
    "        },\n",
    "        lambda sd: None\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        save_checkpoint(best_path, {\n",
    "            \"conv1.W\": model.conv1.W.data,\n",
    "            \"conv1.b\": None if model.conv1.b is None else model.conv1.b.data,\n",
    "            \"conv2.W\": model.conv2.W.data,\n",
    "            \"conv2.b\": None if model.conv2.b is None else model.conv2.b.data,\n",
    "            \"conv3.W\": model.conv3.W.data,\n",
    "            \"conv3.b\": None if model.conv3.b is None else model.conv3.b.data,\n",
    "            \"conv4.W\": model.conv4.W.data,\n",
    "            \"conv4.b\": None if model.conv4.b is None else model.conv4.b.data,\n",
    "            \"fc1.W\":   model.fc1.W.data,\n",
    "            \"fc1.b\":   None if model.fc1.b is None else model.fc1.b.data,\n",
    "            \"bn1.gamma\": model.bn1.gamma.data,\n",
    "            \"bn1.beta\":  model.bn1.beta.data,\n",
    "            \"bn1.running_mean\": model.bn1.running_mean,\n",
    "            \"bn1.running_var\":  model.bn1.running_var,\n",
    "            \"fc2.W\":   model.fc2.W.data,\n",
    "            \"fc2.b\":   None if model.fc2.b is None else model.fc2.b.data,\n",
    "        })\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dt=time.time()-t0\n",
    "    print(f\"[{epoch:03d}] lr={optimizer.lr:.5f}  train loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val loss={va_loss:.4f} acc={va_acc:.3f}  ({dt:.1f}s)\")\n",
    "\n",
    "    if early.stopped:\n",
    "        print(f\"Early stopped at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"Done.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
